{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saad Khan and Veer Khosla**\n",
    "\n",
    "Fall 2024\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 2: Multi-layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for preprocessing dataset\n",
    "import preprocess_data\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.show()\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement single layer network to test softmax activation and cross-entropy loss\n",
    "\n",
    "You will first implement and test out the softmax activation and cross-entropy loss in a single layer net before embedding it in a more complex multi-layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Load in preprocessed STL-10 data\n",
    "\n",
    "Use your automated preprocessing function to load in the STL-10 data in the following split:\n",
    "- 3000 training samples\n",
    "- 750 test samples\n",
    "- 1000 validation samples\n",
    "- 250 samples for development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04439e5fe34c8f6809f31a3c37641e6d",
     "grade": false,
     "grade_id": "cell-41a2ba4df2cf6f32",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "x_train, y_train, x_test, y_test, x_val, y_val, x_dev, y_dev = preprocess_data.load_stl10(n_train_samps=3000, n_test_samps=750, n_valid_samps=1000, n_dev_samps=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Implement the following functions\n",
    "\n",
    "In `softmax_layer.py`, implement the following methods in the base class `SoftmaxLayer`:\n",
    "\n",
    "- `fit`\n",
    "- `net_in`\n",
    "- `predict`\n",
    "- `one_hot`\n",
    "- `accuracy`\n",
    "- `activation` (softmax) $f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^C e^{x_j}}$ where $x_i$ are the \"net in\" values and there are $C$ output neurons (one per input class). $f(x_i)$ is the activation values of each output neuron $i$. Since this is softmax, it is the probability that a given input belongs to the class $i$ coded by the output neuron.\n",
    "- `loss` (cross-entropy) $L(x_m) = -\\frac{1}{B}\\sum_{b=1}^B{Log \\left (\\frac{e^{x_m}}{\\sum_{n=1}^C e^{x_n}}\\right )}$. $m$ is the correct class for the $b^{th}$ input. $x_m$ is the output neuron activation for the correct class, $x_n$ is the output neuron activation for all of the classes (in the sum). The batch size is $B$, so the loss is averaged over each mini-batch of inputs. The expression in the $Log$ is just the softmax.\n",
    "- `gradient` (for softmax/cross-entropy)\n",
    "\n",
    "You're welcome to work in any order, but I recommend starting with `fit` because as you work though it, you should recognize why we need most of the other methods. You can finish `fit` or branch off as you need the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Test key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softmax_layer import SoftmaxLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some small Gaussian weights equal to the length of an image feature vector\n",
    "rng = np.random.default_rng(0)\n",
    "randWts = rng.normal(loc=0, scale=0.01, size=(x_dev.shape[1], 10))\n",
    "b = 1\n",
    "softmaxNet = SoftmaxLayer(-1)\n",
    "\n",
    "# Fake data for consistent debugging\n",
    "test_imgs = rng.random(size=(15, x_dev.shape[1])) - 0.5\n",
    "test_labels = rng.integers(low=0, high=6, size=(15,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `onehot` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your one hot vectors:\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y_test1 = np.array([1, 2, 2, 0, 1, 3])\n",
    "c_test = 4\n",
    "y_one_hot = softmaxNet.one_hot(y_test1, c_test)\n",
    "print(f'Your one hot vectors:\\n{y_one_hot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your one hot vectors should look like:\n",
    "\n",
    "    [[0. 1. 0. 0.]\n",
    "     [0. 0. 1. 0.]\n",
    "     [0. 0. 1. 0.]\n",
    "     [1. 0. 0. 0.]\n",
    "     [0. 1. 0. 0.]\n",
    "     [0. 0. 0. 1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `loss`,  `net_in`, softmax `activation` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net in shape=(15, 10), min=0.5846647992996103, max=1.4113966700992964\n",
      "Should be\n",
      "net in shape=(15, 10), min=0.584664799299611, max=1.411396670099296\n",
      "\n",
      "net act shape=(15, 10), min=0.06651346722629758, max=0.14392819816212585\n",
      "Should be\n",
      "net act shape=(15, 10), min=0.0665134672262976, max=0.1439281981621258\n",
      "\n",
      "The loss (without regularization) is 2.30 and it should approx be 2.30\n",
      "The loss (with 0.5 regularization) is 3.07 and it should approx be 3.07\n"
     ]
    }
   ],
   "source": [
    "lossNoReg, lossReg = softmaxNet.test_loss(randWts, b, test_imgs, test_labels)\n",
    "print(f'The loss (without regularization) is {lossNoReg:.2f} and it should approx be 2.30')\n",
    "print(f'The loss (with 0.5 regularization) is {lossReg:.2f} and it should approx be 3.07')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net in: (15, 10), 0.5846647992996103, 1.4113966700992964\n",
      "net in 1st few values of 1st input are:\n",
      "[0.798 1.095 0.969 0.9   0.958]\n",
      "and should be\n",
      "[0.798 1.095 0.969 0.9   0.958]\n",
      "net act 1st few values of 1st input are:\n",
      "[0.078 0.105 0.092 0.086 0.091]\n",
      "and should be\n",
      "[0.078 0.105 0.092 0.086 0.091]\n",
      "y one hot: (15, 10), sum is 15.0.\n",
      "You should know what the sum should be :)\n",
      "\n",
      "1st few Wt gradient values are [ 0.009 -0.047  0.088  0.01 ]\n",
      "and should be                  [ 0.009 -0.047  0.088  0.01 ] \n",
      "1st few Wt bias values are [-0.429  0.034  0.034 -0.098]\n",
      "and should be              [-0.429  0.034  0.034 -0.098]\n"
     ]
    }
   ],
   "source": [
    "grad_wts, grad_b = softmaxNet.test_gradient(randWts, b, test_imgs, test_labels, 10)\n",
    "print()\n",
    "print(f'1st few Wt gradient values are {grad_wts[:4,0]}\\nand should be                  [ 0.009 -0.047  0.088  0.01 ] ')\n",
    "print(f'1st few Wt bias values are {grad_b[:4]}\\nand should be              [-0.429  0.034  0.034 -0.098]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/600, Loss: 0.1881\n",
      "Iteration 200/600, Loss: 0.0926\n",
      "Iteration 300/600, Loss: 0.0596\n",
      "Iteration 400/600, Loss: 0.0428\n",
      "Iteration 500/600, Loss: 0.0315\n",
      "Iteration 600/600, Loss: 0.0254\n",
      "Training completed. 600 epochs, 1 iterations per epoch.\n"
     ]
    }
   ],
   "source": [
    "softmaxNet = SoftmaxLayer(10)\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev,\n",
    "                              n_epochs=600,\n",
    "                              mini_batch_sz=250,\n",
    "                              r_seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the random mini-batch sampling process, you may get different specific numbers, but the loss should generally decrease over iterations. You should get something like this:\n",
    "\n",
    "```\n",
    "Starting to train network...There will be 600 epochs and 600 iterations total, 1 iter/epoch.\n",
    "  Completed iter 0/600. Training loss: 2.37.\n",
    "  Completed iter 100/600. Training loss: 2.13.\n",
    "  Completed iter 200/600. Training loss: 2.01.\n",
    "  Completed iter 300/600. Training loss: 1.92.\n",
    "  Completed iter 400/600. Training loss: 1.87.\n",
    "  Completed iter 500/600. Training loss: 1.78.\n",
    "Finished training!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the loss\n",
    "\n",
    "It should look noisy, but decrease on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_cross_entropy_loss(loss_history):\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Training iteration')\n",
    "    plt.ylabel('loss (cross-entropy)')\n",
    "    plt.show()\n",
    "\n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** What do you think the decrease in loss over the epochs tells us about the state of the training process? What's the future potential for improvement like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47d60e151e7fd4c05473372588c9b311",
     "grade": true,
     "grade_id": "cell-e9b59c54f6d5a3a3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Decrease in loss continues to slow down as training continues, while loss goes from a very fast rate of improvement before slowing down and improving less with every iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question 2.** Below, write code to test regularization with training: check to make sure that the loss starts at ~155 for `reg=100`. Once you get this working, play around with the regularization parameter. You can drastically change the magnitude, but it should always remain nonnegative. \n",
    "\n",
    "(i) How does regularization affect the training loss and **why**?\n",
    "\n",
    "(ii) Make a plot that shows the ~155 initial loss for `reg=100`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/600, Loss: 85.7461\n",
      "Iteration 200/600, Loss: 87.4609\n",
      "Iteration 300/600, Loss: 88.5947\n",
      "Iteration 400/600, Loss: 87.3938\n",
      "Iteration 500/600, Loss: 87.6460\n",
      "Iteration 600/600, Loss: 89.2162\n",
      "Training completed. 600 epochs, 1 iterations per epoch.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test regularization\n",
    "\n",
    "softmaxNet = SoftmaxLayer(10)\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev,\n",
    "                              n_epochs=600,\n",
    "                              mini_batch_sz=250,\n",
    "                              r_seed=0, reg=1000)\n",
    "\n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44c4642eb9b9b1167db2bd0c5f8d8d70",
     "grade": true,
     "grade_id": "cell-f97644608496a921",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Since regularizaion effectively \"smooths out\" the loss curve, there is much less noise during training at higher regularization values, but as a result the initial and final loss are several orders of magnitude higher. At low regularization values, we observe much more noise along the curve as there is no regularization to smooth the results after every iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2a60c0d267b5c7c10aa78fc3ba886ff",
     "grade": false,
     "grade_id": "cell-951c6b67d237dd62",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/600, Loss: 96.9351\n",
      "Iteration 200/600, Loss: 88.2372\n",
      "Iteration 300/600, Loss: 87.0972\n",
      "Iteration 400/600, Loss: 87.0433\n",
      "Iteration 500/600, Loss: 87.0785\n",
      "Iteration 600/600, Loss: 86.6348\n",
      "Training completed. 600 epochs, 1 iterations per epoch.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "softmaxNet = SoftmaxLayer(10)\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev,\n",
    "                              n_epochs=600,\n",
    "                              mini_batch_sz=250,\n",
    "                              r_seed=0, reg=100)\n",
    "\n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Question 3.** Play around with the batch size parameter.\n",
    "\n",
    "(i) How does this affect the training loss and **why**? (*Think about the error gradient and how the weights change*)\n",
    "\n",
    "(ii) Make a plot that showcases the effect of large mini-batch size.\n",
    "\n",
    "(iii) Make a plot that showcases the effect of small mini-batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b2f634e1f3d2acc13917d05584889a3",
     "grade": true,
     "grade_id": "cell-f3d67c341804b455",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "A smaller batch results in a very very noisy curve due to much more frequent, but rougher updates to the gradient (since very small batches may not represent the entire data well). The weights also update more frequently, which can help escape local minima, but may result in a less-than-ideal loss. Smaller batches do however have the benefit of possibly converging faster due to the more frequent updates, and also require less memory.\n",
    "\n",
    "For larger batches, the opposite applies. The gradient and weight updates are slower, but each update is more accurate as larger batches represent the whole dataset more accurately. As a result, there is less extreme noise across the curve, and larger batches may converge more smoothly but slower, while using more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d730a79739bfc14f41ba3a4d4045413e",
     "grade": false,
     "grade_id": "cell-a2c3cf1ed832f667",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/600, Loss: 0.0878\n",
      "Iteration 200/600, Loss: 0.0430\n",
      "Iteration 300/600, Loss: 0.0259\n",
      "Iteration 400/600, Loss: 0.0197\n",
      "Iteration 500/600, Loss: 0.0153\n",
      "Iteration 600/600, Loss: 0.0128\n",
      "Training completed. 600 epochs, 1 iterations per epoch.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/3000, Loss: 0.8010\n",
      "Iteration 200/3000, Loss: 0.4625\n",
      "Iteration 300/3000, Loss: 0.3444\n",
      "Iteration 400/3000, Loss: 0.2176\n",
      "Iteration 500/3000, Loss: 0.1875\n",
      "Iteration 600/3000, Loss: 0.1938\n",
      "Iteration 700/3000, Loss: 0.1172\n",
      "Iteration 800/3000, Loss: 0.1210\n",
      "Iteration 900/3000, Loss: 0.1169\n",
      "Iteration 1000/3000, Loss: 0.1008\n",
      "Iteration 1100/3000, Loss: 0.0764\n",
      "Iteration 1200/3000, Loss: 0.0935\n",
      "Iteration 1300/3000, Loss: 0.0836\n",
      "Iteration 1400/3000, Loss: 0.0735\n",
      "Iteration 1500/3000, Loss: 0.0554\n",
      "Iteration 1600/3000, Loss: 0.0535\n",
      "Iteration 1700/3000, Loss: 0.0457\n",
      "Iteration 1800/3000, Loss: 0.0508\n",
      "Iteration 1900/3000, Loss: 0.0467\n",
      "Iteration 2000/3000, Loss: 0.0433\n",
      "Iteration 2100/3000, Loss: 0.0426\n",
      "Iteration 2200/3000, Loss: 0.0372\n",
      "Iteration 2300/3000, Loss: 0.0287\n",
      "Iteration 2400/3000, Loss: 0.0425\n",
      "Iteration 2500/3000, Loss: 0.0298\n",
      "Iteration 2600/3000, Loss: 0.0346\n",
      "Iteration 2700/3000, Loss: 0.0294\n",
      "Iteration 2800/3000, Loss: 0.0322\n",
      "Iteration 2900/3000, Loss: 0.0309\n",
      "Iteration 3000/3000, Loss: 0.0290\n",
      "Training completed. 600 epochs, 5 iterations per epoch.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "#large batch size\n",
    "softmaxNet = SoftmaxLayer(10)\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev,\n",
    "                              n_epochs=600,\n",
    "                              mini_batch_sz=500,\n",
    "                              r_seed=0)\n",
    "\n",
    "plot_cross_entropy_loss(loss_history)\n",
    "\n",
    "\n",
    "#small batch size\n",
    "softmaxNet = SoftmaxLayer(10)\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev,\n",
    "                              n_epochs=600,\n",
    "                              mini_batch_sz=50,\n",
    "                              r_seed=0)\n",
    "\n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Retraining the same net\n",
    "\n",
    "Train 7 fresh softmax networks with default hyperparameters, except for `verbose=0`. Use the actual training set to train the networks rather than the dev set. Each time, compute and record the accuracy on the test set. After the 7 training sessions are over, print out all 7 accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61b5a824d12c3d53100ad167eac71ea0",
     "grade": false,
     "grade_id": "cell-b1a3d8ffda9bc8f2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network 1/7\n",
      "Network 1 test accuracy: 0.3213\n",
      "Training network 2/7\n",
      "Network 2 test accuracy: 0.3400\n",
      "Training network 3/7\n",
      "Network 3 test accuracy: 0.3053\n",
      "Training network 4/7\n",
      "Network 4 test accuracy: 0.3280\n",
      "Training network 5/7\n",
      "Network 5 test accuracy: 0.3160\n",
      "Training network 6/7\n",
      "Network 6 test accuracy: 0.3280\n",
      "Training network 7/7\n",
      "Network 7 test accuracy: 0.3227\n",
      "\n",
      "All accuracies:\n",
      "Network 1: 0.3213\n",
      "Network 2: 0.3400\n",
      "Network 3: 0.3053\n",
      "Network 4: 0.3280\n",
      "Network 5: 0.3160\n",
      "Network 6: 0.3280\n",
      "Network 7: 0.3227\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "def train_multiple_softmax_networks(X_train, y_train, X_test, y_test, num_networks=7, num_classes=10):\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(num_networks):\n",
    "        print(f\"Training network {i+1}/{num_networks}\")\n",
    "        \n",
    "        # Initialize a new SoftmaxLayer\n",
    "        network = SoftmaxLayer(num_classes)\n",
    "        \n",
    "        # Train the network\n",
    "        network.fit(X_train, y_train, verbose=0)\n",
    "        \n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred = network.predict(X_test)\n",
    "        \n",
    "        # Compute and record the accuracy\n",
    "        accuracy = network.accuracy(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Network {i+1} test accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nAll accuracies:\")\n",
    "    for i, acc in enumerate(accuracies, 1):\n",
    "        print(f\"Network {i}: {acc:.4f}\")\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "accuracies = train_multiple_softmax_networks(x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f. Questions\n",
    "\n",
    "**Question: 4:** What do you notice about the accuracies? Is this what you expected? Why does this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e81d1241c4546b6b652beb8f0bc8b881",
     "grade": true,
     "grade_id": "cell-9fb842e1b3831f68",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Accuracy is rather low across the board and very consistent despite random initializations (which are proven random by the slight discrepancies in accuracies). The hyperparameters chosen, such as learning rate, regularization, batch size, and epochs all play a part in determining accuracy. As such, I did not expect the defaults to necessarily produce the best results, even if running it multiple times with slight deviations due to randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2g. Train and optimize STL-10 dataset performance\n",
    "\n",
    "As you've surely noticed, hyperparameters can drastically affect learning! \n",
    "\n",
    "\n",
    "Implement a grid search for the best **combinations** of hyperparameters\n",
    "\n",
    "- learning rate,\n",
    "- regularization\n",
    "- batch size \n",
    "\n",
    "Fix the training duration to `75` epochs.\n",
    "\n",
    "The grid search process should:\n",
    "\n",
    "1. Fit the model with specific values of hyperparameters that we're testing (using the training set).\n",
    "2. Compute the accuracy on the **training set**. \n",
    "3. Compute the accuracy on the **validation set.** \n",
    "4. Print out and record the best parameter combination as you go (that *improves* the **validation set accuracy**).\n",
    "5. Wipe the weights clean (reinitialize them) every time you try new parameters. It's easiest just to create a new net object on each run.\n",
    "\n",
    "Configure your search *in the way that we prefer from class* so that you get reproducible accuracy. For example, if your search prints out that the best hyperparameters give X% accuracy in the search, training a net with those same hyperparameters after the search is completed also gives you X% accuracy.\n",
    "\n",
    "#### Suggestions\n",
    "\n",
    "This can take quite a bit of simulation time! Here are some tips:\n",
    "- I suggest using a coarse-to-fine search strategy. Try varying hyperparameters over many orders of magnitude, then follow up with a 2nd search that \"zooms in\" around the hyperparameters that gave you the best results from the first search, as determined by your print outs. You can be informal about this and hard-code the coarse and fine hyperparameter choices. Abort searches prematurely if you feel there aren't productive (no reason to wait!). This can take however long or short that you want to dedicate. Remember, you are printing out the best parameter values on each run, so you can always just proceed with those.\n",
    "- High learning rates don't really make sense. You'll know if your value is \"high\" if numpy complains about numerical issues.\n",
    "- Your mini-batch sizes should be `<= N` and `>= 1`.\n",
    "- Time single network runs with a few different batch sizes you plan on trying in your big search. This will help you figure out a ballpark estimate how long grid search will take (*you can decide whether to go eat dinner, run it overnight, etc.*). If it will take an unreasonable amount of time, reduce the number of parameters you try in one search.\n",
    "- Think about whether you need *3 nested loops* or *a sequence of single loops*.\n",
    "- Turn off print outs from `fit` (adjust `verbose` argument) and only print out things related to your search.\n",
    "- Feel free to try a search with more than 75 training epochs for potentially better accuracy, but note that the search will take longer to complete.\n",
    "\n",
    "**Important note:** I am not grading based on the number of hours your computer spends searching. I want to see that you successfully implemented the grid search to find progressively better hyperparameters on STL-10 and use the outcome to inform your ultimate training session that you use to evaluate predictions on the test set. You should be able to achieve ~30% accuracy without too much effort. *Getting full credit here does not require you spending hours of searching!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30011c48c63181e4b0095147b433c7b1",
     "grade": false,
     "grade_id": "cell-fe3760a80f4af169",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/28125, Loss: 1.8595\n",
      "Iteration 200/28125, Loss: 1.8634\n",
      "Iteration 300/28125, Loss: 2.0717\n",
      "Iteration 400/28125, Loss: 2.4736\n",
      "Iteration 500/28125, Loss: 1.8111\n",
      "Iteration 600/28125, Loss: 1.2603\n",
      "Iteration 700/28125, Loss: 1.6679\n",
      "Iteration 800/28125, Loss: 1.7684\n",
      "Iteration 900/28125, Loss: 1.5445\n",
      "Iteration 1000/28125, Loss: 1.7632\n",
      "Iteration 1100/28125, Loss: 1.4726\n",
      "Iteration 1200/28125, Loss: 1.1918\n",
      "Iteration 1300/28125, Loss: 2.0740\n",
      "Iteration 1400/28125, Loss: 1.2419\n",
      "Iteration 1500/28125, Loss: 1.2564\n",
      "Iteration 1600/28125, Loss: 1.3445\n",
      "Iteration 1700/28125, Loss: 2.1064\n",
      "Iteration 1800/28125, Loss: 1.6153\n",
      "Iteration 1900/28125, Loss: 1.0887\n",
      "Iteration 2000/28125, Loss: 1.6504\n",
      "Iteration 2100/28125, Loss: 1.5239\n",
      "Iteration 2200/28125, Loss: 1.4765\n",
      "Iteration 2300/28125, Loss: 1.5754\n",
      "Iteration 2400/28125, Loss: 1.7352\n",
      "Iteration 2500/28125, Loss: 1.1495\n",
      "Iteration 2600/28125, Loss: 1.6986\n",
      "Iteration 2700/28125, Loss: 1.6046\n",
      "Iteration 2800/28125, Loss: 1.7368\n",
      "Iteration 2900/28125, Loss: 1.6550\n",
      "Iteration 3000/28125, Loss: 1.5959\n",
      "Iteration 3100/28125, Loss: 1.8395\n",
      "Iteration 3200/28125, Loss: 1.4950\n",
      "Iteration 3300/28125, Loss: 1.0345\n",
      "Iteration 3400/28125, Loss: 1.3680\n",
      "Iteration 3500/28125, Loss: 1.0899\n",
      "Iteration 3600/28125, Loss: 1.6204\n",
      "Iteration 3700/28125, Loss: 1.5339\n",
      "Iteration 3800/28125, Loss: 1.3167\n",
      "Iteration 3900/28125, Loss: 0.9635\n",
      "Iteration 4000/28125, Loss: 1.8012\n",
      "Iteration 4100/28125, Loss: 1.1495\n",
      "Iteration 4200/28125, Loss: 1.8738\n",
      "Iteration 4300/28125, Loss: 1.1154\n",
      "Iteration 4400/28125, Loss: 1.2866\n",
      "Iteration 4500/28125, Loss: 1.7288\n",
      "Iteration 4600/28125, Loss: 1.4826\n",
      "Iteration 4700/28125, Loss: 1.2898\n",
      "Iteration 4800/28125, Loss: 2.1968\n",
      "Iteration 4900/28125, Loss: 1.2921\n",
      "Iteration 5000/28125, Loss: 1.2065\n",
      "Iteration 5100/28125, Loss: 1.3540\n",
      "Iteration 5200/28125, Loss: 1.4565\n",
      "Iteration 5300/28125, Loss: 1.4118\n",
      "Iteration 5400/28125, Loss: 1.2785\n",
      "Iteration 5500/28125, Loss: 2.2155\n",
      "Iteration 5600/28125, Loss: 1.1851\n",
      "Iteration 5700/28125, Loss: 1.6256\n",
      "Iteration 5800/28125, Loss: 1.4308\n",
      "Iteration 5900/28125, Loss: 0.5326\n",
      "Iteration 6000/28125, Loss: 0.7919\n",
      "Iteration 6100/28125, Loss: 0.9622\n",
      "Iteration 6200/28125, Loss: 1.5847\n",
      "Iteration 6300/28125, Loss: 1.1968\n",
      "Iteration 6400/28125, Loss: 1.1716\n",
      "Iteration 6500/28125, Loss: 0.9257\n",
      "Iteration 6600/28125, Loss: 1.2306\n",
      "Iteration 6700/28125, Loss: 1.4377\n",
      "Iteration 6800/28125, Loss: 1.2407\n",
      "Iteration 6900/28125, Loss: 1.1940\n",
      "Iteration 7000/28125, Loss: 0.9229\n",
      "Iteration 7100/28125, Loss: 1.1051\n",
      "Iteration 7200/28125, Loss: 0.7451\n",
      "Iteration 7300/28125, Loss: 1.4508\n",
      "Iteration 7400/28125, Loss: 1.4070\n",
      "Iteration 7500/28125, Loss: 1.5559\n",
      "Iteration 7600/28125, Loss: 1.4449\n",
      "Iteration 7700/28125, Loss: 1.6489\n",
      "Iteration 7800/28125, Loss: 1.0259\n",
      "Iteration 7900/28125, Loss: 1.5628\n",
      "Iteration 8000/28125, Loss: 1.2884\n",
      "Iteration 8100/28125, Loss: 0.9864\n",
      "Iteration 8200/28125, Loss: 1.2757\n",
      "Iteration 8300/28125, Loss: 1.2255\n",
      "Iteration 8400/28125, Loss: 1.0314\n",
      "Iteration 8500/28125, Loss: 1.0065\n",
      "Iteration 8600/28125, Loss: 1.4619\n",
      "Iteration 8700/28125, Loss: 0.7088\n",
      "Iteration 8800/28125, Loss: 1.2890\n",
      "Iteration 8900/28125, Loss: 1.6669\n",
      "Iteration 9000/28125, Loss: 1.2235\n",
      "Iteration 9100/28125, Loss: 0.8086\n",
      "Iteration 9200/28125, Loss: 1.3002\n",
      "Iteration 9300/28125, Loss: 0.9395\n",
      "Iteration 9400/28125, Loss: 0.6761\n",
      "Iteration 9500/28125, Loss: 0.9539\n",
      "Iteration 9600/28125, Loss: 1.1944\n",
      "Iteration 9700/28125, Loss: 1.2042\n",
      "Iteration 9800/28125, Loss: 1.5779\n",
      "Iteration 9900/28125, Loss: 1.4710\n",
      "Iteration 10000/28125, Loss: 1.0468\n",
      "Iteration 10100/28125, Loss: 1.3183\n",
      "Iteration 10200/28125, Loss: 0.9939\n",
      "Iteration 10300/28125, Loss: 0.7893\n",
      "Iteration 10400/28125, Loss: 1.4845\n",
      "Iteration 10500/28125, Loss: 1.1807\n",
      "Iteration 10600/28125, Loss: 1.1837\n",
      "Iteration 10700/28125, Loss: 1.0035\n",
      "Iteration 10800/28125, Loss: 0.9095\n",
      "Iteration 10900/28125, Loss: 0.5744\n",
      "Iteration 11000/28125, Loss: 0.8671\n",
      "Iteration 11100/28125, Loss: 0.9834\n",
      "Iteration 11200/28125, Loss: 1.0393\n",
      "Iteration 11300/28125, Loss: 1.0078\n",
      "Iteration 11400/28125, Loss: 0.9301\n",
      "Iteration 11500/28125, Loss: 1.5271\n",
      "Iteration 11600/28125, Loss: 0.8299\n",
      "Iteration 11700/28125, Loss: 1.0627\n",
      "Iteration 11800/28125, Loss: 1.2507\n",
      "Iteration 11900/28125, Loss: 0.7062\n",
      "Iteration 12000/28125, Loss: 1.1868\n",
      "Iteration 12100/28125, Loss: 1.5696\n",
      "Iteration 12200/28125, Loss: 1.0006\n",
      "Iteration 12300/28125, Loss: 1.0126\n",
      "Iteration 12400/28125, Loss: 1.9338\n",
      "Iteration 12500/28125, Loss: 1.3610\n",
      "Iteration 12600/28125, Loss: 1.0481\n",
      "Iteration 12700/28125, Loss: 1.3796\n",
      "Iteration 12800/28125, Loss: 1.0952\n",
      "Iteration 12900/28125, Loss: 0.6567\n",
      "Iteration 13000/28125, Loss: 1.1726\n",
      "Iteration 13100/28125, Loss: 0.7473\n",
      "Iteration 13200/28125, Loss: 1.4996\n",
      "Iteration 13300/28125, Loss: 1.2278\n",
      "Iteration 13400/28125, Loss: 1.1148\n",
      "Iteration 13500/28125, Loss: 0.7642\n",
      "Iteration 13600/28125, Loss: 0.9206\n",
      "Iteration 13700/28125, Loss: 1.0168\n",
      "Iteration 13800/28125, Loss: 0.6379\n",
      "Iteration 13900/28125, Loss: 0.5527\n",
      "Iteration 14000/28125, Loss: 1.3246\n",
      "Iteration 14100/28125, Loss: 0.6969\n",
      "Iteration 14200/28125, Loss: 0.7807\n",
      "Iteration 14300/28125, Loss: 1.1306\n",
      "Iteration 14400/28125, Loss: 1.4154\n",
      "Iteration 14500/28125, Loss: 1.3309\n",
      "Iteration 14600/28125, Loss: 1.0799\n",
      "Iteration 14700/28125, Loss: 0.9568\n",
      "Iteration 14800/28125, Loss: 1.0814\n",
      "Iteration 14900/28125, Loss: 0.9908\n",
      "Iteration 15000/28125, Loss: 1.0357\n",
      "Iteration 15100/28125, Loss: 0.8453\n",
      "Iteration 15200/28125, Loss: 1.1301\n",
      "Iteration 15300/28125, Loss: 1.3620\n",
      "Iteration 15400/28125, Loss: 1.3340\n",
      "Iteration 15500/28125, Loss: 0.5793\n",
      "Iteration 15600/28125, Loss: 1.2705\n",
      "Iteration 15700/28125, Loss: 0.7077\n",
      "Iteration 15800/28125, Loss: 0.9962\n",
      "Iteration 15900/28125, Loss: 1.0896\n",
      "Iteration 16000/28125, Loss: 0.6098\n",
      "Iteration 16100/28125, Loss: 1.1329\n",
      "Iteration 16200/28125, Loss: 1.3171\n",
      "Iteration 16300/28125, Loss: 0.5678\n",
      "Iteration 16400/28125, Loss: 0.9084\n",
      "Iteration 16500/28125, Loss: 1.2400\n",
      "Iteration 16600/28125, Loss: 0.9152\n",
      "Iteration 16700/28125, Loss: 0.9134\n",
      "Iteration 16800/28125, Loss: 1.1201\n",
      "Iteration 16900/28125, Loss: 0.9993\n",
      "Iteration 17000/28125, Loss: 1.0567\n",
      "Iteration 17100/28125, Loss: 0.8215\n",
      "Iteration 17200/28125, Loss: 0.8188\n",
      "Iteration 17300/28125, Loss: 1.2412\n",
      "Iteration 17400/28125, Loss: 1.0573\n",
      "Iteration 17500/28125, Loss: 0.9492\n",
      "Iteration 17600/28125, Loss: 0.9610\n",
      "Iteration 17700/28125, Loss: 1.2420\n",
      "Iteration 17800/28125, Loss: 0.8195\n",
      "Iteration 17900/28125, Loss: 0.7549\n",
      "Iteration 18000/28125, Loss: 1.0725\n",
      "Iteration 18100/28125, Loss: 1.3623\n",
      "Iteration 18200/28125, Loss: 1.1147\n",
      "Iteration 18300/28125, Loss: 0.7487\n",
      "Iteration 18400/28125, Loss: 0.7497\n",
      "Iteration 18500/28125, Loss: 1.6247\n",
      "Iteration 18600/28125, Loss: 1.0526\n",
      "Iteration 18700/28125, Loss: 0.8468\n",
      "Iteration 18800/28125, Loss: 0.9540\n",
      "Iteration 18900/28125, Loss: 0.8340\n",
      "Iteration 19000/28125, Loss: 1.2044\n",
      "Iteration 19100/28125, Loss: 0.7753\n",
      "Iteration 19200/28125, Loss: 0.8000\n",
      "Iteration 19300/28125, Loss: 0.5673\n",
      "Iteration 19400/28125, Loss: 1.1575\n",
      "Iteration 19500/28125, Loss: 1.0199\n",
      "Iteration 19600/28125, Loss: 1.0509\n",
      "Iteration 19700/28125, Loss: 0.7921\n",
      "Iteration 19800/28125, Loss: 1.1155\n",
      "Iteration 19900/28125, Loss: 0.9191\n",
      "Iteration 20000/28125, Loss: 0.9046\n",
      "Iteration 20100/28125, Loss: 1.0063\n",
      "Iteration 20200/28125, Loss: 1.3851\n",
      "Iteration 20300/28125, Loss: 0.7613\n",
      "Iteration 20400/28125, Loss: 1.2551\n",
      "Iteration 20500/28125, Loss: 0.7888\n",
      "Iteration 20600/28125, Loss: 0.8388\n",
      "Iteration 20700/28125, Loss: 0.8865\n",
      "Iteration 20800/28125, Loss: 0.7798\n",
      "Iteration 20900/28125, Loss: 0.7579\n",
      "Iteration 21000/28125, Loss: 0.7606\n",
      "Iteration 21100/28125, Loss: 0.8359\n",
      "Iteration 21200/28125, Loss: 1.2460\n",
      "Iteration 21300/28125, Loss: 0.6059\n",
      "Iteration 21400/28125, Loss: 1.2158\n",
      "Iteration 21500/28125, Loss: 1.0630\n",
      "Iteration 21600/28125, Loss: 1.2718\n",
      "Iteration 21700/28125, Loss: 0.6834\n",
      "Iteration 21800/28125, Loss: 0.7312\n",
      "Iteration 21900/28125, Loss: 0.5735\n",
      "Iteration 22000/28125, Loss: 0.3149\n",
      "Iteration 22100/28125, Loss: 1.0405\n",
      "Iteration 22200/28125, Loss: 0.5819\n",
      "Iteration 22300/28125, Loss: 0.7914\n",
      "Iteration 22400/28125, Loss: 1.0179\n",
      "Iteration 22500/28125, Loss: 1.4733\n",
      "Iteration 22600/28125, Loss: 0.6315\n",
      "Iteration 22700/28125, Loss: 0.7174\n",
      "Iteration 22800/28125, Loss: 0.9874\n",
      "Iteration 22900/28125, Loss: 1.1595\n",
      "Iteration 23000/28125, Loss: 0.9234\n",
      "Iteration 23100/28125, Loss: 1.2008\n",
      "Iteration 23200/28125, Loss: 0.6076\n",
      "Iteration 23300/28125, Loss: 0.8676\n",
      "Iteration 23400/28125, Loss: 0.4419\n",
      "Iteration 23500/28125, Loss: 0.6702\n",
      "Iteration 23600/28125, Loss: 1.2273\n",
      "Iteration 23700/28125, Loss: 0.3508\n",
      "Iteration 23800/28125, Loss: 0.7419\n",
      "Iteration 23900/28125, Loss: 1.2158\n",
      "Iteration 24000/28125, Loss: 0.9623\n",
      "Iteration 24100/28125, Loss: 0.6379\n",
      "Iteration 24200/28125, Loss: 0.5388\n",
      "Iteration 24300/28125, Loss: 1.1107\n",
      "Iteration 24400/28125, Loss: 0.7558\n",
      "Iteration 24500/28125, Loss: 1.1560\n",
      "Iteration 24600/28125, Loss: 0.8783\n",
      "Iteration 24700/28125, Loss: 1.1820\n",
      "Iteration 24800/28125, Loss: 1.0232\n",
      "Iteration 24900/28125, Loss: 0.7053\n",
      "Iteration 25000/28125, Loss: 0.8727\n",
      "Iteration 25100/28125, Loss: 0.6156\n",
      "Iteration 25200/28125, Loss: 0.9782\n",
      "Iteration 25300/28125, Loss: 0.5020\n",
      "Iteration 25400/28125, Loss: 0.7903\n",
      "Iteration 25500/28125, Loss: 0.4957\n",
      "Iteration 25600/28125, Loss: 0.8078\n",
      "Iteration 25700/28125, Loss: 0.7111\n",
      "Iteration 25800/28125, Loss: 0.8063\n",
      "Iteration 25900/28125, Loss: 1.1467\n",
      "Iteration 26000/28125, Loss: 1.1267\n",
      "Iteration 26100/28125, Loss: 0.9073\n",
      "Iteration 26200/28125, Loss: 1.0052\n",
      "Iteration 26300/28125, Loss: 0.6527\n",
      "Iteration 26400/28125, Loss: 1.2283\n",
      "Iteration 26500/28125, Loss: 0.3881\n",
      "Iteration 26600/28125, Loss: 0.9456\n",
      "Iteration 26700/28125, Loss: 1.2447\n",
      "Iteration 26800/28125, Loss: 0.7858\n",
      "Iteration 26900/28125, Loss: 0.9052\n",
      "Iteration 27000/28125, Loss: 1.2855\n",
      "Iteration 27100/28125, Loss: 0.8149\n",
      "Iteration 27200/28125, Loss: 0.9809\n",
      "Iteration 27300/28125, Loss: 0.7526\n",
      "Iteration 27400/28125, Loss: 0.4547\n",
      "Iteration 27500/28125, Loss: 1.0341\n",
      "Iteration 27600/28125, Loss: 0.7656\n",
      "Iteration 27700/28125, Loss: 0.6877\n",
      "Iteration 27800/28125, Loss: 0.5978\n",
      "Iteration 27900/28125, Loss: 0.8127\n",
      "Iteration 28000/28125, Loss: 0.4755\n",
      "Iteration 28100/28125, Loss: 0.8109\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "train accuracy: 0.7676666666666667\n",
      "val accuracy: 0.325\n",
      "batch_size: 8 reg: 0.0001 rate: 0.0001\n",
      "Iteration 100/14025, Loss: 1.9163\n",
      "Iteration 200/14025, Loss: 2.1453\n",
      "Iteration 300/14025, Loss: 1.5167\n",
      "Iteration 400/14025, Loss: 1.9443\n",
      "Iteration 500/14025, Loss: 1.6593\n",
      "Iteration 600/14025, Loss: 1.3267\n",
      "Iteration 700/14025, Loss: 1.4075\n",
      "Iteration 800/14025, Loss: 1.4624\n",
      "Iteration 900/14025, Loss: 1.1822\n",
      "Iteration 1000/14025, Loss: 1.6087\n",
      "Iteration 1100/14025, Loss: 1.4375\n",
      "Iteration 1200/14025, Loss: 1.3880\n",
      "Iteration 1300/14025, Loss: 1.3824\n",
      "Iteration 1400/14025, Loss: 1.4495\n",
      "Iteration 1500/14025, Loss: 1.7391\n",
      "Iteration 1600/14025, Loss: 1.5352\n",
      "Iteration 1700/14025, Loss: 1.4004\n",
      "Iteration 1800/14025, Loss: 1.5377\n",
      "Iteration 1900/14025, Loss: 1.2276\n",
      "Iteration 2000/14025, Loss: 1.5332\n",
      "Iteration 2100/14025, Loss: 1.6874\n",
      "Iteration 2200/14025, Loss: 1.4284\n",
      "Iteration 2300/14025, Loss: 1.7087\n",
      "Iteration 2400/14025, Loss: 1.8136\n",
      "Iteration 2500/14025, Loss: 1.5447\n",
      "Iteration 2600/14025, Loss: 1.2457\n",
      "Iteration 2700/14025, Loss: 1.2141\n",
      "Iteration 2800/14025, Loss: 1.1910\n",
      "Iteration 2900/14025, Loss: 1.1512\n",
      "Iteration 3000/14025, Loss: 0.8611\n",
      "Iteration 3100/14025, Loss: 1.5111\n",
      "Iteration 3200/14025, Loss: 1.2288\n",
      "Iteration 3300/14025, Loss: 1.1099\n",
      "Iteration 3400/14025, Loss: 1.4690\n",
      "Iteration 3500/14025, Loss: 1.1365\n",
      "Iteration 3600/14025, Loss: 1.1223\n",
      "Iteration 3700/14025, Loss: 1.1975\n",
      "Iteration 3800/14025, Loss: 1.3536\n",
      "Iteration 3900/14025, Loss: 1.0537\n",
      "Iteration 4000/14025, Loss: 1.3330\n",
      "Iteration 4100/14025, Loss: 1.1472\n",
      "Iteration 4200/14025, Loss: 1.3028\n",
      "Iteration 4300/14025, Loss: 1.1989\n",
      "Iteration 4400/14025, Loss: 1.2632\n",
      "Iteration 4500/14025, Loss: 1.1440\n",
      "Iteration 4600/14025, Loss: 1.2273\n",
      "Iteration 4700/14025, Loss: 0.9283\n",
      "Iteration 4800/14025, Loss: 1.0499\n",
      "Iteration 4900/14025, Loss: 1.1424\n",
      "Iteration 5000/14025, Loss: 1.0889\n",
      "Iteration 5100/14025, Loss: 0.9213\n",
      "Iteration 5200/14025, Loss: 1.7332\n",
      "Iteration 5300/14025, Loss: 1.3092\n",
      "Iteration 5400/14025, Loss: 0.9642\n",
      "Iteration 5500/14025, Loss: 0.9739\n",
      "Iteration 5600/14025, Loss: 1.0812\n",
      "Iteration 5700/14025, Loss: 0.8995\n",
      "Iteration 5800/14025, Loss: 1.0360\n",
      "Iteration 5900/14025, Loss: 1.3871\n",
      "Iteration 6000/14025, Loss: 1.1184\n",
      "Iteration 6100/14025, Loss: 1.0845\n",
      "Iteration 6200/14025, Loss: 1.3306\n",
      "Iteration 6300/14025, Loss: 1.0918\n",
      "Iteration 6400/14025, Loss: 1.1762\n",
      "Iteration 6500/14025, Loss: 1.0077\n",
      "Iteration 6600/14025, Loss: 1.4872\n",
      "Iteration 6700/14025, Loss: 0.8725\n",
      "Iteration 6800/14025, Loss: 0.9627\n",
      "Iteration 6900/14025, Loss: 0.9143\n",
      "Iteration 7000/14025, Loss: 1.1105\n",
      "Iteration 7100/14025, Loss: 0.7353\n",
      "Iteration 7200/14025, Loss: 1.2160\n",
      "Iteration 7300/14025, Loss: 0.9107\n",
      "Iteration 7400/14025, Loss: 0.9315\n",
      "Iteration 7500/14025, Loss: 1.3739\n",
      "Iteration 7600/14025, Loss: 1.1593\n",
      "Iteration 7700/14025, Loss: 1.4115\n",
      "Iteration 7800/14025, Loss: 1.3239\n",
      "Iteration 7900/14025, Loss: 0.7973\n",
      "Iteration 8000/14025, Loss: 1.0440\n",
      "Iteration 8100/14025, Loss: 1.1369\n",
      "Iteration 8200/14025, Loss: 1.1919\n",
      "Iteration 8300/14025, Loss: 1.1036\n",
      "Iteration 8400/14025, Loss: 0.9912\n",
      "Iteration 8500/14025, Loss: 0.9539\n",
      "Iteration 8600/14025, Loss: 0.7360\n",
      "Iteration 8700/14025, Loss: 0.9983\n",
      "Iteration 8800/14025, Loss: 0.8043\n",
      "Iteration 8900/14025, Loss: 0.8544\n",
      "Iteration 9000/14025, Loss: 1.0455\n",
      "Iteration 9100/14025, Loss: 1.2484\n",
      "Iteration 9200/14025, Loss: 0.6893\n",
      "Iteration 9300/14025, Loss: 0.9783\n",
      "Iteration 9400/14025, Loss: 0.8734\n",
      "Iteration 9500/14025, Loss: 1.0457\n",
      "Iteration 9600/14025, Loss: 0.8153\n",
      "Iteration 9700/14025, Loss: 0.9509\n",
      "Iteration 9800/14025, Loss: 0.8980\n",
      "Iteration 9900/14025, Loss: 1.0314\n",
      "Iteration 10000/14025, Loss: 1.0199\n",
      "Iteration 10100/14025, Loss: 0.8871\n",
      "Iteration 10200/14025, Loss: 1.1589\n",
      "Iteration 10300/14025, Loss: 0.8818\n",
      "Iteration 10400/14025, Loss: 0.7638\n",
      "Iteration 10500/14025, Loss: 0.7723\n",
      "Iteration 10600/14025, Loss: 1.0281\n",
      "Iteration 10700/14025, Loss: 1.3001\n",
      "Iteration 10800/14025, Loss: 1.0198\n",
      "Iteration 10900/14025, Loss: 0.8583\n",
      "Iteration 11000/14025, Loss: 0.5421\n",
      "Iteration 11100/14025, Loss: 0.8850\n",
      "Iteration 11200/14025, Loss: 1.0965\n",
      "Iteration 11300/14025, Loss: 0.7444\n",
      "Iteration 11400/14025, Loss: 1.1079\n",
      "Iteration 11500/14025, Loss: 1.0634\n",
      "Iteration 11600/14025, Loss: 0.6222\n",
      "Iteration 11700/14025, Loss: 0.7292\n",
      "Iteration 11800/14025, Loss: 1.1990\n",
      "Iteration 11900/14025, Loss: 0.8111\n",
      "Iteration 12000/14025, Loss: 0.9755\n",
      "Iteration 12100/14025, Loss: 0.5948\n",
      "Iteration 12200/14025, Loss: 0.6976\n",
      "Iteration 12300/14025, Loss: 0.7983\n",
      "Iteration 12400/14025, Loss: 1.0356\n",
      "Iteration 12500/14025, Loss: 0.8967\n",
      "Iteration 12600/14025, Loss: 1.0322\n",
      "Iteration 12700/14025, Loss: 0.7303\n",
      "Iteration 12800/14025, Loss: 0.9661\n",
      "Iteration 12900/14025, Loss: 0.8794\n",
      "Iteration 13000/14025, Loss: 1.0876\n",
      "Iteration 13100/14025, Loss: 0.8763\n",
      "Iteration 13200/14025, Loss: 0.8098\n",
      "Iteration 13300/14025, Loss: 0.6356\n",
      "Iteration 13400/14025, Loss: 0.9672\n",
      "Iteration 13500/14025, Loss: 0.9344\n",
      "Iteration 13600/14025, Loss: 0.9930\n",
      "Iteration 13700/14025, Loss: 0.5444\n",
      "Iteration 13800/14025, Loss: 0.7590\n",
      "Iteration 13900/14025, Loss: 0.5547\n",
      "Iteration 14000/14025, Loss: 0.7302\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "train accuracy: 0.769\n",
      "val accuracy: 0.329\n",
      "batch_size: 16 reg: 0.0001 rate: 0.0001\n",
      "Iteration 100/6975, Loss: 1.8193\n",
      "Iteration 200/6975, Loss: 1.9486\n",
      "Iteration 300/6975, Loss: 1.5153\n",
      "Iteration 400/6975, Loss: 1.5620\n",
      "Iteration 500/6975, Loss: 1.5372\n",
      "Iteration 600/6975, Loss: 1.5486\n",
      "Iteration 700/6975, Loss: 1.5304\n",
      "Iteration 800/6975, Loss: 1.5356\n",
      "Iteration 900/6975, Loss: 1.4057\n",
      "Iteration 1000/6975, Loss: 1.4580\n",
      "Iteration 1100/6975, Loss: 1.4317\n",
      "Iteration 1200/6975, Loss: 1.3815\n",
      "Iteration 1300/6975, Loss: 1.2517\n",
      "Iteration 1400/6975, Loss: 1.3756\n",
      "Iteration 1500/6975, Loss: 1.1187\n",
      "Iteration 1600/6975, Loss: 1.1719\n",
      "Iteration 1700/6975, Loss: 1.1535\n",
      "Iteration 1800/6975, Loss: 1.0991\n",
      "Iteration 1900/6975, Loss: 1.3207\n",
      "Iteration 2000/6975, Loss: 1.2309\n",
      "Iteration 2100/6975, Loss: 1.3303\n",
      "Iteration 2200/6975, Loss: 1.1233\n",
      "Iteration 2300/6975, Loss: 1.2210\n",
      "Iteration 2400/6975, Loss: 1.1305\n",
      "Iteration 2500/6975, Loss: 1.1926\n",
      "Iteration 2600/6975, Loss: 1.3571\n",
      "Iteration 2700/6975, Loss: 0.9863\n",
      "Iteration 2800/6975, Loss: 1.1359\n",
      "Iteration 2900/6975, Loss: 1.2359\n",
      "Iteration 3000/6975, Loss: 1.1107\n",
      "Iteration 3100/6975, Loss: 1.1280\n",
      "Iteration 3200/6975, Loss: 1.1927\n",
      "Iteration 3300/6975, Loss: 1.3005\n",
      "Iteration 3400/6975, Loss: 0.9194\n",
      "Iteration 3500/6975, Loss: 0.8655\n",
      "Iteration 3600/6975, Loss: 1.2165\n",
      "Iteration 3700/6975, Loss: 0.9687\n",
      "Iteration 3800/6975, Loss: 1.1650\n",
      "Iteration 3900/6975, Loss: 1.3358\n",
      "Iteration 4000/6975, Loss: 1.1200\n",
      "Iteration 4100/6975, Loss: 0.9263\n",
      "Iteration 4200/6975, Loss: 0.9619\n",
      "Iteration 4300/6975, Loss: 0.8452\n",
      "Iteration 4400/6975, Loss: 1.0233\n",
      "Iteration 4500/6975, Loss: 1.0326\n",
      "Iteration 4600/6975, Loss: 0.9530\n",
      "Iteration 4700/6975, Loss: 0.9340\n",
      "Iteration 4800/6975, Loss: 0.8830\n",
      "Iteration 4900/6975, Loss: 0.9852\n",
      "Iteration 5000/6975, Loss: 0.9963\n",
      "Iteration 5100/6975, Loss: 0.9869\n",
      "Iteration 5200/6975, Loss: 0.9479\n",
      "Iteration 5300/6975, Loss: 0.8322\n",
      "Iteration 5400/6975, Loss: 0.9797\n",
      "Iteration 5500/6975, Loss: 0.9578\n",
      "Iteration 5600/6975, Loss: 0.8736\n",
      "Iteration 5700/6975, Loss: 0.9363\n",
      "Iteration 5800/6975, Loss: 0.7315\n",
      "Iteration 5900/6975, Loss: 0.9679\n",
      "Iteration 6000/6975, Loss: 0.9490\n",
      "Iteration 6100/6975, Loss: 0.7842\n",
      "Iteration 6200/6975, Loss: 1.0259\n",
      "Iteration 6300/6975, Loss: 0.9504\n",
      "Iteration 6400/6975, Loss: 0.9482\n",
      "Iteration 6500/6975, Loss: 0.9044\n",
      "Iteration 6600/6975, Loss: 0.7280\n",
      "Iteration 6700/6975, Loss: 0.9236\n",
      "Iteration 6800/6975, Loss: 0.7683\n",
      "Iteration 6900/6975, Loss: 0.8363\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 1.8039\n",
      "Iteration 200/3450, Loss: 1.6595\n",
      "Iteration 300/3450, Loss: 1.3086\n",
      "Iteration 400/3450, Loss: 1.3569\n",
      "Iteration 500/3450, Loss: 1.4264\n",
      "Iteration 600/3450, Loss: 1.3042\n",
      "Iteration 700/3450, Loss: 1.4520\n",
      "Iteration 800/3450, Loss: 1.1044\n",
      "Iteration 900/3450, Loss: 1.0199\n",
      "Iteration 1000/3450, Loss: 1.2223\n",
      "Iteration 1100/3450, Loss: 0.9999\n",
      "Iteration 1200/3450, Loss: 1.0291\n",
      "Iteration 1300/3450, Loss: 1.3694\n",
      "Iteration 1400/3450, Loss: 1.1036\n",
      "Iteration 1500/3450, Loss: 1.1596\n",
      "Iteration 1600/3450, Loss: 0.9767\n",
      "Iteration 1700/3450, Loss: 1.0417\n",
      "Iteration 1800/3450, Loss: 1.3683\n",
      "Iteration 1900/3450, Loss: 1.0851\n",
      "Iteration 2000/3450, Loss: 0.9708\n",
      "Iteration 2100/3450, Loss: 0.8421\n",
      "Iteration 2200/3450, Loss: 1.1485\n",
      "Iteration 2300/3450, Loss: 1.0253\n",
      "Iteration 2400/3450, Loss: 0.8626\n",
      "Iteration 2500/3450, Loss: 1.0085\n",
      "Iteration 2600/3450, Loss: 0.9128\n",
      "Iteration 2700/3450, Loss: 0.9280\n",
      "Iteration 2800/3450, Loss: 0.9117\n",
      "Iteration 2900/3450, Loss: 0.8912\n",
      "Iteration 3000/3450, Loss: 0.8260\n",
      "Iteration 3100/3450, Loss: 0.9081\n",
      "Iteration 3200/3450, Loss: 0.9575\n",
      "Iteration 3300/3450, Loss: 0.7700\n",
      "Iteration 3400/3450, Loss: 0.7372\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 1.7416\n",
      "Iteration 200/1725, Loss: 1.3187\n",
      "Iteration 300/1725, Loss: 1.3072\n",
      "Iteration 400/1725, Loss: 1.2903\n",
      "Iteration 500/1725, Loss: 1.2705\n",
      "Iteration 600/1725, Loss: 1.1330\n",
      "Iteration 700/1725, Loss: 1.0164\n",
      "Iteration 800/1725, Loss: 1.0370\n",
      "Iteration 900/1725, Loss: 1.2645\n",
      "Iteration 1000/1725, Loss: 0.9416\n",
      "Iteration 1100/1725, Loss: 1.0767\n",
      "Iteration 1200/1725, Loss: 0.9231\n",
      "Iteration 1300/1725, Loss: 0.9918\n",
      "Iteration 1400/1725, Loss: 0.8925\n",
      "Iteration 1500/1725, Loss: 0.9270\n",
      "Iteration 1600/1725, Loss: 0.8898\n",
      "Iteration 1700/1725, Loss: 0.7134\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 1.3724\n",
      "Iteration 200/825, Loss: 1.2712\n",
      "Iteration 300/825, Loss: 1.1226\n",
      "Iteration 400/825, Loss: 1.1102\n",
      "Iteration 500/825, Loss: 0.9773\n",
      "Iteration 600/825, Loss: 0.9230\n",
      "Iteration 700/825, Loss: 0.8702\n",
      "Iteration 800/825, Loss: 0.8797\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "train accuracy: 0.7516666666666667\n",
      "val accuracy: 0.333\n",
      "batch_size: 256 reg: 0.0001 rate: 0.0001\n",
      "Iteration 100/375, Loss: 2.1926\n",
      "Iteration 200/375, Loss: 1.3942\n",
      "Iteration 300/375, Loss: 1.0020\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 3.3540\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 1.8609\n",
      "Iteration 200/28125, Loss: 1.8647\n",
      "Iteration 300/28125, Loss: 2.0731\n",
      "Iteration 400/28125, Loss: 2.4750\n",
      "Iteration 500/28125, Loss: 1.8125\n",
      "Iteration 600/28125, Loss: 1.2618\n",
      "Iteration 700/28125, Loss: 1.6694\n",
      "Iteration 800/28125, Loss: 1.7699\n",
      "Iteration 900/28125, Loss: 1.5460\n",
      "Iteration 1000/28125, Loss: 1.7647\n",
      "Iteration 1100/28125, Loss: 1.4741\n",
      "Iteration 1200/28125, Loss: 1.1933\n",
      "Iteration 1300/28125, Loss: 2.0756\n",
      "Iteration 1400/28125, Loss: 1.2435\n",
      "Iteration 1500/28125, Loss: 1.2580\n",
      "Iteration 1600/28125, Loss: 1.3461\n",
      "Iteration 1700/28125, Loss: 2.1079\n",
      "Iteration 1800/28125, Loss: 1.6169\n",
      "Iteration 1900/28125, Loss: 1.0903\n",
      "Iteration 2000/28125, Loss: 1.6521\n",
      "Iteration 2100/28125, Loss: 1.5256\n",
      "Iteration 2200/28125, Loss: 1.4782\n",
      "Iteration 2300/28125, Loss: 1.5771\n",
      "Iteration 2400/28125, Loss: 1.7369\n",
      "Iteration 2500/28125, Loss: 1.1512\n",
      "Iteration 2600/28125, Loss: 1.7004\n",
      "Iteration 2700/28125, Loss: 1.6064\n",
      "Iteration 2800/28125, Loss: 1.7386\n",
      "Iteration 2900/28125, Loss: 1.6569\n",
      "Iteration 3000/28125, Loss: 1.5977\n",
      "Iteration 3100/28125, Loss: 1.8414\n",
      "Iteration 3200/28125, Loss: 1.4968\n",
      "Iteration 3300/28125, Loss: 1.0364\n",
      "Iteration 3400/28125, Loss: 1.3700\n",
      "Iteration 3500/28125, Loss: 1.0918\n",
      "Iteration 3600/28125, Loss: 1.6223\n",
      "Iteration 3700/28125, Loss: 1.5359\n",
      "Iteration 3800/28125, Loss: 1.3188\n",
      "Iteration 3900/28125, Loss: 0.9655\n",
      "Iteration 4000/28125, Loss: 1.8032\n",
      "Iteration 4100/28125, Loss: 1.1516\n",
      "Iteration 4200/28125, Loss: 1.8759\n",
      "Iteration 4300/28125, Loss: 1.1176\n",
      "Iteration 4400/28125, Loss: 1.2888\n",
      "Iteration 4500/28125, Loss: 1.7309\n",
      "Iteration 4600/28125, Loss: 1.4848\n",
      "Iteration 4700/28125, Loss: 1.2920\n",
      "Iteration 4800/28125, Loss: 2.1990\n",
      "Iteration 4900/28125, Loss: 1.2944\n",
      "Iteration 5000/28125, Loss: 1.2088\n",
      "Iteration 5100/28125, Loss: 1.3563\n",
      "Iteration 5200/28125, Loss: 1.4588\n",
      "Iteration 5300/28125, Loss: 1.4141\n",
      "Iteration 5400/28125, Loss: 1.2809\n",
      "Iteration 5500/28125, Loss: 2.2179\n",
      "Iteration 5600/28125, Loss: 1.1876\n",
      "Iteration 5700/28125, Loss: 1.6280\n",
      "Iteration 5800/28125, Loss: 1.4333\n",
      "Iteration 5900/28125, Loss: 0.5351\n",
      "Iteration 6000/28125, Loss: 0.7945\n",
      "Iteration 6100/28125, Loss: 0.9647\n",
      "Iteration 6200/28125, Loss: 1.5872\n",
      "Iteration 6300/28125, Loss: 1.1994\n",
      "Iteration 6400/28125, Loss: 1.1742\n",
      "Iteration 6500/28125, Loss: 0.9284\n",
      "Iteration 6600/28125, Loss: 1.2332\n",
      "Iteration 6700/28125, Loss: 1.4403\n",
      "Iteration 6800/28125, Loss: 1.2434\n",
      "Iteration 6900/28125, Loss: 1.1968\n",
      "Iteration 7000/28125, Loss: 0.9257\n",
      "Iteration 7100/28125, Loss: 1.1079\n",
      "Iteration 7200/28125, Loss: 0.7480\n",
      "Iteration 7300/28125, Loss: 1.4537\n",
      "Iteration 7400/28125, Loss: 1.4098\n",
      "Iteration 7500/28125, Loss: 1.5589\n",
      "Iteration 7600/28125, Loss: 1.4479\n",
      "Iteration 7700/28125, Loss: 1.6518\n",
      "Iteration 7800/28125, Loss: 1.0289\n",
      "Iteration 7900/28125, Loss: 1.5659\n",
      "Iteration 8000/28125, Loss: 1.2914\n",
      "Iteration 8100/28125, Loss: 0.9895\n",
      "Iteration 8200/28125, Loss: 1.2788\n",
      "Iteration 8300/28125, Loss: 1.2286\n",
      "Iteration 8400/28125, Loss: 1.0346\n",
      "Iteration 8500/28125, Loss: 1.0097\n",
      "Iteration 8600/28125, Loss: 1.4651\n",
      "Iteration 8700/28125, Loss: 0.7121\n",
      "Iteration 8800/28125, Loss: 1.2922\n",
      "Iteration 8900/28125, Loss: 1.6701\n",
      "Iteration 9000/28125, Loss: 1.2267\n",
      "Iteration 9100/28125, Loss: 0.8119\n",
      "Iteration 9200/28125, Loss: 1.3036\n",
      "Iteration 9300/28125, Loss: 0.9429\n",
      "Iteration 9400/28125, Loss: 0.6795\n",
      "Iteration 9500/28125, Loss: 0.9573\n",
      "Iteration 9600/28125, Loss: 1.1978\n",
      "Iteration 9700/28125, Loss: 1.2076\n",
      "Iteration 9800/28125, Loss: 1.5814\n",
      "Iteration 9900/28125, Loss: 1.4745\n",
      "Iteration 10000/28125, Loss: 1.0505\n",
      "Iteration 10100/28125, Loss: 1.3218\n",
      "Iteration 10200/28125, Loss: 0.9976\n",
      "Iteration 10300/28125, Loss: 0.7929\n",
      "Iteration 10400/28125, Loss: 1.4882\n",
      "Iteration 10500/28125, Loss: 1.1844\n",
      "Iteration 10600/28125, Loss: 1.1874\n",
      "Iteration 10700/28125, Loss: 1.0073\n",
      "Iteration 10800/28125, Loss: 0.9134\n",
      "Iteration 10900/28125, Loss: 0.5782\n",
      "Iteration 11000/28125, Loss: 0.8709\n",
      "Iteration 11100/28125, Loss: 0.9874\n",
      "Iteration 11200/28125, Loss: 1.0433\n",
      "Iteration 11300/28125, Loss: 1.0119\n",
      "Iteration 11400/28125, Loss: 0.9341\n",
      "Iteration 11500/28125, Loss: 1.5312\n",
      "Iteration 11600/28125, Loss: 0.8339\n",
      "Iteration 11700/28125, Loss: 1.0668\n",
      "Iteration 11800/28125, Loss: 1.2547\n",
      "Iteration 11900/28125, Loss: 0.7103\n",
      "Iteration 12000/28125, Loss: 1.1909\n",
      "Iteration 12100/28125, Loss: 1.5738\n",
      "Iteration 12200/28125, Loss: 1.0048\n",
      "Iteration 12300/28125, Loss: 1.0168\n",
      "Iteration 12400/28125, Loss: 1.9380\n",
      "Iteration 12500/28125, Loss: 1.3654\n",
      "Iteration 12600/28125, Loss: 1.0525\n",
      "Iteration 12700/28125, Loss: 1.3840\n",
      "Iteration 12800/28125, Loss: 1.0994\n",
      "Iteration 12900/28125, Loss: 0.6612\n",
      "Iteration 13000/28125, Loss: 1.1769\n",
      "Iteration 13100/28125, Loss: 0.7519\n",
      "Iteration 13200/28125, Loss: 1.5040\n",
      "Iteration 13300/28125, Loss: 1.2322\n",
      "Iteration 13400/28125, Loss: 1.1193\n",
      "Iteration 13500/28125, Loss: 0.7689\n",
      "Iteration 13600/28125, Loss: 0.9252\n",
      "Iteration 13700/28125, Loss: 1.0214\n",
      "Iteration 13800/28125, Loss: 0.6425\n",
      "Iteration 13900/28125, Loss: 0.5574\n",
      "Iteration 14000/28125, Loss: 1.3293\n",
      "Iteration 14100/28125, Loss: 0.7016\n",
      "Iteration 14200/28125, Loss: 0.7855\n",
      "Iteration 14300/28125, Loss: 1.1354\n",
      "Iteration 14400/28125, Loss: 1.4203\n",
      "Iteration 14500/28125, Loss: 1.3359\n",
      "Iteration 14600/28125, Loss: 1.0849\n",
      "Iteration 14700/28125, Loss: 0.9619\n",
      "Iteration 14800/28125, Loss: 1.0864\n",
      "Iteration 14900/28125, Loss: 0.9957\n",
      "Iteration 15000/28125, Loss: 1.0406\n",
      "Iteration 15100/28125, Loss: 0.8504\n",
      "Iteration 15200/28125, Loss: 1.1351\n",
      "Iteration 15300/28125, Loss: 1.3673\n",
      "Iteration 15400/28125, Loss: 1.3391\n",
      "Iteration 15500/28125, Loss: 0.5844\n",
      "Iteration 15600/28125, Loss: 1.2758\n",
      "Iteration 15700/28125, Loss: 0.7131\n",
      "Iteration 15800/28125, Loss: 1.0015\n",
      "Iteration 15900/28125, Loss: 1.0948\n",
      "Iteration 16000/28125, Loss: 0.6153\n",
      "Iteration 16100/28125, Loss: 1.1384\n",
      "Iteration 16200/28125, Loss: 1.3225\n",
      "Iteration 16300/28125, Loss: 0.5731\n",
      "Iteration 16400/28125, Loss: 0.9139\n",
      "Iteration 16500/28125, Loss: 1.2456\n",
      "Iteration 16600/28125, Loss: 0.9208\n",
      "Iteration 16700/28125, Loss: 0.9190\n",
      "Iteration 16800/28125, Loss: 1.1257\n",
      "Iteration 16900/28125, Loss: 1.0049\n",
      "Iteration 17000/28125, Loss: 1.0624\n",
      "Iteration 17100/28125, Loss: 0.8271\n",
      "Iteration 17200/28125, Loss: 0.8244\n",
      "Iteration 17300/28125, Loss: 1.2470\n",
      "Iteration 17400/28125, Loss: 1.0631\n",
      "Iteration 17500/28125, Loss: 0.9550\n",
      "Iteration 17600/28125, Loss: 0.9668\n",
      "Iteration 17700/28125, Loss: 1.2478\n",
      "Iteration 17800/28125, Loss: 0.8253\n",
      "Iteration 17900/28125, Loss: 0.7608\n",
      "Iteration 18000/28125, Loss: 1.0785\n",
      "Iteration 18100/28125, Loss: 1.3684\n",
      "Iteration 18200/28125, Loss: 1.1208\n",
      "Iteration 18300/28125, Loss: 0.7547\n",
      "Iteration 18400/28125, Loss: 0.7557\n",
      "Iteration 18500/28125, Loss: 1.6308\n",
      "Iteration 18600/28125, Loss: 1.0587\n",
      "Iteration 18700/28125, Loss: 0.8531\n",
      "Iteration 18800/28125, Loss: 0.9602\n",
      "Iteration 18900/28125, Loss: 0.8403\n",
      "Iteration 19000/28125, Loss: 1.2108\n",
      "Iteration 19100/28125, Loss: 0.7817\n",
      "Iteration 19200/28125, Loss: 0.8065\n",
      "Iteration 19300/28125, Loss: 0.5734\n",
      "Iteration 19400/28125, Loss: 1.1638\n",
      "Iteration 19500/28125, Loss: 1.0264\n",
      "Iteration 19600/28125, Loss: 1.0573\n",
      "Iteration 19700/28125, Loss: 0.7985\n",
      "Iteration 19800/28125, Loss: 1.1222\n",
      "Iteration 19900/28125, Loss: 0.9258\n",
      "Iteration 20000/28125, Loss: 0.9112\n",
      "Iteration 20100/28125, Loss: 1.0129\n",
      "Iteration 20200/28125, Loss: 1.3917\n",
      "Iteration 20300/28125, Loss: 0.7681\n",
      "Iteration 20400/28125, Loss: 1.2619\n",
      "Iteration 20500/28125, Loss: 0.7955\n",
      "Iteration 20600/28125, Loss: 0.8457\n",
      "Iteration 20700/28125, Loss: 0.8933\n",
      "Iteration 20800/28125, Loss: 0.7866\n",
      "Iteration 20900/28125, Loss: 0.7650\n",
      "Iteration 21000/28125, Loss: 0.7675\n",
      "Iteration 21100/28125, Loss: 0.8428\n",
      "Iteration 21200/28125, Loss: 1.2530\n",
      "Iteration 21300/28125, Loss: 0.6129\n",
      "Iteration 21400/28125, Loss: 1.2228\n",
      "Iteration 21500/28125, Loss: 1.0702\n",
      "Iteration 21600/28125, Loss: 1.2787\n",
      "Iteration 21700/28125, Loss: 0.6906\n",
      "Iteration 21800/28125, Loss: 0.7384\n",
      "Iteration 21900/28125, Loss: 0.5808\n",
      "Iteration 22000/28125, Loss: 0.3222\n",
      "Iteration 22100/28125, Loss: 1.0478\n",
      "Iteration 22200/28125, Loss: 0.5892\n",
      "Iteration 22300/28125, Loss: 0.7987\n",
      "Iteration 22400/28125, Loss: 1.0251\n",
      "Iteration 22500/28125, Loss: 1.4810\n",
      "Iteration 22600/28125, Loss: 0.6389\n",
      "Iteration 22700/28125, Loss: 0.7249\n",
      "Iteration 22800/28125, Loss: 0.9948\n",
      "Iteration 22900/28125, Loss: 1.1671\n",
      "Iteration 23000/28125, Loss: 0.9308\n",
      "Iteration 23100/28125, Loss: 1.2085\n",
      "Iteration 23200/28125, Loss: 0.6152\n",
      "Iteration 23300/28125, Loss: 0.8751\n",
      "Iteration 23400/28125, Loss: 0.4497\n",
      "Iteration 23500/28125, Loss: 0.6780\n",
      "Iteration 23600/28125, Loss: 1.2354\n",
      "Iteration 23700/28125, Loss: 0.3586\n",
      "Iteration 23800/28125, Loss: 0.7498\n",
      "Iteration 23900/28125, Loss: 1.2237\n",
      "Iteration 24000/28125, Loss: 0.9702\n",
      "Iteration 24100/28125, Loss: 0.6460\n",
      "Iteration 24200/28125, Loss: 0.5468\n",
      "Iteration 24300/28125, Loss: 1.1185\n",
      "Iteration 24400/28125, Loss: 0.7639\n",
      "Iteration 24500/28125, Loss: 1.1639\n",
      "Iteration 24600/28125, Loss: 0.8864\n",
      "Iteration 24700/28125, Loss: 1.1899\n",
      "Iteration 24800/28125, Loss: 1.0314\n",
      "Iteration 24900/28125, Loss: 0.7136\n",
      "Iteration 25000/28125, Loss: 0.8808\n",
      "Iteration 25100/28125, Loss: 0.6240\n",
      "Iteration 25200/28125, Loss: 0.9865\n",
      "Iteration 25300/28125, Loss: 0.5102\n",
      "Iteration 25400/28125, Loss: 0.7988\n",
      "Iteration 25500/28125, Loss: 0.5039\n",
      "Iteration 25600/28125, Loss: 0.8163\n",
      "Iteration 25700/28125, Loss: 0.7197\n",
      "Iteration 25800/28125, Loss: 0.8147\n",
      "Iteration 25900/28125, Loss: 1.1552\n",
      "Iteration 26000/28125, Loss: 1.1356\n",
      "Iteration 26100/28125, Loss: 0.9159\n",
      "Iteration 26200/28125, Loss: 1.0141\n",
      "Iteration 26300/28125, Loss: 0.6614\n",
      "Iteration 26400/28125, Loss: 1.2368\n",
      "Iteration 26500/28125, Loss: 0.3968\n",
      "Iteration 26600/28125, Loss: 0.9546\n",
      "Iteration 26700/28125, Loss: 1.2533\n",
      "Iteration 26800/28125, Loss: 0.7949\n",
      "Iteration 26900/28125, Loss: 0.9143\n",
      "Iteration 27000/28125, Loss: 1.2946\n",
      "Iteration 27100/28125, Loss: 0.8241\n",
      "Iteration 27200/28125, Loss: 0.9900\n",
      "Iteration 27300/28125, Loss: 0.7616\n",
      "Iteration 27400/28125, Loss: 0.4637\n",
      "Iteration 27500/28125, Loss: 1.0436\n",
      "Iteration 27600/28125, Loss: 0.7749\n",
      "Iteration 27700/28125, Loss: 0.6972\n",
      "Iteration 27800/28125, Loss: 0.6070\n",
      "Iteration 27900/28125, Loss: 0.8220\n",
      "Iteration 28000/28125, Loss: 0.4846\n",
      "Iteration 28100/28125, Loss: 0.8200\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 1.9177\n",
      "Iteration 200/14025, Loss: 2.1467\n",
      "Iteration 300/14025, Loss: 1.5181\n",
      "Iteration 400/14025, Loss: 1.9458\n",
      "Iteration 500/14025, Loss: 1.6608\n",
      "Iteration 600/14025, Loss: 1.3282\n",
      "Iteration 700/14025, Loss: 1.4090\n",
      "Iteration 800/14025, Loss: 1.4639\n",
      "Iteration 900/14025, Loss: 1.1838\n",
      "Iteration 1000/14025, Loss: 1.6103\n",
      "Iteration 1100/14025, Loss: 1.4392\n",
      "Iteration 1200/14025, Loss: 1.3897\n",
      "Iteration 1300/14025, Loss: 1.3842\n",
      "Iteration 1400/14025, Loss: 1.4513\n",
      "Iteration 1500/14025, Loss: 1.7409\n",
      "Iteration 1600/14025, Loss: 1.5370\n",
      "Iteration 1700/14025, Loss: 1.4023\n",
      "Iteration 1800/14025, Loss: 1.5397\n",
      "Iteration 1900/14025, Loss: 1.2296\n",
      "Iteration 2000/14025, Loss: 1.5352\n",
      "Iteration 2100/14025, Loss: 1.6894\n",
      "Iteration 2200/14025, Loss: 1.4305\n",
      "Iteration 2300/14025, Loss: 1.7108\n",
      "Iteration 2400/14025, Loss: 1.8158\n",
      "Iteration 2500/14025, Loss: 1.5469\n",
      "Iteration 2600/14025, Loss: 1.2479\n",
      "Iteration 2700/14025, Loss: 1.2164\n",
      "Iteration 2800/14025, Loss: 1.1934\n",
      "Iteration 2900/14025, Loss: 1.1537\n",
      "Iteration 3000/14025, Loss: 0.8636\n",
      "Iteration 3100/14025, Loss: 1.5136\n",
      "Iteration 3200/14025, Loss: 1.2314\n",
      "Iteration 3300/14025, Loss: 1.1125\n",
      "Iteration 3400/14025, Loss: 1.4716\n",
      "Iteration 3500/14025, Loss: 1.1392\n",
      "Iteration 3600/14025, Loss: 1.1251\n",
      "Iteration 3700/14025, Loss: 1.2003\n",
      "Iteration 3800/14025, Loss: 1.3564\n",
      "Iteration 3900/14025, Loss: 1.0567\n",
      "Iteration 4000/14025, Loss: 1.3360\n",
      "Iteration 4100/14025, Loss: 1.1502\n",
      "Iteration 4200/14025, Loss: 1.3059\n",
      "Iteration 4300/14025, Loss: 1.2020\n",
      "Iteration 4400/14025, Loss: 1.2664\n",
      "Iteration 4500/14025, Loss: 1.1472\n",
      "Iteration 4600/14025, Loss: 1.2306\n",
      "Iteration 4700/14025, Loss: 0.9316\n",
      "Iteration 4800/14025, Loss: 1.0533\n",
      "Iteration 4900/14025, Loss: 1.1458\n",
      "Iteration 5000/14025, Loss: 1.0924\n",
      "Iteration 5100/14025, Loss: 0.9249\n",
      "Iteration 5200/14025, Loss: 1.7368\n",
      "Iteration 5300/14025, Loss: 1.3129\n",
      "Iteration 5400/14025, Loss: 0.9680\n",
      "Iteration 5500/14025, Loss: 0.9776\n",
      "Iteration 5600/14025, Loss: 1.0851\n",
      "Iteration 5700/14025, Loss: 0.9035\n",
      "Iteration 5800/14025, Loss: 1.0399\n",
      "Iteration 5900/14025, Loss: 1.3911\n",
      "Iteration 6000/14025, Loss: 1.1225\n",
      "Iteration 6100/14025, Loss: 1.0886\n",
      "Iteration 6200/14025, Loss: 1.3348\n",
      "Iteration 6300/14025, Loss: 1.0960\n",
      "Iteration 6400/14025, Loss: 1.1804\n",
      "Iteration 6500/14025, Loss: 1.0120\n",
      "Iteration 6600/14025, Loss: 1.4916\n",
      "Iteration 6700/14025, Loss: 0.8769\n",
      "Iteration 6800/14025, Loss: 0.9673\n",
      "Iteration 6900/14025, Loss: 0.9189\n",
      "Iteration 7000/14025, Loss: 1.1151\n",
      "Iteration 7100/14025, Loss: 0.7400\n",
      "Iteration 7200/14025, Loss: 1.2207\n",
      "Iteration 7300/14025, Loss: 0.9155\n",
      "Iteration 7400/14025, Loss: 0.9363\n",
      "Iteration 7500/14025, Loss: 1.3788\n",
      "Iteration 7600/14025, Loss: 1.1643\n",
      "Iteration 7700/14025, Loss: 1.4165\n",
      "Iteration 7800/14025, Loss: 1.3290\n",
      "Iteration 7900/14025, Loss: 0.8025\n",
      "Iteration 8000/14025, Loss: 1.0493\n",
      "Iteration 8100/14025, Loss: 1.1422\n",
      "Iteration 8200/14025, Loss: 1.1973\n",
      "Iteration 8300/14025, Loss: 1.1090\n",
      "Iteration 8400/14025, Loss: 0.9967\n",
      "Iteration 8500/14025, Loss: 0.9594\n",
      "Iteration 8600/14025, Loss: 0.7415\n",
      "Iteration 8700/14025, Loss: 1.0039\n",
      "Iteration 8800/14025, Loss: 0.8100\n",
      "Iteration 8900/14025, Loss: 0.8601\n",
      "Iteration 9000/14025, Loss: 1.0514\n",
      "Iteration 9100/14025, Loss: 1.2542\n",
      "Iteration 9200/14025, Loss: 0.6952\n",
      "Iteration 9300/14025, Loss: 0.9843\n",
      "Iteration 9400/14025, Loss: 0.8794\n",
      "Iteration 9500/14025, Loss: 1.0519\n",
      "Iteration 9600/14025, Loss: 0.8215\n",
      "Iteration 9700/14025, Loss: 0.9571\n",
      "Iteration 9800/14025, Loss: 0.9042\n",
      "Iteration 9900/14025, Loss: 1.0378\n",
      "Iteration 10000/14025, Loss: 1.0263\n",
      "Iteration 10100/14025, Loss: 0.8935\n",
      "Iteration 10200/14025, Loss: 1.1656\n",
      "Iteration 10300/14025, Loss: 0.8885\n",
      "Iteration 10400/14025, Loss: 0.7705\n",
      "Iteration 10500/14025, Loss: 0.7790\n",
      "Iteration 10600/14025, Loss: 1.0349\n",
      "Iteration 10700/14025, Loss: 1.3070\n",
      "Iteration 10800/14025, Loss: 1.0267\n",
      "Iteration 10900/14025, Loss: 0.8653\n",
      "Iteration 11000/14025, Loss: 0.5492\n",
      "Iteration 11100/14025, Loss: 0.8921\n",
      "Iteration 11200/14025, Loss: 1.1037\n",
      "Iteration 11300/14025, Loss: 0.7517\n",
      "Iteration 11400/14025, Loss: 1.1153\n",
      "Iteration 11500/14025, Loss: 1.0708\n",
      "Iteration 11600/14025, Loss: 0.6297\n",
      "Iteration 11700/14025, Loss: 0.7368\n",
      "Iteration 11800/14025, Loss: 1.2067\n",
      "Iteration 11900/14025, Loss: 0.8188\n",
      "Iteration 12000/14025, Loss: 0.9832\n",
      "Iteration 12100/14025, Loss: 0.6027\n",
      "Iteration 12200/14025, Loss: 0.7055\n",
      "Iteration 12300/14025, Loss: 0.8062\n",
      "Iteration 12400/14025, Loss: 1.0436\n",
      "Iteration 12500/14025, Loss: 0.9048\n",
      "Iteration 12600/14025, Loss: 1.0403\n",
      "Iteration 12700/14025, Loss: 0.7385\n",
      "Iteration 12800/14025, Loss: 0.9743\n",
      "Iteration 12900/14025, Loss: 0.8877\n",
      "Iteration 13000/14025, Loss: 1.0960\n",
      "Iteration 13100/14025, Loss: 0.8848\n",
      "Iteration 13200/14025, Loss: 0.8182\n",
      "Iteration 13300/14025, Loss: 0.6442\n",
      "Iteration 13400/14025, Loss: 0.9759\n",
      "Iteration 13500/14025, Loss: 0.9432\n",
      "Iteration 13600/14025, Loss: 1.0019\n",
      "Iteration 13700/14025, Loss: 0.5532\n",
      "Iteration 13800/14025, Loss: 0.7680\n",
      "Iteration 13900/14025, Loss: 0.5636\n",
      "Iteration 14000/14025, Loss: 0.7391\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 1.8207\n",
      "Iteration 200/6975, Loss: 1.9500\n",
      "Iteration 300/6975, Loss: 1.5168\n",
      "Iteration 400/6975, Loss: 1.5636\n",
      "Iteration 500/6975, Loss: 1.5388\n",
      "Iteration 600/6975, Loss: 1.5503\n",
      "Iteration 700/6975, Loss: 1.5322\n",
      "Iteration 800/6975, Loss: 1.5374\n",
      "Iteration 900/6975, Loss: 1.4076\n",
      "Iteration 1000/6975, Loss: 1.4600\n",
      "Iteration 1100/6975, Loss: 1.4338\n",
      "Iteration 1200/6975, Loss: 1.3837\n",
      "Iteration 1300/6975, Loss: 1.2540\n",
      "Iteration 1400/6975, Loss: 1.3779\n",
      "Iteration 1500/6975, Loss: 1.1212\n",
      "Iteration 1600/6975, Loss: 1.1745\n",
      "Iteration 1700/6975, Loss: 1.1562\n",
      "Iteration 1800/6975, Loss: 1.1018\n",
      "Iteration 1900/6975, Loss: 1.3235\n",
      "Iteration 2000/6975, Loss: 1.2339\n",
      "Iteration 2100/6975, Loss: 1.3334\n",
      "Iteration 2200/6975, Loss: 1.1265\n",
      "Iteration 2300/6975, Loss: 1.2243\n",
      "Iteration 2400/6975, Loss: 1.1338\n",
      "Iteration 2500/6975, Loss: 1.1961\n",
      "Iteration 2600/6975, Loss: 1.3606\n",
      "Iteration 2700/6975, Loss: 0.9900\n",
      "Iteration 2800/6975, Loss: 1.1397\n",
      "Iteration 2900/6975, Loss: 1.2398\n",
      "Iteration 3000/6975, Loss: 1.1148\n",
      "Iteration 3100/6975, Loss: 1.1322\n",
      "Iteration 3200/6975, Loss: 1.1969\n",
      "Iteration 3300/6975, Loss: 1.3049\n",
      "Iteration 3400/6975, Loss: 0.9239\n",
      "Iteration 3500/6975, Loss: 0.8701\n",
      "Iteration 3600/6975, Loss: 1.2212\n",
      "Iteration 3700/6975, Loss: 0.9735\n",
      "Iteration 3800/6975, Loss: 1.1699\n",
      "Iteration 3900/6975, Loss: 1.3409\n",
      "Iteration 4000/6975, Loss: 1.1251\n",
      "Iteration 4100/6975, Loss: 0.9315\n",
      "Iteration 4200/6975, Loss: 0.9673\n",
      "Iteration 4300/6975, Loss: 0.8507\n",
      "Iteration 4400/6975, Loss: 1.0289\n",
      "Iteration 4500/6975, Loss: 1.0384\n",
      "Iteration 4600/6975, Loss: 0.9589\n",
      "Iteration 4700/6975, Loss: 0.9400\n",
      "Iteration 4800/6975, Loss: 0.8892\n",
      "Iteration 4900/6975, Loss: 0.9914\n",
      "Iteration 5000/6975, Loss: 1.0026\n",
      "Iteration 5100/6975, Loss: 0.9934\n",
      "Iteration 5200/6975, Loss: 0.9545\n",
      "Iteration 5300/6975, Loss: 0.8389\n",
      "Iteration 5400/6975, Loss: 0.9866\n",
      "Iteration 5500/6975, Loss: 0.9648\n",
      "Iteration 5600/6975, Loss: 0.8807\n",
      "Iteration 5700/6975, Loss: 0.9435\n",
      "Iteration 5800/6975, Loss: 0.7389\n",
      "Iteration 5900/6975, Loss: 0.9754\n",
      "Iteration 6000/6975, Loss: 0.9566\n",
      "Iteration 6100/6975, Loss: 0.7920\n",
      "Iteration 6200/6975, Loss: 1.0338\n",
      "Iteration 6300/6975, Loss: 0.9585\n",
      "Iteration 6400/6975, Loss: 0.9563\n",
      "Iteration 6500/6975, Loss: 0.9127\n",
      "Iteration 6600/6975, Loss: 0.7363\n",
      "Iteration 6700/6975, Loss: 0.9322\n",
      "Iteration 6800/6975, Loss: 0.7770\n",
      "Iteration 6900/6975, Loss: 0.8451\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 1.8053\n",
      "Iteration 200/3450, Loss: 1.6611\n",
      "Iteration 300/3450, Loss: 1.3103\n",
      "Iteration 400/3450, Loss: 1.3588\n",
      "Iteration 500/3450, Loss: 1.4284\n",
      "Iteration 600/3450, Loss: 1.3064\n",
      "Iteration 700/3450, Loss: 1.4543\n",
      "Iteration 800/3450, Loss: 1.1070\n",
      "Iteration 900/3450, Loss: 1.0226\n",
      "Iteration 1000/3450, Loss: 1.2252\n",
      "Iteration 1100/3450, Loss: 1.0030\n",
      "Iteration 1200/3450, Loss: 1.0325\n",
      "Iteration 1300/3450, Loss: 1.3730\n",
      "Iteration 1400/3450, Loss: 1.1073\n",
      "Iteration 1500/3450, Loss: 1.1636\n",
      "Iteration 1600/3450, Loss: 0.9809\n",
      "Iteration 1700/3450, Loss: 1.0461\n",
      "Iteration 1800/3450, Loss: 1.3729\n",
      "Iteration 1900/3450, Loss: 1.0900\n",
      "Iteration 2000/3450, Loss: 0.9760\n",
      "Iteration 2100/3450, Loss: 0.8475\n",
      "Iteration 2200/3450, Loss: 1.1541\n",
      "Iteration 2300/3450, Loss: 1.0311\n",
      "Iteration 2400/3450, Loss: 0.8687\n",
      "Iteration 2500/3450, Loss: 1.0148\n",
      "Iteration 2600/3450, Loss: 0.9193\n",
      "Iteration 2700/3450, Loss: 0.9348\n",
      "Iteration 2800/3450, Loss: 0.9188\n",
      "Iteration 2900/3450, Loss: 0.8985\n",
      "Iteration 3000/3450, Loss: 0.8336\n",
      "Iteration 3100/3450, Loss: 0.9160\n",
      "Iteration 3200/3450, Loss: 0.9655\n",
      "Iteration 3300/3450, Loss: 0.7783\n",
      "Iteration 3400/3450, Loss: 0.7458\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 1.7432\n",
      "Iteration 200/1725, Loss: 1.3205\n",
      "Iteration 300/1725, Loss: 1.3094\n",
      "Iteration 400/1725, Loss: 1.2929\n",
      "Iteration 500/1725, Loss: 1.2734\n",
      "Iteration 600/1725, Loss: 1.1364\n",
      "Iteration 700/1725, Loss: 1.0202\n",
      "Iteration 800/1725, Loss: 1.0413\n",
      "Iteration 900/1725, Loss: 1.2692\n",
      "Iteration 1000/1725, Loss: 0.9467\n",
      "Iteration 1100/1725, Loss: 1.0823\n",
      "Iteration 1200/1725, Loss: 0.9292\n",
      "Iteration 1300/1725, Loss: 0.9984\n",
      "Iteration 1400/1725, Loss: 0.8995\n",
      "Iteration 1500/1725, Loss: 0.9346\n",
      "Iteration 1600/1725, Loss: 0.8979\n",
      "Iteration 1700/1725, Loss: 0.7219\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 1.3742\n",
      "Iteration 200/825, Loss: 1.2737\n",
      "Iteration 300/825, Loss: 1.1260\n",
      "Iteration 400/825, Loss: 1.1144\n",
      "Iteration 500/825, Loss: 0.9824\n",
      "Iteration 600/825, Loss: 0.9291\n",
      "Iteration 700/825, Loss: 0.8773\n",
      "Iteration 800/825, Loss: 0.8878\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "train accuracy: 0.7516666666666667\n",
      "val accuracy: 0.333\n",
      "batch_size: 256 reg: 0.001 rate: 0.0001\n",
      "Iteration 100/375, Loss: 2.1955\n",
      "Iteration 200/375, Loss: 1.3990\n",
      "Iteration 300/375, Loss: 1.0087\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 3.3611\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 1.8746\n",
      "Iteration 200/28125, Loss: 1.8786\n",
      "Iteration 300/28125, Loss: 2.0869\n",
      "Iteration 400/28125, Loss: 2.4890\n",
      "Iteration 500/28125, Loss: 1.8266\n",
      "Iteration 600/28125, Loss: 1.2761\n",
      "Iteration 700/28125, Loss: 1.6837\n",
      "Iteration 800/28125, Loss: 1.7844\n",
      "Iteration 900/28125, Loss: 1.5606\n",
      "Iteration 1000/28125, Loss: 1.7794\n",
      "Iteration 1100/28125, Loss: 1.4889\n",
      "Iteration 1200/28125, Loss: 1.2084\n",
      "Iteration 1300/28125, Loss: 2.0908\n",
      "Iteration 1400/28125, Loss: 1.2591\n",
      "Iteration 1500/28125, Loss: 1.2735\n",
      "Iteration 1600/28125, Loss: 1.3618\n",
      "Iteration 1700/28125, Loss: 2.1234\n",
      "Iteration 1800/28125, Loss: 1.6331\n",
      "Iteration 1900/28125, Loss: 1.1066\n",
      "Iteration 2000/28125, Loss: 1.6684\n",
      "Iteration 2100/28125, Loss: 1.5420\n",
      "Iteration 2200/28125, Loss: 1.4948\n",
      "Iteration 2300/28125, Loss: 1.5939\n",
      "Iteration 2400/28125, Loss: 1.7538\n",
      "Iteration 2500/28125, Loss: 1.1683\n",
      "Iteration 2600/28125, Loss: 1.7178\n",
      "Iteration 2700/28125, Loss: 1.6238\n",
      "Iteration 2800/28125, Loss: 1.7563\n",
      "Iteration 2900/28125, Loss: 1.6751\n",
      "Iteration 3000/28125, Loss: 1.6156\n",
      "Iteration 3100/28125, Loss: 1.8596\n",
      "Iteration 3200/28125, Loss: 1.5151\n",
      "Iteration 3300/28125, Loss: 1.0552\n",
      "Iteration 3400/28125, Loss: 1.3894\n",
      "Iteration 3500/28125, Loss: 1.1113\n",
      "Iteration 3600/28125, Loss: 1.6419\n",
      "Iteration 3700/28125, Loss: 1.5556\n",
      "Iteration 3800/28125, Loss: 1.3387\n",
      "Iteration 3900/28125, Loss: 0.9856\n",
      "Iteration 4000/28125, Loss: 1.8237\n",
      "Iteration 4100/28125, Loss: 1.1723\n",
      "Iteration 4200/28125, Loss: 1.8960\n",
      "Iteration 4300/28125, Loss: 1.1392\n",
      "Iteration 4400/28125, Loss: 1.3102\n",
      "Iteration 4500/28125, Loss: 1.7517\n",
      "Iteration 4600/28125, Loss: 1.5062\n",
      "Iteration 4700/28125, Loss: 1.3142\n",
      "Iteration 4800/28125, Loss: 2.2208\n",
      "Iteration 4900/28125, Loss: 1.3168\n",
      "Iteration 5000/28125, Loss: 1.2316\n",
      "Iteration 5100/28125, Loss: 1.3792\n",
      "Iteration 5200/28125, Loss: 1.4816\n",
      "Iteration 5300/28125, Loss: 1.4371\n",
      "Iteration 5400/28125, Loss: 1.3045\n",
      "Iteration 5500/28125, Loss: 2.2412\n",
      "Iteration 5600/28125, Loss: 1.2122\n",
      "Iteration 5700/28125, Loss: 1.6519\n",
      "Iteration 5800/28125, Loss: 1.4576\n",
      "Iteration 5900/28125, Loss: 0.5600\n",
      "Iteration 6000/28125, Loss: 0.8203\n",
      "Iteration 6100/28125, Loss: 0.9896\n",
      "Iteration 6200/28125, Loss: 1.6119\n",
      "Iteration 6300/28125, Loss: 1.2250\n",
      "Iteration 6400/28125, Loss: 1.2008\n",
      "Iteration 6500/28125, Loss: 0.9550\n",
      "Iteration 6600/28125, Loss: 1.2595\n",
      "Iteration 6700/28125, Loss: 1.4667\n",
      "Iteration 6800/28125, Loss: 1.2701\n",
      "Iteration 6900/28125, Loss: 1.2238\n",
      "Iteration 7000/28125, Loss: 0.9535\n",
      "Iteration 7100/28125, Loss: 1.1361\n",
      "Iteration 7200/28125, Loss: 0.7767\n",
      "Iteration 7300/28125, Loss: 1.4815\n",
      "Iteration 7400/28125, Loss: 1.4379\n",
      "Iteration 7500/28125, Loss: 1.5887\n",
      "Iteration 7600/28125, Loss: 1.4775\n",
      "Iteration 7700/28125, Loss: 1.6805\n",
      "Iteration 7800/28125, Loss: 1.0586\n",
      "Iteration 7900/28125, Loss: 1.5962\n",
      "Iteration 8000/28125, Loss: 1.3207\n",
      "Iteration 8100/28125, Loss: 1.0200\n",
      "Iteration 8200/28125, Loss: 1.3097\n",
      "Iteration 8300/28125, Loss: 1.2588\n",
      "Iteration 8400/28125, Loss: 1.0659\n",
      "Iteration 8500/28125, Loss: 1.0415\n",
      "Iteration 8600/28125, Loss: 1.4967\n",
      "Iteration 8700/28125, Loss: 0.7438\n",
      "Iteration 8800/28125, Loss: 1.3239\n",
      "Iteration 8900/28125, Loss: 1.7019\n",
      "Iteration 9000/28125, Loss: 1.2582\n",
      "Iteration 9100/28125, Loss: 0.8448\n",
      "Iteration 9200/28125, Loss: 1.3369\n",
      "Iteration 9300/28125, Loss: 0.9763\n",
      "Iteration 9400/28125, Loss: 0.7132\n",
      "Iteration 9500/28125, Loss: 0.9909\n",
      "Iteration 9600/28125, Loss: 1.2315\n",
      "Iteration 9700/28125, Loss: 1.2421\n",
      "Iteration 9800/28125, Loss: 1.6156\n",
      "Iteration 9900/28125, Loss: 1.5089\n",
      "Iteration 10000/28125, Loss: 1.0870\n",
      "Iteration 10100/28125, Loss: 1.3566\n",
      "Iteration 10200/28125, Loss: 1.0345\n",
      "Iteration 10300/28125, Loss: 0.8287\n",
      "Iteration 10400/28125, Loss: 1.5243\n",
      "Iteration 10500/28125, Loss: 1.2213\n",
      "Iteration 10600/28125, Loss: 1.2245\n",
      "Iteration 10700/28125, Loss: 1.0452\n",
      "Iteration 10800/28125, Loss: 0.9510\n",
      "Iteration 10900/28125, Loss: 0.6159\n",
      "Iteration 11000/28125, Loss: 0.9086\n",
      "Iteration 11100/28125, Loss: 1.0266\n",
      "Iteration 11200/28125, Loss: 1.0827\n",
      "Iteration 11300/28125, Loss: 1.0523\n",
      "Iteration 11400/28125, Loss: 0.9736\n",
      "Iteration 11500/28125, Loss: 1.5712\n",
      "Iteration 11600/28125, Loss: 0.8733\n",
      "Iteration 11700/28125, Loss: 1.1075\n",
      "Iteration 11800/28125, Loss: 1.2941\n",
      "Iteration 11900/28125, Loss: 0.7505\n",
      "Iteration 12000/28125, Loss: 1.2316\n",
      "Iteration 12100/28125, Loss: 1.6148\n",
      "Iteration 12200/28125, Loss: 1.0463\n",
      "Iteration 12300/28125, Loss: 1.0581\n",
      "Iteration 12400/28125, Loss: 1.9795\n",
      "Iteration 12500/28125, Loss: 1.4084\n",
      "Iteration 12600/28125, Loss: 1.0954\n",
      "Iteration 12700/28125, Loss: 1.4277\n",
      "Iteration 12800/28125, Loss: 1.1411\n",
      "Iteration 12900/28125, Loss: 0.7048\n",
      "Iteration 13000/28125, Loss: 1.2197\n",
      "Iteration 13100/28125, Loss: 0.7968\n",
      "Iteration 13200/28125, Loss: 1.5472\n",
      "Iteration 13300/28125, Loss: 1.2758\n",
      "Iteration 13400/28125, Loss: 1.1636\n",
      "Iteration 13500/28125, Loss: 0.8143\n",
      "Iteration 13600/28125, Loss: 0.9710\n",
      "Iteration 13700/28125, Loss: 1.0663\n",
      "Iteration 13800/28125, Loss: 0.6885\n",
      "Iteration 13900/28125, Loss: 0.6039\n",
      "Iteration 14000/28125, Loss: 1.3753\n",
      "Iteration 14100/28125, Loss: 0.7482\n",
      "Iteration 14200/28125, Loss: 0.8326\n",
      "Iteration 14300/28125, Loss: 1.1819\n",
      "Iteration 14400/28125, Loss: 1.4678\n",
      "Iteration 14500/28125, Loss: 1.3854\n",
      "Iteration 14600/28125, Loss: 1.1344\n",
      "Iteration 14700/28125, Loss: 1.0119\n",
      "Iteration 14800/28125, Loss: 1.1354\n",
      "Iteration 14900/28125, Loss: 1.0441\n",
      "Iteration 15000/28125, Loss: 1.0896\n",
      "Iteration 15100/28125, Loss: 0.9002\n",
      "Iteration 15200/28125, Loss: 1.1839\n",
      "Iteration 15300/28125, Loss: 1.4201\n",
      "Iteration 15400/28125, Loss: 1.3886\n",
      "Iteration 15500/28125, Loss: 0.6348\n",
      "Iteration 15600/28125, Loss: 1.3274\n",
      "Iteration 15700/28125, Loss: 0.7657\n",
      "Iteration 15800/28125, Loss: 1.0542\n",
      "Iteration 15900/28125, Loss: 1.1457\n",
      "Iteration 16000/28125, Loss: 0.6686\n",
      "Iteration 16100/28125, Loss: 1.1918\n",
      "Iteration 16200/28125, Loss: 1.3755\n",
      "Iteration 16300/28125, Loss: 0.6251\n",
      "Iteration 16400/28125, Loss: 0.9686\n",
      "Iteration 16500/28125, Loss: 1.3005\n",
      "Iteration 16600/28125, Loss: 0.9762\n",
      "Iteration 16700/28125, Loss: 0.9744\n",
      "Iteration 16800/28125, Loss: 1.1809\n",
      "Iteration 16900/28125, Loss: 1.0602\n",
      "Iteration 17000/28125, Loss: 1.1185\n",
      "Iteration 17100/28125, Loss: 0.8831\n",
      "Iteration 17200/28125, Loss: 0.8791\n",
      "Iteration 17300/28125, Loss: 1.3035\n",
      "Iteration 17400/28125, Loss: 1.1195\n",
      "Iteration 17500/28125, Loss: 1.0125\n",
      "Iteration 17600/28125, Loss: 1.0237\n",
      "Iteration 17700/28125, Loss: 1.3040\n",
      "Iteration 17800/28125, Loss: 0.8820\n",
      "Iteration 17900/28125, Loss: 0.8190\n",
      "Iteration 18000/28125, Loss: 1.1373\n",
      "Iteration 18100/28125, Loss: 1.4283\n",
      "Iteration 18200/28125, Loss: 1.1808\n",
      "Iteration 18300/28125, Loss: 0.8135\n",
      "Iteration 18400/28125, Loss: 0.8149\n",
      "Iteration 18500/28125, Loss: 1.6913\n",
      "Iteration 18600/28125, Loss: 1.1183\n",
      "Iteration 18700/28125, Loss: 0.9147\n",
      "Iteration 18800/28125, Loss: 1.0212\n",
      "Iteration 18900/28125, Loss: 0.9022\n",
      "Iteration 19000/28125, Loss: 1.2739\n",
      "Iteration 19100/28125, Loss: 0.8443\n",
      "Iteration 19200/28125, Loss: 0.8694\n",
      "Iteration 19300/28125, Loss: 0.6331\n",
      "Iteration 19400/28125, Loss: 1.2261\n",
      "Iteration 19500/28125, Loss: 1.0902\n",
      "Iteration 19600/28125, Loss: 1.1200\n",
      "Iteration 19700/28125, Loss: 0.8617\n",
      "Iteration 19800/28125, Loss: 1.1879\n",
      "Iteration 19900/28125, Loss: 0.9907\n",
      "Iteration 20000/28125, Loss: 0.9754\n",
      "Iteration 20100/28125, Loss: 1.0771\n",
      "Iteration 20200/28125, Loss: 1.4563\n",
      "Iteration 20300/28125, Loss: 0.8347\n",
      "Iteration 20400/28125, Loss: 1.3287\n",
      "Iteration 20500/28125, Loss: 0.8609\n",
      "Iteration 20600/28125, Loss: 0.9137\n",
      "Iteration 20700/28125, Loss: 0.9603\n",
      "Iteration 20800/28125, Loss: 0.8528\n",
      "Iteration 20900/28125, Loss: 0.8348\n",
      "Iteration 21000/28125, Loss: 0.8346\n",
      "Iteration 21100/28125, Loss: 0.9103\n",
      "Iteration 21200/28125, Loss: 1.3216\n",
      "Iteration 21300/28125, Loss: 0.6819\n",
      "Iteration 21400/28125, Loss: 1.2909\n",
      "Iteration 21500/28125, Loss: 1.1408\n",
      "Iteration 21600/28125, Loss: 1.3466\n",
      "Iteration 21700/28125, Loss: 0.7608\n",
      "Iteration 21800/28125, Loss: 0.8090\n",
      "Iteration 21900/28125, Loss: 0.6516\n",
      "Iteration 22000/28125, Loss: 0.3927\n",
      "Iteration 22100/28125, Loss: 1.1187\n",
      "Iteration 22200/28125, Loss: 0.6608\n",
      "Iteration 22300/28125, Loss: 0.8708\n",
      "Iteration 22400/28125, Loss: 1.0958\n",
      "Iteration 22500/28125, Loss: 1.5570\n",
      "Iteration 22600/28125, Loss: 0.7109\n",
      "Iteration 22700/28125, Loss: 0.7992\n",
      "Iteration 22800/28125, Loss: 1.0675\n",
      "Iteration 22900/28125, Loss: 1.2413\n",
      "Iteration 23000/28125, Loss: 1.0036\n",
      "Iteration 23100/28125, Loss: 1.2845\n",
      "Iteration 23200/28125, Loss: 0.6895\n",
      "Iteration 23300/28125, Loss: 0.9482\n",
      "Iteration 23400/28125, Loss: 0.5257\n",
      "Iteration 23500/28125, Loss: 0.7546\n",
      "Iteration 23600/28125, Loss: 1.3153\n",
      "Iteration 23700/28125, Loss: 0.4349\n",
      "Iteration 23800/28125, Loss: 0.8269\n",
      "Iteration 23900/28125, Loss: 1.3016\n",
      "Iteration 24000/28125, Loss: 1.0483\n",
      "Iteration 24100/28125, Loss: 0.7248\n",
      "Iteration 24200/28125, Loss: 0.6249\n",
      "Iteration 24300/28125, Loss: 1.1951\n",
      "Iteration 24400/28125, Loss: 0.8430\n",
      "Iteration 24500/28125, Loss: 1.2415\n",
      "Iteration 24600/28125, Loss: 0.9652\n",
      "Iteration 24700/28125, Loss: 1.2670\n",
      "Iteration 24800/28125, Loss: 1.1116\n",
      "Iteration 24900/28125, Loss: 0.7950\n",
      "Iteration 25000/28125, Loss: 0.9601\n",
      "Iteration 25100/28125, Loss: 0.7058\n",
      "Iteration 25200/28125, Loss: 1.0679\n",
      "Iteration 25300/28125, Loss: 0.5902\n",
      "Iteration 25400/28125, Loss: 0.8813\n",
      "Iteration 25500/28125, Loss: 0.5836\n",
      "Iteration 25600/28125, Loss: 0.8995\n",
      "Iteration 25700/28125, Loss: 0.8031\n",
      "Iteration 25800/28125, Loss: 0.8966\n",
      "Iteration 25900/28125, Loss: 1.2384\n",
      "Iteration 26000/28125, Loss: 1.2224\n",
      "Iteration 26100/28125, Loss: 1.0002\n",
      "Iteration 26200/28125, Loss: 1.1012\n",
      "Iteration 26300/28125, Loss: 0.7469\n",
      "Iteration 26400/28125, Loss: 1.3198\n",
      "Iteration 26500/28125, Loss: 0.4815\n",
      "Iteration 26600/28125, Loss: 1.0433\n",
      "Iteration 26700/28125, Loss: 1.3366\n",
      "Iteration 26800/28125, Loss: 0.8832\n",
      "Iteration 26900/28125, Loss: 1.0033\n",
      "Iteration 27000/28125, Loss: 1.3833\n",
      "Iteration 27100/28125, Loss: 0.9140\n",
      "Iteration 27200/28125, Loss: 1.0792\n",
      "Iteration 27300/28125, Loss: 0.8493\n",
      "Iteration 27400/28125, Loss: 0.5517\n",
      "Iteration 27500/28125, Loss: 1.1362\n",
      "Iteration 27600/28125, Loss: 0.8649\n",
      "Iteration 27700/28125, Loss: 0.7897\n",
      "Iteration 27800/28125, Loss: 0.6963\n",
      "Iteration 27900/28125, Loss: 0.9126\n",
      "Iteration 28000/28125, Loss: 0.5732\n",
      "Iteration 28100/28125, Loss: 0.9081\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 1.9315\n",
      "Iteration 200/14025, Loss: 2.1607\n",
      "Iteration 300/14025, Loss: 1.5324\n",
      "Iteration 400/14025, Loss: 1.9603\n",
      "Iteration 500/14025, Loss: 1.6756\n",
      "Iteration 600/14025, Loss: 1.3432\n",
      "Iteration 700/14025, Loss: 1.4245\n",
      "Iteration 800/14025, Loss: 1.4796\n",
      "Iteration 900/14025, Loss: 1.1999\n",
      "Iteration 1000/14025, Loss: 1.6266\n",
      "Iteration 1100/14025, Loss: 1.4558\n",
      "Iteration 1200/14025, Loss: 1.4067\n",
      "Iteration 1300/14025, Loss: 1.4017\n",
      "Iteration 1400/14025, Loss: 1.4689\n",
      "Iteration 1500/14025, Loss: 1.7587\n",
      "Iteration 1600/14025, Loss: 1.5554\n",
      "Iteration 1700/14025, Loss: 1.4213\n",
      "Iteration 1800/14025, Loss: 1.5590\n",
      "Iteration 1900/14025, Loss: 1.2493\n",
      "Iteration 2000/14025, Loss: 1.5555\n",
      "Iteration 2100/14025, Loss: 1.7100\n",
      "Iteration 2200/14025, Loss: 1.4517\n",
      "Iteration 2300/14025, Loss: 1.7322\n",
      "Iteration 2400/14025, Loss: 1.8376\n",
      "Iteration 2500/14025, Loss: 1.5692\n",
      "Iteration 2600/14025, Loss: 1.2706\n",
      "Iteration 2700/14025, Loss: 1.2397\n",
      "Iteration 2800/14025, Loss: 1.2172\n",
      "Iteration 2900/14025, Loss: 1.1779\n",
      "Iteration 3000/14025, Loss: 0.8885\n",
      "Iteration 3100/14025, Loss: 1.5385\n",
      "Iteration 3200/14025, Loss: 1.2575\n",
      "Iteration 3300/14025, Loss: 1.1387\n",
      "Iteration 3400/14025, Loss: 1.4980\n",
      "Iteration 3500/14025, Loss: 1.1664\n",
      "Iteration 3600/14025, Loss: 1.1529\n",
      "Iteration 3700/14025, Loss: 1.2283\n",
      "Iteration 3800/14025, Loss: 1.3849\n",
      "Iteration 3900/14025, Loss: 1.0858\n",
      "Iteration 4000/14025, Loss: 1.3655\n",
      "Iteration 4100/14025, Loss: 1.1805\n",
      "Iteration 4200/14025, Loss: 1.3366\n",
      "Iteration 4300/14025, Loss: 1.2333\n",
      "Iteration 4400/14025, Loss: 1.2982\n",
      "Iteration 4500/14025, Loss: 1.1791\n",
      "Iteration 4600/14025, Loss: 1.2633\n",
      "Iteration 4700/14025, Loss: 0.9648\n",
      "Iteration 4800/14025, Loss: 1.0870\n",
      "Iteration 4900/14025, Loss: 1.1802\n",
      "Iteration 5000/14025, Loss: 1.1275\n",
      "Iteration 5100/14025, Loss: 0.9604\n",
      "Iteration 5200/14025, Loss: 1.7724\n",
      "Iteration 5300/14025, Loss: 1.3496\n",
      "Iteration 5400/14025, Loss: 1.0051\n",
      "Iteration 5500/14025, Loss: 1.0153\n",
      "Iteration 5600/14025, Loss: 1.1235\n",
      "Iteration 5700/14025, Loss: 0.9424\n",
      "Iteration 5800/14025, Loss: 1.0794\n",
      "Iteration 5900/14025, Loss: 1.4310\n",
      "Iteration 6000/14025, Loss: 1.1630\n",
      "Iteration 6100/14025, Loss: 1.1294\n",
      "Iteration 6200/14025, Loss: 1.3761\n",
      "Iteration 6300/14025, Loss: 1.1382\n",
      "Iteration 6400/14025, Loss: 1.2226\n",
      "Iteration 6500/14025, Loss: 1.0545\n",
      "Iteration 6600/14025, Loss: 1.5352\n",
      "Iteration 6700/14025, Loss: 0.9210\n",
      "Iteration 6800/14025, Loss: 1.0123\n",
      "Iteration 6900/14025, Loss: 0.9642\n",
      "Iteration 7000/14025, Loss: 1.1607\n",
      "Iteration 7100/14025, Loss: 0.7867\n",
      "Iteration 7200/14025, Loss: 1.2675\n",
      "Iteration 7300/14025, Loss: 0.9635\n",
      "Iteration 7400/14025, Loss: 0.9847\n",
      "Iteration 7500/14025, Loss: 1.4274\n",
      "Iteration 7600/14025, Loss: 1.2133\n",
      "Iteration 7700/14025, Loss: 1.4659\n",
      "Iteration 7800/14025, Loss: 1.3797\n",
      "Iteration 7900/14025, Loss: 0.8537\n",
      "Iteration 8000/14025, Loss: 1.1010\n",
      "Iteration 8100/14025, Loss: 1.1950\n",
      "Iteration 8200/14025, Loss: 1.2506\n",
      "Iteration 8300/14025, Loss: 1.1625\n",
      "Iteration 8400/14025, Loss: 1.0507\n",
      "Iteration 8500/14025, Loss: 1.0143\n",
      "Iteration 8600/14025, Loss: 0.7967\n",
      "Iteration 8700/14025, Loss: 1.0599\n",
      "Iteration 8800/14025, Loss: 0.8666\n",
      "Iteration 8900/14025, Loss: 0.9172\n",
      "Iteration 9000/14025, Loss: 1.1098\n",
      "Iteration 9100/14025, Loss: 1.3124\n",
      "Iteration 9200/14025, Loss: 0.7537\n",
      "Iteration 9300/14025, Loss: 1.0438\n",
      "Iteration 9400/14025, Loss: 0.9394\n",
      "Iteration 9500/14025, Loss: 1.1132\n",
      "Iteration 9600/14025, Loss: 0.8831\n",
      "Iteration 9700/14025, Loss: 1.0189\n",
      "Iteration 9800/14025, Loss: 0.9665\n",
      "Iteration 9900/14025, Loss: 1.1017\n",
      "Iteration 10000/14025, Loss: 1.0895\n",
      "Iteration 10100/14025, Loss: 0.9577\n",
      "Iteration 10200/14025, Loss: 1.2312\n",
      "Iteration 10300/14025, Loss: 0.9543\n",
      "Iteration 10400/14025, Loss: 0.8364\n",
      "Iteration 10500/14025, Loss: 0.8456\n",
      "Iteration 10600/14025, Loss: 1.1027\n",
      "Iteration 10700/14025, Loss: 1.3752\n",
      "Iteration 10800/14025, Loss: 1.0948\n",
      "Iteration 10900/14025, Loss: 0.9349\n",
      "Iteration 11000/14025, Loss: 0.6191\n",
      "Iteration 11100/14025, Loss: 0.9626\n",
      "Iteration 11200/14025, Loss: 1.1748\n",
      "Iteration 11300/14025, Loss: 0.8236\n",
      "Iteration 11400/14025, Loss: 1.1878\n",
      "Iteration 11500/14025, Loss: 1.1434\n",
      "Iteration 11600/14025, Loss: 0.7031\n",
      "Iteration 11700/14025, Loss: 0.8117\n",
      "Iteration 11800/14025, Loss: 1.2826\n",
      "Iteration 11900/14025, Loss: 0.8951\n",
      "Iteration 12000/14025, Loss: 1.0599\n",
      "Iteration 12100/14025, Loss: 0.6802\n",
      "Iteration 12200/14025, Loss: 0.7831\n",
      "Iteration 12300/14025, Loss: 0.8843\n",
      "Iteration 12400/14025, Loss: 1.1227\n",
      "Iteration 12500/14025, Loss: 0.9841\n",
      "Iteration 12600/14025, Loss: 1.1206\n",
      "Iteration 12700/14025, Loss: 0.8197\n",
      "Iteration 12800/14025, Loss: 1.0555\n",
      "Iteration 12900/14025, Loss: 0.9699\n",
      "Iteration 13000/14025, Loss: 1.1789\n",
      "Iteration 13100/14025, Loss: 0.9688\n",
      "Iteration 13200/14025, Loss: 0.9015\n",
      "Iteration 13300/14025, Loss: 0.7295\n",
      "Iteration 13400/14025, Loss: 1.0623\n",
      "Iteration 13500/14025, Loss: 1.0298\n",
      "Iteration 13600/14025, Loss: 1.0892\n",
      "Iteration 13700/14025, Loss: 0.6406\n",
      "Iteration 13800/14025, Loss: 0.8559\n",
      "Iteration 13900/14025, Loss: 0.6519\n",
      "Iteration 14000/14025, Loss: 0.8278\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 1.8347\n",
      "Iteration 200/6975, Loss: 1.9645\n",
      "Iteration 300/6975, Loss: 1.5318\n",
      "Iteration 400/6975, Loss: 1.5792\n",
      "Iteration 500/6975, Loss: 1.5551\n",
      "Iteration 600/6975, Loss: 1.5672\n",
      "Iteration 700/6975, Loss: 1.5499\n",
      "Iteration 800/6975, Loss: 1.5559\n",
      "Iteration 900/6975, Loss: 1.4269\n",
      "Iteration 1000/6975, Loss: 1.4801\n",
      "Iteration 1100/6975, Loss: 1.4547\n",
      "Iteration 1200/6975, Loss: 1.4055\n",
      "Iteration 1300/6975, Loss: 1.2766\n",
      "Iteration 1400/6975, Loss: 1.4015\n",
      "Iteration 1500/6975, Loss: 1.1458\n",
      "Iteration 1600/6975, Loss: 1.2001\n",
      "Iteration 1700/6975, Loss: 1.1826\n",
      "Iteration 1800/6975, Loss: 1.1293\n",
      "Iteration 1900/6975, Loss: 1.3518\n",
      "Iteration 2000/6975, Loss: 1.2631\n",
      "Iteration 2100/6975, Loss: 1.3637\n",
      "Iteration 2200/6975, Loss: 1.1579\n",
      "Iteration 2300/6975, Loss: 1.2567\n",
      "Iteration 2400/6975, Loss: 1.1672\n",
      "Iteration 2500/6975, Loss: 1.2310\n",
      "Iteration 2600/6975, Loss: 1.3962\n",
      "Iteration 2700/6975, Loss: 1.0269\n",
      "Iteration 2800/6975, Loss: 1.1778\n",
      "Iteration 2900/6975, Loss: 1.2788\n",
      "Iteration 3000/6975, Loss: 1.1549\n",
      "Iteration 3100/6975, Loss: 1.1733\n",
      "Iteration 3200/6975, Loss: 1.2391\n",
      "Iteration 3300/6975, Loss: 1.3482\n",
      "Iteration 3400/6975, Loss: 0.9684\n",
      "Iteration 3500/6975, Loss: 0.9157\n",
      "Iteration 3600/6975, Loss: 1.2681\n",
      "Iteration 3700/6975, Loss: 1.0216\n",
      "Iteration 3800/6975, Loss: 1.2188\n",
      "Iteration 3900/6975, Loss: 1.3910\n",
      "Iteration 4000/6975, Loss: 1.1766\n",
      "Iteration 4100/6975, Loss: 0.9842\n",
      "Iteration 4200/6975, Loss: 1.0210\n",
      "Iteration 4300/6975, Loss: 0.9057\n",
      "Iteration 4400/6975, Loss: 1.0848\n",
      "Iteration 4500/6975, Loss: 1.0958\n",
      "Iteration 4600/6975, Loss: 1.0175\n",
      "Iteration 4700/6975, Loss: 0.9998\n",
      "Iteration 4800/6975, Loss: 0.9503\n",
      "Iteration 4900/6975, Loss: 1.0536\n",
      "Iteration 5000/6975, Loss: 1.0659\n",
      "Iteration 5100/6975, Loss: 1.0582\n",
      "Iteration 5200/6975, Loss: 1.0200\n",
      "Iteration 5300/6975, Loss: 0.9063\n",
      "Iteration 5400/6975, Loss: 1.0547\n",
      "Iteration 5500/6975, Loss: 1.0344\n",
      "Iteration 5600/6975, Loss: 0.9515\n",
      "Iteration 5700/6975, Loss: 1.0155\n",
      "Iteration 5800/6975, Loss: 0.8123\n",
      "Iteration 5900/6975, Loss: 1.0501\n",
      "Iteration 6000/6975, Loss: 1.0323\n",
      "Iteration 6100/6975, Loss: 0.8691\n",
      "Iteration 6200/6975, Loss: 1.1125\n",
      "Iteration 6300/6975, Loss: 1.0383\n",
      "Iteration 6400/6975, Loss: 1.0372\n",
      "Iteration 6500/6975, Loss: 0.9949\n",
      "Iteration 6600/6975, Loss: 0.8197\n",
      "Iteration 6700/6975, Loss: 1.0174\n",
      "Iteration 6800/6975, Loss: 0.8633\n",
      "Iteration 6900/6975, Loss: 0.9323\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 1.8198\n",
      "Iteration 200/3450, Loss: 1.6767\n",
      "Iteration 300/3450, Loss: 1.3272\n",
      "Iteration 400/3450, Loss: 1.3772\n",
      "Iteration 500/3450, Loss: 1.4485\n",
      "Iteration 600/3450, Loss: 1.3282\n",
      "Iteration 700/3450, Loss: 1.4779\n",
      "Iteration 800/3450, Loss: 1.1324\n",
      "Iteration 900/3450, Loss: 1.0500\n",
      "Iteration 1000/3450, Loss: 1.2545\n",
      "Iteration 1100/3450, Loss: 1.0344\n",
      "Iteration 1200/3450, Loss: 1.0659\n",
      "Iteration 1300/3450, Loss: 1.4085\n",
      "Iteration 1400/3450, Loss: 1.1452\n",
      "Iteration 1500/3450, Loss: 1.2035\n",
      "Iteration 1600/3450, Loss: 1.0230\n",
      "Iteration 1700/3450, Loss: 1.0905\n",
      "Iteration 1800/3450, Loss: 1.4196\n",
      "Iteration 1900/3450, Loss: 1.1389\n",
      "Iteration 2000/3450, Loss: 1.0272\n",
      "Iteration 2100/3450, Loss: 0.9011\n",
      "Iteration 2200/3450, Loss: 1.2100\n",
      "Iteration 2300/3450, Loss: 1.0895\n",
      "Iteration 2400/3450, Loss: 0.9295\n",
      "Iteration 2500/3450, Loss: 1.0780\n",
      "Iteration 2600/3450, Loss: 0.9849\n",
      "Iteration 2700/3450, Loss: 1.0029\n",
      "Iteration 2800/3450, Loss: 0.9895\n",
      "Iteration 2900/3450, Loss: 0.9716\n",
      "Iteration 3000/3450, Loss: 0.9092\n",
      "Iteration 3100/3450, Loss: 0.9943\n",
      "Iteration 3200/3450, Loss: 1.0462\n",
      "Iteration 3300/3450, Loss: 0.8615\n",
      "Iteration 3400/3450, Loss: 0.8317\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 1.7588\n",
      "Iteration 200/1725, Loss: 1.3389\n",
      "Iteration 300/1725, Loss: 1.3311\n",
      "Iteration 400/1725, Loss: 1.3183\n",
      "Iteration 500/1725, Loss: 1.3027\n",
      "Iteration 600/1725, Loss: 1.1698\n",
      "Iteration 700/1725, Loss: 1.0580\n",
      "Iteration 800/1725, Loss: 1.0833\n",
      "Iteration 900/1725, Loss: 1.3157\n",
      "Iteration 1000/1725, Loss: 0.9978\n",
      "Iteration 1100/1725, Loss: 1.1381\n",
      "Iteration 1200/1725, Loss: 0.9899\n",
      "Iteration 1300/1725, Loss: 1.0641\n",
      "Iteration 1400/1725, Loss: 0.9701\n",
      "Iteration 1500/1725, Loss: 1.0102\n",
      "Iteration 1600/1725, Loss: 0.9786\n",
      "Iteration 1700/1725, Loss: 0.8077\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 1.3926\n",
      "Iteration 200/825, Loss: 1.2992\n",
      "Iteration 300/825, Loss: 1.1594\n",
      "Iteration 400/825, Loss: 1.1565\n",
      "Iteration 500/825, Loss: 1.0336\n",
      "Iteration 600/825, Loss: 0.9900\n",
      "Iteration 700/825, Loss: 0.9480\n",
      "Iteration 800/825, Loss: 0.9686\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "train accuracy: 0.7516666666666667\n",
      "val accuracy: 0.333\n",
      "batch_size: 256 reg: 0.01 rate: 0.0001\n",
      "Iteration 100/375, Loss: 2.2244\n",
      "Iteration 200/375, Loss: 1.4464\n",
      "Iteration 300/375, Loss: 1.0765\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 3.4319\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 2.0117\n",
      "Iteration 200/28125, Loss: 2.0162\n",
      "Iteration 300/28125, Loss: 2.2249\n",
      "Iteration 400/28125, Loss: 2.6279\n",
      "Iteration 500/28125, Loss: 1.9661\n",
      "Iteration 600/28125, Loss: 1.4177\n",
      "Iteration 700/28125, Loss: 1.8255\n",
      "Iteration 800/28125, Loss: 1.9276\n",
      "Iteration 900/28125, Loss: 1.7041\n",
      "Iteration 1000/28125, Loss: 1.9236\n",
      "Iteration 1100/28125, Loss: 1.6345\n",
      "Iteration 1200/28125, Loss: 1.3564\n",
      "Iteration 1300/28125, Loss: 2.2391\n",
      "Iteration 1400/28125, Loss: 1.4117\n",
      "Iteration 1500/28125, Loss: 1.4243\n",
      "Iteration 1600/28125, Loss: 1.5151\n",
      "Iteration 1700/28125, Loss: 2.2733\n",
      "Iteration 1800/28125, Loss: 1.7896\n",
      "Iteration 1900/28125, Loss: 1.2642\n",
      "Iteration 2000/28125, Loss: 1.8260\n",
      "Iteration 2100/28125, Loss: 1.7006\n",
      "Iteration 2200/28125, Loss: 1.6548\n",
      "Iteration 2300/28125, Loss: 1.7555\n",
      "Iteration 2400/28125, Loss: 1.9162\n",
      "Iteration 2500/28125, Loss: 1.3324\n",
      "Iteration 2600/28125, Loss: 1.8848\n",
      "Iteration 2700/28125, Loss: 1.7905\n",
      "Iteration 2800/28125, Loss: 1.9261\n",
      "Iteration 2900/28125, Loss: 1.8490\n",
      "Iteration 3000/28125, Loss: 1.7854\n",
      "Iteration 3100/28125, Loss: 2.0332\n",
      "Iteration 3200/28125, Loss: 1.6890\n",
      "Iteration 3300/28125, Loss: 1.2336\n",
      "Iteration 3400/28125, Loss: 1.5737\n",
      "Iteration 3500/28125, Loss: 1.2956\n",
      "Iteration 3600/28125, Loss: 1.8273\n",
      "Iteration 3700/28125, Loss: 1.7421\n",
      "Iteration 3800/28125, Loss: 1.5267\n",
      "Iteration 3900/28125, Loss: 1.1754\n",
      "Iteration 4000/28125, Loss: 2.0167\n",
      "Iteration 4100/28125, Loss: 1.3674\n",
      "Iteration 4200/28125, Loss: 2.0851\n",
      "Iteration 4300/28125, Loss: 1.3421\n",
      "Iteration 4400/28125, Loss: 1.5105\n",
      "Iteration 4500/28125, Loss: 1.9461\n",
      "Iteration 4600/28125, Loss: 1.7058\n",
      "Iteration 4700/28125, Loss: 1.5216\n",
      "Iteration 4800/28125, Loss: 2.4237\n",
      "Iteration 4900/28125, Loss: 1.5262\n",
      "Iteration 5000/28125, Loss: 1.4437\n",
      "Iteration 5100/28125, Loss: 1.5928\n",
      "Iteration 5200/28125, Loss: 1.6932\n",
      "Iteration 5300/28125, Loss: 1.6503\n",
      "Iteration 5400/28125, Loss: 1.5231\n",
      "Iteration 5500/28125, Loss: 2.4563\n",
      "Iteration 5600/28125, Loss: 1.4400\n",
      "Iteration 5700/28125, Loss: 1.8719\n",
      "Iteration 5800/28125, Loss: 1.6824\n",
      "Iteration 5900/28125, Loss: 0.7895\n",
      "Iteration 6000/28125, Loss: 1.0586\n",
      "Iteration 6100/28125, Loss: 1.2175\n",
      "Iteration 6200/28125, Loss: 1.8390\n",
      "Iteration 6300/28125, Loss: 1.4600\n",
      "Iteration 6400/28125, Loss: 1.4444\n",
      "Iteration 6500/28125, Loss: 1.1994\n",
      "Iteration 6600/28125, Loss: 1.5005\n",
      "Iteration 6700/28125, Loss: 1.7081\n",
      "Iteration 6800/28125, Loss: 1.5133\n",
      "Iteration 6900/28125, Loss: 1.4709\n",
      "Iteration 7000/28125, Loss: 1.2073\n",
      "Iteration 7100/28125, Loss: 1.3934\n",
      "Iteration 7200/28125, Loss: 1.0387\n",
      "Iteration 7300/28125, Loss: 1.7349\n",
      "Iteration 7400/28125, Loss: 1.6933\n",
      "Iteration 7500/28125, Loss: 1.8600\n",
      "Iteration 7600/28125, Loss: 1.7462\n",
      "Iteration 7700/28125, Loss: 1.9402\n",
      "Iteration 7800/28125, Loss: 1.3275\n",
      "Iteration 7900/28125, Loss: 1.8709\n",
      "Iteration 8000/28125, Loss: 1.5856\n",
      "Iteration 8100/28125, Loss: 1.2957\n",
      "Iteration 8200/28125, Loss: 1.5884\n",
      "Iteration 8300/28125, Loss: 1.5303\n",
      "Iteration 8400/28125, Loss: 1.3478\n",
      "Iteration 8500/28125, Loss: 1.3282\n",
      "Iteration 8600/28125, Loss: 1.7810\n",
      "Iteration 8700/28125, Loss: 1.0289\n",
      "Iteration 8800/28125, Loss: 1.6076\n",
      "Iteration 8900/28125, Loss: 1.9862\n",
      "Iteration 9000/28125, Loss: 1.5401\n",
      "Iteration 9100/28125, Loss: 1.1397\n",
      "Iteration 9200/28125, Loss: 1.6345\n",
      "Iteration 9300/28125, Loss: 1.2752\n",
      "Iteration 9400/28125, Loss: 1.0146\n",
      "Iteration 9500/28125, Loss: 1.2904\n",
      "Iteration 9600/28125, Loss: 1.5310\n",
      "Iteration 9700/28125, Loss: 1.5490\n",
      "Iteration 9800/28125, Loss: 1.9194\n",
      "Iteration 9900/28125, Loss: 1.8144\n",
      "Iteration 10000/28125, Loss: 1.4133\n",
      "Iteration 10100/28125, Loss: 1.6652\n",
      "Iteration 10200/28125, Loss: 1.3627\n",
      "Iteration 10300/28125, Loss: 1.1455\n",
      "Iteration 10400/28125, Loss: 1.8445\n",
      "Iteration 10500/28125, Loss: 1.5471\n",
      "Iteration 10600/28125, Loss: 1.5525\n",
      "Iteration 10700/28125, Loss: 1.3802\n",
      "Iteration 10800/28125, Loss: 1.2831\n",
      "Iteration 10900/28125, Loss: 0.9482\n",
      "Iteration 11000/28125, Loss: 1.2404\n",
      "Iteration 11100/28125, Loss: 1.3720\n",
      "Iteration 11200/28125, Loss: 1.4305\n",
      "Iteration 11300/28125, Loss: 1.4095\n",
      "Iteration 11400/28125, Loss: 1.3199\n",
      "Iteration 11500/28125, Loss: 1.9229\n",
      "Iteration 11600/28125, Loss: 1.2184\n",
      "Iteration 11700/28125, Loss: 1.4650\n",
      "Iteration 11800/28125, Loss: 1.6371\n",
      "Iteration 11900/28125, Loss: 1.1019\n",
      "Iteration 12000/28125, Loss: 1.5876\n",
      "Iteration 12100/28125, Loss: 1.9725\n",
      "Iteration 12200/28125, Loss: 1.4085\n",
      "Iteration 12300/28125, Loss: 1.4180\n",
      "Iteration 12400/28125, Loss: 2.3399\n",
      "Iteration 12500/28125, Loss: 1.7832\n",
      "Iteration 12600/28125, Loss: 1.4688\n",
      "Iteration 12700/28125, Loss: 1.8081\n",
      "Iteration 12800/28125, Loss: 1.5017\n",
      "Iteration 12900/28125, Loss: 1.0832\n",
      "Iteration 13000/28125, Loss: 1.5899\n",
      "Iteration 13100/28125, Loss: 1.1867\n",
      "Iteration 13200/28125, Loss: 1.9199\n",
      "Iteration 13300/28125, Loss: 1.6518\n",
      "Iteration 13400/28125, Loss: 1.5459\n",
      "Iteration 13500/28125, Loss: 1.2071\n",
      "Iteration 13600/28125, Loss: 1.3662\n",
      "Iteration 13700/28125, Loss: 1.4524\n",
      "Iteration 13800/28125, Loss: 1.0846\n",
      "Iteration 13900/28125, Loss: 1.0036\n",
      "Iteration 14000/28125, Loss: 1.7702\n",
      "Iteration 14100/28125, Loss: 1.1483\n",
      "Iteration 14200/28125, Loss: 1.2372\n",
      "Iteration 14300/28125, Loss: 1.5806\n",
      "Iteration 14400/28125, Loss: 1.8756\n",
      "Iteration 14500/28125, Loss: 1.8117\n",
      "Iteration 14600/28125, Loss: 1.5600\n",
      "Iteration 14700/28125, Loss: 1.4413\n",
      "Iteration 14800/28125, Loss: 1.5542\n",
      "Iteration 14900/28125, Loss: 1.4564\n",
      "Iteration 15000/28125, Loss: 1.5070\n",
      "Iteration 15100/28125, Loss: 1.3255\n",
      "Iteration 15200/28125, Loss: 1.5981\n",
      "Iteration 15300/28125, Loss: 1.8725\n",
      "Iteration 15400/28125, Loss: 1.8091\n",
      "Iteration 15500/28125, Loss: 1.0628\n",
      "Iteration 15600/28125, Loss: 1.7665\n",
      "Iteration 15700/28125, Loss: 1.2138\n",
      "Iteration 15800/28125, Loss: 1.5014\n",
      "Iteration 15900/28125, Loss: 1.5761\n",
      "Iteration 16000/28125, Loss: 1.1221\n",
      "Iteration 16100/28125, Loss: 1.6452\n",
      "Iteration 16200/28125, Loss: 1.8242\n",
      "Iteration 16300/28125, Loss: 1.0638\n",
      "Iteration 16400/28125, Loss: 1.4322\n",
      "Iteration 16500/28125, Loss: 1.7649\n",
      "Iteration 16600/28125, Loss: 1.4443\n",
      "Iteration 16700/28125, Loss: 1.4417\n",
      "Iteration 16800/28125, Loss: 1.6457\n",
      "Iteration 16900/28125, Loss: 1.5257\n",
      "Iteration 17000/28125, Loss: 1.5904\n",
      "Iteration 17100/28125, Loss: 1.3528\n",
      "Iteration 17200/28125, Loss: 1.3377\n",
      "Iteration 17300/28125, Loss: 1.7788\n",
      "Iteration 17400/28125, Loss: 1.5932\n",
      "Iteration 17500/28125, Loss: 1.4944\n",
      "Iteration 17600/28125, Loss: 1.4991\n",
      "Iteration 17700/28125, Loss: 1.7722\n",
      "Iteration 17800/28125, Loss: 1.3546\n",
      "Iteration 17900/28125, Loss: 1.3054\n",
      "Iteration 18000/28125, Loss: 1.6292\n",
      "Iteration 18100/28125, Loss: 1.9294\n",
      "Iteration 18200/28125, Loss: 1.6813\n",
      "Iteration 18300/28125, Loss: 1.3024\n",
      "Iteration 18400/28125, Loss: 1.3070\n",
      "Iteration 18500/28125, Loss: 2.1952\n",
      "Iteration 18600/28125, Loss: 1.6129\n",
      "Iteration 18700/28125, Loss: 1.4273\n",
      "Iteration 18800/28125, Loss: 1.5274\n",
      "Iteration 18900/28125, Loss: 1.4170\n",
      "Iteration 19000/28125, Loss: 1.7982\n",
      "Iteration 19100/28125, Loss: 1.3638\n",
      "Iteration 19200/28125, Loss: 1.3906\n",
      "Iteration 19300/28125, Loss: 1.1233\n",
      "Iteration 19400/28125, Loss: 1.7405\n",
      "Iteration 19500/28125, Loss: 1.6177\n",
      "Iteration 19600/28125, Loss: 1.6369\n",
      "Iteration 19700/28125, Loss: 1.3818\n",
      "Iteration 19800/28125, Loss: 1.7307\n",
      "Iteration 19900/28125, Loss: 1.5249\n",
      "Iteration 20000/28125, Loss: 1.5027\n",
      "Iteration 20100/28125, Loss: 1.6041\n",
      "Iteration 20200/28125, Loss: 1.9865\n",
      "Iteration 20300/28125, Loss: 1.3828\n",
      "Iteration 20400/28125, Loss: 1.8774\n",
      "Iteration 20500/28125, Loss: 1.3961\n",
      "Iteration 20600/28125, Loss: 1.4721\n",
      "Iteration 20700/28125, Loss: 1.5076\n",
      "Iteration 20800/28125, Loss: 1.3929\n",
      "Iteration 20900/28125, Loss: 1.4080\n",
      "Iteration 21000/28125, Loss: 1.3818\n",
      "Iteration 21100/28125, Loss: 1.4599\n",
      "Iteration 21200/28125, Loss: 1.8806\n",
      "Iteration 21300/28125, Loss: 1.2440\n",
      "Iteration 21400/28125, Loss: 1.8443\n",
      "Iteration 21500/28125, Loss: 1.7172\n",
      "Iteration 21600/28125, Loss: 1.8953\n",
      "Iteration 21700/28125, Loss: 1.3313\n",
      "Iteration 21800/28125, Loss: 1.3818\n",
      "Iteration 21900/28125, Loss: 1.2259\n",
      "Iteration 22000/28125, Loss: 0.9644\n",
      "Iteration 22100/28125, Loss: 1.6927\n",
      "Iteration 22200/28125, Loss: 1.2391\n",
      "Iteration 22300/28125, Loss: 1.4545\n",
      "Iteration 22400/28125, Loss: 1.6652\n",
      "Iteration 22500/28125, Loss: 2.1761\n",
      "Iteration 22600/28125, Loss: 1.2911\n",
      "Iteration 22700/28125, Loss: 1.3997\n",
      "Iteration 22800/28125, Loss: 1.6512\n",
      "Iteration 22900/28125, Loss: 1.8388\n",
      "Iteration 23000/28125, Loss: 1.5870\n",
      "Iteration 23100/28125, Loss: 1.8978\n",
      "Iteration 23200/28125, Loss: 1.2865\n",
      "Iteration 23300/28125, Loss: 1.5327\n",
      "Iteration 23400/28125, Loss: 1.1369\n",
      "Iteration 23500/28125, Loss: 1.3695\n",
      "Iteration 23600/28125, Loss: 1.9608\n",
      "Iteration 23700/28125, Loss: 1.0458\n",
      "Iteration 23800/28125, Loss: 1.4445\n",
      "Iteration 23900/28125, Loss: 1.9257\n",
      "Iteration 24000/28125, Loss: 1.6732\n",
      "Iteration 24100/28125, Loss: 1.3571\n",
      "Iteration 24200/28125, Loss: 1.2485\n",
      "Iteration 24300/28125, Loss: 1.8047\n",
      "Iteration 24400/28125, Loss: 1.4739\n",
      "Iteration 24500/28125, Loss: 1.8571\n",
      "Iteration 24600/28125, Loss: 1.5909\n",
      "Iteration 24700/28125, Loss: 1.8762\n",
      "Iteration 24800/28125, Loss: 1.7489\n",
      "Iteration 24900/28125, Loss: 1.4426\n",
      "Iteration 25000/28125, Loss: 1.5876\n",
      "Iteration 25100/28125, Loss: 1.3568\n",
      "Iteration 25200/28125, Loss: 1.7125\n",
      "Iteration 25300/28125, Loss: 1.2204\n",
      "Iteration 25400/28125, Loss: 1.5345\n",
      "Iteration 25500/28125, Loss: 1.2087\n",
      "Iteration 25600/28125, Loss: 1.5572\n",
      "Iteration 25700/28125, Loss: 1.4622\n",
      "Iteration 25800/28125, Loss: 1.5407\n",
      "Iteration 25900/28125, Loss: 1.8927\n",
      "Iteration 26000/28125, Loss: 1.9112\n",
      "Iteration 26100/28125, Loss: 1.6640\n",
      "Iteration 26200/28125, Loss: 1.7898\n",
      "Iteration 26300/28125, Loss: 1.4196\n",
      "Iteration 26400/28125, Loss: 1.9682\n",
      "Iteration 26500/28125, Loss: 1.1459\n",
      "Iteration 26600/28125, Loss: 1.7440\n",
      "Iteration 26700/28125, Loss: 1.9866\n",
      "Iteration 26800/28125, Loss: 1.5788\n",
      "Iteration 26900/28125, Loss: 1.7032\n",
      "Iteration 27000/28125, Loss: 2.0783\n",
      "Iteration 27100/28125, Loss: 1.6213\n",
      "Iteration 27200/28125, Loss: 1.7785\n",
      "Iteration 27300/28125, Loss: 1.5322\n",
      "Iteration 27400/28125, Loss: 1.2376\n",
      "Iteration 27500/28125, Loss: 1.8639\n",
      "Iteration 27600/28125, Loss: 1.5692\n",
      "Iteration 27700/28125, Loss: 1.5163\n",
      "Iteration 27800/28125, Loss: 1.3902\n",
      "Iteration 27900/28125, Loss: 1.6173\n",
      "Iteration 28000/28125, Loss: 1.2586\n",
      "Iteration 28100/28125, Loss: 1.5874\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 2.0694\n",
      "Iteration 200/14025, Loss: 2.3002\n",
      "Iteration 300/14025, Loss: 1.6738\n",
      "Iteration 400/14025, Loss: 2.1042\n",
      "Iteration 500/14025, Loss: 1.8218\n",
      "Iteration 600/14025, Loss: 1.4919\n",
      "Iteration 700/14025, Loss: 1.5769\n",
      "Iteration 800/14025, Loss: 1.6340\n",
      "Iteration 900/14025, Loss: 1.3579\n",
      "Iteration 1000/14025, Loss: 1.7869\n",
      "Iteration 1100/14025, Loss: 1.6187\n",
      "Iteration 1200/14025, Loss: 1.5731\n",
      "Iteration 1300/14025, Loss: 1.5730\n",
      "Iteration 1400/14025, Loss: 1.6417\n",
      "Iteration 1500/14025, Loss: 1.9325\n",
      "Iteration 1600/14025, Loss: 1.7346\n",
      "Iteration 1700/14025, Loss: 1.6065\n",
      "Iteration 1800/14025, Loss: 1.7469\n",
      "Iteration 1900/14025, Loss: 1.4412\n",
      "Iteration 2000/14025, Loss: 1.7518\n",
      "Iteration 2100/14025, Loss: 1.9091\n",
      "Iteration 2200/14025, Loss: 1.6570\n",
      "Iteration 2300/14025, Loss: 1.9386\n",
      "Iteration 2400/14025, Loss: 2.0481\n",
      "Iteration 2500/14025, Loss: 1.7845\n",
      "Iteration 2600/14025, Loss: 1.4886\n",
      "Iteration 2700/14025, Loss: 1.4641\n",
      "Iteration 2800/14025, Loss: 1.4463\n",
      "Iteration 2900/14025, Loss: 1.4109\n",
      "Iteration 3000/14025, Loss: 1.1276\n",
      "Iteration 3100/14025, Loss: 1.7767\n",
      "Iteration 3200/14025, Loss: 1.5079\n",
      "Iteration 3300/14025, Loss: 1.3885\n",
      "Iteration 3400/14025, Loss: 1.7494\n",
      "Iteration 3500/14025, Loss: 1.4258\n",
      "Iteration 3600/14025, Loss: 1.4180\n",
      "Iteration 3700/14025, Loss: 1.4940\n",
      "Iteration 3800/14025, Loss: 1.6558\n",
      "Iteration 3900/14025, Loss: 1.3627\n",
      "Iteration 4000/14025, Loss: 1.6457\n",
      "Iteration 4100/14025, Loss: 1.4682\n",
      "Iteration 4200/14025, Loss: 1.6274\n",
      "Iteration 4300/14025, Loss: 1.5291\n",
      "Iteration 4400/14025, Loss: 1.5985\n",
      "Iteration 4500/14025, Loss: 1.4798\n",
      "Iteration 4600/14025, Loss: 1.5724\n",
      "Iteration 4700/14025, Loss: 1.2778\n",
      "Iteration 4800/14025, Loss: 1.4044\n",
      "Iteration 4900/14025, Loss: 1.5038\n",
      "Iteration 5000/14025, Loss: 1.4580\n",
      "Iteration 5100/14025, Loss: 1.2948\n",
      "Iteration 5200/14025, Loss: 2.1070\n",
      "Iteration 5300/14025, Loss: 1.6945\n",
      "Iteration 5400/14025, Loss: 1.3530\n",
      "Iteration 5500/14025, Loss: 1.3683\n",
      "Iteration 5600/14025, Loss: 1.4831\n",
      "Iteration 5700/14025, Loss: 1.3067\n",
      "Iteration 5800/14025, Loss: 1.4488\n",
      "Iteration 5900/14025, Loss: 1.8033\n",
      "Iteration 6000/14025, Loss: 1.5405\n",
      "Iteration 6100/14025, Loss: 1.5089\n",
      "Iteration 6200/14025, Loss: 1.7614\n",
      "Iteration 6300/14025, Loss: 1.5307\n",
      "Iteration 6400/14025, Loss: 1.6145\n",
      "Iteration 6500/14025, Loss: 1.4491\n",
      "Iteration 6600/14025, Loss: 1.9400\n",
      "Iteration 6700/14025, Loss: 1.3290\n",
      "Iteration 6800/14025, Loss: 1.4300\n",
      "Iteration 6900/14025, Loss: 1.3841\n",
      "Iteration 7000/14025, Loss: 1.5823\n",
      "Iteration 7100/14025, Loss: 1.2184\n",
      "Iteration 7200/14025, Loss: 1.6991\n",
      "Iteration 7300/14025, Loss: 1.4069\n",
      "Iteration 7400/14025, Loss: 1.4302\n",
      "Iteration 7500/14025, Loss: 1.8750\n",
      "Iteration 7600/14025, Loss: 1.6642\n",
      "Iteration 7700/14025, Loss: 1.9201\n",
      "Iteration 7800/14025, Loss: 1.8452\n",
      "Iteration 7900/14025, Loss: 1.3238\n",
      "Iteration 8000/14025, Loss: 1.5760\n",
      "Iteration 8100/14025, Loss: 1.6788\n",
      "Iteration 8200/14025, Loss: 1.7391\n",
      "Iteration 8300/14025, Loss: 1.6528\n",
      "Iteration 8400/14025, Loss: 1.5446\n",
      "Iteration 8500/14025, Loss: 1.5160\n",
      "Iteration 8600/14025, Loss: 1.3005\n",
      "Iteration 8700/14025, Loss: 1.5705\n",
      "Iteration 8800/14025, Loss: 1.3824\n",
      "Iteration 8900/14025, Loss: 1.4372\n",
      "Iteration 9000/14025, Loss: 1.6423\n",
      "Iteration 9100/14025, Loss: 1.8417\n",
      "Iteration 9200/14025, Loss: 1.2850\n",
      "Iteration 9300/14025, Loss: 1.5836\n",
      "Iteration 9400/14025, Loss: 1.4838\n",
      "Iteration 9500/14025, Loss: 1.6692\n",
      "Iteration 9600/14025, Loss: 1.4411\n",
      "Iteration 9700/14025, Loss: 1.5779\n",
      "Iteration 9800/14025, Loss: 1.5294\n",
      "Iteration 9900/14025, Loss: 1.6790\n",
      "Iteration 10000/14025, Loss: 1.6604\n",
      "Iteration 10100/14025, Loss: 1.5362\n",
      "Iteration 10200/14025, Loss: 1.8233\n",
      "Iteration 10300/14025, Loss: 1.5470\n",
      "Iteration 10400/14025, Loss: 1.4293\n",
      "Iteration 10500/14025, Loss: 1.4446\n",
      "Iteration 10600/14025, Loss: 1.7121\n",
      "Iteration 10700/14025, Loss: 1.9873\n",
      "Iteration 10800/14025, Loss: 1.7059\n",
      "Iteration 10900/14025, Loss: 1.5590\n",
      "Iteration 11000/14025, Loss: 1.2460\n",
      "Iteration 11100/14025, Loss: 1.5934\n",
      "Iteration 11200/14025, Loss: 1.8112\n",
      "Iteration 11300/14025, Loss: 1.4667\n",
      "Iteration 11400/14025, Loss: 1.8354\n",
      "Iteration 11500/14025, Loss: 1.7908\n",
      "Iteration 11600/14025, Loss: 1.3574\n",
      "Iteration 11700/14025, Loss: 1.4796\n",
      "Iteration 11800/14025, Loss: 1.9589\n",
      "Iteration 11900/14025, Loss: 1.5744\n",
      "Iteration 12000/14025, Loss: 1.7417\n",
      "Iteration 12100/14025, Loss: 1.3696\n",
      "Iteration 12200/14025, Loss: 1.4721\n",
      "Iteration 12300/14025, Loss: 1.5773\n",
      "Iteration 12400/14025, Loss: 1.8243\n",
      "Iteration 12500/14025, Loss: 1.6865\n",
      "Iteration 12600/14025, Loss: 1.8307\n",
      "Iteration 12700/14025, Loss: 1.5382\n",
      "Iteration 12800/14025, Loss: 1.7727\n",
      "Iteration 12900/14025, Loss: 1.6955\n",
      "Iteration 13000/14025, Loss: 1.9103\n",
      "Iteration 13100/14025, Loss: 1.7096\n",
      "Iteration 13200/14025, Loss: 1.6348\n",
      "Iteration 13300/14025, Loss: 1.4805\n",
      "Iteration 13400/14025, Loss: 1.8226\n",
      "Iteration 13500/14025, Loss: 1.7912\n",
      "Iteration 13600/14025, Loss: 1.8566\n",
      "Iteration 13700/14025, Loss: 1.4078\n",
      "Iteration 13800/14025, Loss: 1.6273\n",
      "Iteration 13900/14025, Loss: 1.4251\n",
      "Iteration 14000/14025, Loss: 1.6033\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 1.9746\n",
      "Iteration 200/6975, Loss: 2.1088\n",
      "Iteration 300/6975, Loss: 1.6811\n",
      "Iteration 400/6975, Loss: 1.7346\n",
      "Iteration 500/6975, Loss: 1.7167\n",
      "Iteration 600/6975, Loss: 1.7348\n",
      "Iteration 700/6975, Loss: 1.7242\n",
      "Iteration 800/6975, Loss: 1.7375\n",
      "Iteration 900/6975, Loss: 1.6169\n",
      "Iteration 1000/6975, Loss: 1.6779\n",
      "Iteration 1100/6975, Loss: 1.6605\n",
      "Iteration 1200/6975, Loss: 1.6198\n",
      "Iteration 1300/6975, Loss: 1.4987\n",
      "Iteration 1400/6975, Loss: 1.6329\n",
      "Iteration 1500/6975, Loss: 1.3873\n",
      "Iteration 1600/6975, Loss: 1.4509\n",
      "Iteration 1700/6975, Loss: 1.4411\n",
      "Iteration 1800/6975, Loss: 1.3975\n",
      "Iteration 1900/6975, Loss: 1.6279\n",
      "Iteration 2000/6975, Loss: 1.5484\n",
      "Iteration 2100/6975, Loss: 1.6592\n",
      "Iteration 2200/6975, Loss: 1.4638\n",
      "Iteration 2300/6975, Loss: 1.5716\n",
      "Iteration 2400/6975, Loss: 1.4916\n",
      "Iteration 2500/6975, Loss: 1.5688\n",
      "Iteration 2600/6975, Loss: 1.7412\n",
      "Iteration 2700/6975, Loss: 1.3833\n",
      "Iteration 2800/6975, Loss: 1.5461\n",
      "Iteration 2900/6975, Loss: 1.6563\n",
      "Iteration 3000/6975, Loss: 1.5429\n",
      "Iteration 3100/6975, Loss: 1.5700\n",
      "Iteration 3200/6975, Loss: 1.6455\n",
      "Iteration 3300/6975, Loss: 1.7656\n",
      "Iteration 3400/6975, Loss: 1.3967\n",
      "Iteration 3500/6975, Loss: 1.3536\n",
      "Iteration 3600/6975, Loss: 1.7183\n",
      "Iteration 3700/6975, Loss: 1.4831\n",
      "Iteration 3800/6975, Loss: 1.6879\n",
      "Iteration 3900/6975, Loss: 1.8713\n",
      "Iteration 4000/6975, Loss: 1.6694\n",
      "Iteration 4100/6975, Loss: 1.4880\n",
      "Iteration 4200/6975, Loss: 1.5350\n",
      "Iteration 4300/6975, Loss: 1.4310\n",
      "Iteration 4400/6975, Loss: 1.6184\n",
      "Iteration 4500/6975, Loss: 1.6439\n",
      "Iteration 4600/6975, Loss: 1.5751\n",
      "Iteration 4700/6975, Loss: 1.5692\n",
      "Iteration 4800/6975, Loss: 1.5318\n",
      "Iteration 4900/6975, Loss: 1.6438\n",
      "Iteration 5000/6975, Loss: 1.6668\n",
      "Iteration 5100/6975, Loss: 1.6732\n",
      "Iteration 5200/6975, Loss: 1.6407\n",
      "Iteration 5300/6975, Loss: 1.5443\n",
      "Iteration 5400/6975, Loss: 1.6993\n",
      "Iteration 5500/6975, Loss: 1.6920\n",
      "Iteration 5600/6975, Loss: 1.6202\n",
      "Iteration 5700/6975, Loss: 1.6947\n",
      "Iteration 5800/6975, Loss: 1.5041\n",
      "Iteration 5900/6975, Loss: 1.7540\n",
      "Iteration 6000/6975, Loss: 1.7453\n",
      "Iteration 6100/6975, Loss: 1.5949\n",
      "Iteration 6200/6975, Loss: 1.8520\n",
      "Iteration 6300/6975, Loss: 1.7886\n",
      "Iteration 6400/6975, Loss: 1.7962\n",
      "Iteration 6500/6975, Loss: 1.7662\n",
      "Iteration 6600/6975, Loss: 1.6005\n",
      "Iteration 6700/6975, Loss: 1.8151\n",
      "Iteration 6800/6975, Loss: 1.6709\n",
      "Iteration 6900/6975, Loss: 1.7476\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 1.9642\n",
      "Iteration 200/3450, Loss: 1.8325\n",
      "Iteration 300/3450, Loss: 1.4956\n",
      "Iteration 400/3450, Loss: 1.5600\n",
      "Iteration 500/3450, Loss: 1.6474\n",
      "Iteration 600/3450, Loss: 1.5440\n",
      "Iteration 700/3450, Loss: 1.7111\n",
      "Iteration 800/3450, Loss: 1.3843\n",
      "Iteration 900/3450, Loss: 1.3203\n",
      "Iteration 1000/3450, Loss: 1.5437\n",
      "Iteration 1100/3450, Loss: 1.3438\n",
      "Iteration 1200/3450, Loss: 1.3953\n",
      "Iteration 1300/3450, Loss: 1.7583\n",
      "Iteration 1400/3450, Loss: 1.5172\n",
      "Iteration 1500/3450, Loss: 1.5959\n",
      "Iteration 1600/3450, Loss: 1.4361\n",
      "Iteration 1700/3450, Loss: 1.5256\n",
      "Iteration 1800/3450, Loss: 1.8765\n",
      "Iteration 1900/3450, Loss: 1.6175\n",
      "Iteration 2000/3450, Loss: 1.5286\n",
      "Iteration 2100/3450, Loss: 1.4248\n",
      "Iteration 2200/3450, Loss: 1.7556\n",
      "Iteration 2300/3450, Loss: 1.6590\n",
      "Iteration 2400/3450, Loss: 1.5225\n",
      "Iteration 2500/3450, Loss: 1.6937\n",
      "Iteration 2600/3450, Loss: 1.6236\n",
      "Iteration 2700/3450, Loss: 1.6649\n",
      "Iteration 2800/3450, Loss: 1.6766\n",
      "Iteration 2900/3450, Loss: 1.6811\n",
      "Iteration 3000/3450, Loss: 1.6422\n",
      "Iteration 3100/3450, Loss: 1.7531\n",
      "Iteration 3200/3450, Loss: 1.8278\n",
      "Iteration 3300/3450, Loss: 1.6667\n",
      "Iteration 3400/3450, Loss: 1.6622\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 1.9151\n",
      "Iteration 200/1725, Loss: 1.5220\n",
      "Iteration 300/1725, Loss: 1.5477\n",
      "Iteration 400/1725, Loss: 1.5712\n",
      "Iteration 500/1725, Loss: 1.5937\n",
      "Iteration 600/1725, Loss: 1.5012\n",
      "Iteration 700/1725, Loss: 1.4323\n",
      "Iteration 800/1725, Loss: 1.4998\n",
      "Iteration 900/1725, Loss: 1.7768\n",
      "Iteration 1000/1725, Loss: 1.5037\n",
      "Iteration 1100/1725, Loss: 1.6900\n",
      "Iteration 1200/1725, Loss: 1.5897\n",
      "Iteration 1300/1725, Loss: 1.7120\n",
      "Iteration 1400/1725, Loss: 1.6660\n",
      "Iteration 1500/1725, Loss: 1.7543\n",
      "Iteration 1600/1725, Loss: 1.7721\n",
      "Iteration 1700/1725, Loss: 1.6510\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 1.5763\n",
      "Iteration 200/825, Loss: 1.5532\n",
      "Iteration 300/825, Loss: 1.4926\n",
      "Iteration 400/825, Loss: 1.5757\n",
      "Iteration 500/825, Loss: 1.5435\n",
      "Iteration 600/825, Loss: 1.5947\n",
      "Iteration 700/825, Loss: 1.6503\n",
      "Iteration 800/825, Loss: 1.7705\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "train accuracy: 0.7516666666666667\n",
      "val accuracy: 0.333\n",
      "batch_size: 256 reg: 0.1 rate: 0.0001\n",
      "Iteration 100/375, Loss: 2.5132\n",
      "Iteration 200/375, Loss: 1.9191\n",
      "Iteration 300/375, Loss: 1.7524\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 4.1398\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 1.8789\n",
      "Iteration 200/28125, Loss: 2.9578\n",
      "Iteration 300/28125, Loss: 3.1247\n",
      "Iteration 400/28125, Loss: 3.1848\n",
      "Iteration 500/28125, Loss: 1.9673\n",
      "Iteration 600/28125, Loss: 1.3478\n",
      "Iteration 700/28125, Loss: 1.0046\n",
      "Iteration 800/28125, Loss: 1.8479\n",
      "Iteration 900/28125, Loss: 1.8395\n",
      "Iteration 1000/28125, Loss: 1.8777\n",
      "Iteration 1100/28125, Loss: 1.2381\n",
      "Iteration 1200/28125, Loss: 0.9271\n",
      "Iteration 1300/28125, Loss: 3.7708\n",
      "Iteration 1400/28125, Loss: 1.7489\n",
      "Iteration 1500/28125, Loss: 0.9239\n",
      "Iteration 1600/28125, Loss: 0.9332\n",
      "Iteration 1700/28125, Loss: 2.3110\n",
      "Iteration 1800/28125, Loss: 1.3163\n",
      "Iteration 1900/28125, Loss: 1.3109\n",
      "Iteration 2000/28125, Loss: 1.9639\n",
      "Iteration 2100/28125, Loss: 1.0482\n",
      "Iteration 2200/28125, Loss: 1.3328\n",
      "Iteration 2300/28125, Loss: 0.9429\n",
      "Iteration 2400/28125, Loss: 1.2790\n",
      "Iteration 2500/28125, Loss: 1.1752\n",
      "Iteration 2600/28125, Loss: 0.9559\n",
      "Iteration 2700/28125, Loss: 0.6154\n",
      "Iteration 2800/28125, Loss: 1.9927\n",
      "Iteration 2900/28125, Loss: 1.5551\n",
      "Iteration 3000/28125, Loss: 1.3259\n",
      "Iteration 3100/28125, Loss: 2.5432\n",
      "Iteration 3200/28125, Loss: 0.8159\n",
      "Iteration 3300/28125, Loss: 0.2525\n",
      "Iteration 3400/28125, Loss: 0.7022\n",
      "Iteration 3500/28125, Loss: 0.7100\n",
      "Iteration 3600/28125, Loss: 1.4004\n",
      "Iteration 3700/28125, Loss: 1.1446\n",
      "Iteration 3800/28125, Loss: 0.4721\n",
      "Iteration 3900/28125, Loss: 0.5474\n",
      "Iteration 4000/28125, Loss: 1.5548\n",
      "Iteration 4100/28125, Loss: 0.2283\n",
      "Iteration 4200/28125, Loss: 1.6052\n",
      "Iteration 4300/28125, Loss: 0.5075\n",
      "Iteration 4400/28125, Loss: 1.0276\n",
      "Iteration 4500/28125, Loss: 1.5003\n",
      "Iteration 4600/28125, Loss: 0.9431\n",
      "Iteration 4700/28125, Loss: 1.2155\n",
      "Iteration 4800/28125, Loss: 1.6356\n",
      "Iteration 4900/28125, Loss: 1.0888\n",
      "Iteration 5000/28125, Loss: 1.3224\n",
      "Iteration 5100/28125, Loss: 0.9617\n",
      "Iteration 5200/28125, Loss: 1.0502\n",
      "Iteration 5300/28125, Loss: 1.0564\n",
      "Iteration 5400/28125, Loss: 0.6501\n",
      "Iteration 5500/28125, Loss: 1.7715\n",
      "Iteration 5600/28125, Loss: 0.4936\n",
      "Iteration 5700/28125, Loss: 1.9182\n",
      "Iteration 5800/28125, Loss: 0.9047\n",
      "Iteration 5900/28125, Loss: 0.1242\n",
      "Iteration 6000/28125, Loss: 0.7549\n",
      "Iteration 6100/28125, Loss: 0.3618\n",
      "Iteration 6200/28125, Loss: 1.0112\n",
      "Iteration 6300/28125, Loss: 0.9794\n",
      "Iteration 6400/28125, Loss: 0.2434\n",
      "Iteration 6500/28125, Loss: 0.4055\n",
      "Iteration 6600/28125, Loss: 0.5447\n",
      "Iteration 6700/28125, Loss: 0.9974\n",
      "Iteration 6800/28125, Loss: 1.9610\n",
      "Iteration 6900/28125, Loss: 0.5541\n",
      "Iteration 7000/28125, Loss: 0.4970\n",
      "Iteration 7100/28125, Loss: 0.2392\n",
      "Iteration 7200/28125, Loss: 0.2490\n",
      "Iteration 7300/28125, Loss: 1.2305\n",
      "Iteration 7400/28125, Loss: 0.6594\n",
      "Iteration 7500/28125, Loss: 0.5145\n",
      "Iteration 7600/28125, Loss: 0.3450\n",
      "Iteration 7700/28125, Loss: 1.7066\n",
      "Iteration 7800/28125, Loss: 0.8915\n",
      "Iteration 7900/28125, Loss: 1.0740\n",
      "Iteration 8000/28125, Loss: 0.7355\n",
      "Iteration 8100/28125, Loss: 0.3050\n",
      "Iteration 8200/28125, Loss: 0.9952\n",
      "Iteration 8300/28125, Loss: 0.6208\n",
      "Iteration 8400/28125, Loss: 0.7608\n",
      "Iteration 8500/28125, Loss: 0.3810\n",
      "Iteration 8600/28125, Loss: 0.8203\n",
      "Iteration 8700/28125, Loss: 0.1618\n",
      "Iteration 8800/28125, Loss: 0.5432\n",
      "Iteration 8900/28125, Loss: 1.8141\n",
      "Iteration 9000/28125, Loss: 0.8689\n",
      "Iteration 9100/28125, Loss: 0.2253\n",
      "Iteration 9200/28125, Loss: 0.7917\n",
      "Iteration 9300/28125, Loss: 0.6517\n",
      "Iteration 9400/28125, Loss: 0.2788\n",
      "Iteration 9500/28125, Loss: 0.1397\n",
      "Iteration 9600/28125, Loss: 0.6408\n",
      "Iteration 9700/28125, Loss: 0.8384\n",
      "Iteration 9800/28125, Loss: 0.7007\n",
      "Iteration 9900/28125, Loss: 0.8557\n",
      "Iteration 10000/28125, Loss: 0.1791\n",
      "Iteration 10100/28125, Loss: 0.9760\n",
      "Iteration 10200/28125, Loss: 0.6179\n",
      "Iteration 10300/28125, Loss: 0.4450\n",
      "Iteration 10400/28125, Loss: 0.8423\n",
      "Iteration 10500/28125, Loss: 0.6157\n",
      "Iteration 10600/28125, Loss: 0.5007\n",
      "Iteration 10700/28125, Loss: 0.7852\n",
      "Iteration 10800/28125, Loss: 0.5470\n",
      "Iteration 10900/28125, Loss: 0.2700\n",
      "Iteration 11000/28125, Loss: 0.4052\n",
      "Iteration 11100/28125, Loss: 0.4754\n",
      "Iteration 11200/28125, Loss: 0.4154\n",
      "Iteration 11300/28125, Loss: 0.2448\n",
      "Iteration 11400/28125, Loss: 0.4271\n",
      "Iteration 11500/28125, Loss: 0.4291\n",
      "Iteration 11600/28125, Loss: 0.3201\n",
      "Iteration 11700/28125, Loss: 0.6397\n",
      "Iteration 11800/28125, Loss: 0.2584\n",
      "Iteration 11900/28125, Loss: 0.2222\n",
      "Iteration 12000/28125, Loss: 0.5051\n",
      "Iteration 12100/28125, Loss: 1.1682\n",
      "Iteration 12200/28125, Loss: 0.2924\n",
      "Iteration 12300/28125, Loss: 0.3228\n",
      "Iteration 12400/28125, Loss: 1.3396\n",
      "Iteration 12500/28125, Loss: 0.4627\n",
      "Iteration 12600/28125, Loss: 0.5573\n",
      "Iteration 12700/28125, Loss: 0.8078\n",
      "Iteration 12800/28125, Loss: 0.3498\n",
      "Iteration 12900/28125, Loss: 0.1469\n",
      "Iteration 13000/28125, Loss: 0.6179\n",
      "Iteration 13100/28125, Loss: 0.1323\n",
      "Iteration 13200/28125, Loss: 1.3194\n",
      "Iteration 13300/28125, Loss: 0.3749\n",
      "Iteration 13400/28125, Loss: 0.5905\n",
      "Iteration 13500/28125, Loss: 0.3604\n",
      "Iteration 13600/28125, Loss: 0.2315\n",
      "Iteration 13700/28125, Loss: 0.3090\n",
      "Iteration 13800/28125, Loss: 0.1263\n",
      "Iteration 13900/28125, Loss: 0.1853\n",
      "Iteration 14000/28125, Loss: 0.3876\n",
      "Iteration 14100/28125, Loss: 0.2055\n",
      "Iteration 14200/28125, Loss: 0.1456\n",
      "Iteration 14300/28125, Loss: 0.4027\n",
      "Iteration 14400/28125, Loss: 0.9400\n",
      "Iteration 14500/28125, Loss: 0.5862\n",
      "Iteration 14600/28125, Loss: 0.4355\n",
      "Iteration 14700/28125, Loss: 0.2815\n",
      "Iteration 14800/28125, Loss: 0.4869\n",
      "Iteration 14900/28125, Loss: 0.4762\n",
      "Iteration 15000/28125, Loss: 0.7518\n",
      "Iteration 15100/28125, Loss: 0.2043\n",
      "Iteration 15200/28125, Loss: 0.3550\n",
      "Iteration 15300/28125, Loss: 0.9330\n",
      "Iteration 15400/28125, Loss: 0.6499\n",
      "Iteration 15500/28125, Loss: 0.2894\n",
      "Iteration 15600/28125, Loss: 0.4273\n",
      "Iteration 15700/28125, Loss: 0.1708\n",
      "Iteration 15800/28125, Loss: 0.3498\n",
      "Iteration 15900/28125, Loss: 0.4701\n",
      "Iteration 16000/28125, Loss: 0.2184\n",
      "Iteration 16100/28125, Loss: 0.5333\n",
      "Iteration 16200/28125, Loss: 0.8334\n",
      "Iteration 16300/28125, Loss: 0.1006\n",
      "Iteration 16400/28125, Loss: 0.7549\n",
      "Iteration 16500/28125, Loss: 0.4441\n",
      "Iteration 16600/28125, Loss: 0.2874\n",
      "Iteration 16700/28125, Loss: 0.3583\n",
      "Iteration 16800/28125, Loss: 0.6657\n",
      "Iteration 16900/28125, Loss: 0.2861\n",
      "Iteration 17000/28125, Loss: 0.3866\n",
      "Iteration 17100/28125, Loss: 0.1202\n",
      "Iteration 17200/28125, Loss: 0.2334\n",
      "Iteration 17300/28125, Loss: 0.3219\n",
      "Iteration 17400/28125, Loss: 0.2117\n",
      "Iteration 17500/28125, Loss: 0.4137\n",
      "Iteration 17600/28125, Loss: 0.2784\n",
      "Iteration 17700/28125, Loss: 0.4876\n",
      "Iteration 17800/28125, Loss: 0.2530\n",
      "Iteration 17900/28125, Loss: 0.3252\n",
      "Iteration 18000/28125, Loss: 0.3568\n",
      "Iteration 18100/28125, Loss: 0.4195\n",
      "Iteration 18200/28125, Loss: 0.4863\n",
      "Iteration 18300/28125, Loss: 0.6529\n",
      "Iteration 18400/28125, Loss: 0.1443\n",
      "Iteration 18500/28125, Loss: 0.6912\n",
      "Iteration 18600/28125, Loss: 0.3437\n",
      "Iteration 18700/28125, Loss: 0.1077\n",
      "Iteration 18800/28125, Loss: 0.3331\n",
      "Iteration 18900/28125, Loss: 0.3186\n",
      "Iteration 19000/28125, Loss: 0.3727\n",
      "Iteration 19100/28125, Loss: 0.2150\n",
      "Iteration 19200/28125, Loss: 0.2630\n",
      "Iteration 19300/28125, Loss: 0.2880\n",
      "Iteration 19400/28125, Loss: 0.2595\n",
      "Iteration 19500/28125, Loss: 0.4875\n",
      "Iteration 19600/28125, Loss: 0.2183\n",
      "Iteration 19700/28125, Loss: 0.2324\n",
      "Iteration 19800/28125, Loss: 0.2807\n",
      "Iteration 19900/28125, Loss: 0.3882\n",
      "Iteration 20000/28125, Loss: 0.8521\n",
      "Iteration 20100/28125, Loss: 0.4148\n",
      "Iteration 20200/28125, Loss: 0.4335\n",
      "Iteration 20300/28125, Loss: 0.1332\n",
      "Iteration 20400/28125, Loss: 0.4019\n",
      "Iteration 20500/28125, Loss: 0.1629\n",
      "Iteration 20600/28125, Loss: 0.2477\n",
      "Iteration 20700/28125, Loss: 0.4125\n",
      "Iteration 20800/28125, Loss: 0.1196\n",
      "Iteration 20900/28125, Loss: 0.2077\n",
      "Iteration 21000/28125, Loss: 0.2363\n",
      "Iteration 21100/28125, Loss: 0.2180\n",
      "Iteration 21200/28125, Loss: 0.5497\n",
      "Iteration 21300/28125, Loss: 0.1288\n",
      "Iteration 21400/28125, Loss: 0.1830\n",
      "Iteration 21500/28125, Loss: 0.4161\n",
      "Iteration 21600/28125, Loss: 0.3903\n",
      "Iteration 21700/28125, Loss: 0.3041\n",
      "Iteration 21800/28125, Loss: 0.3866\n",
      "Iteration 21900/28125, Loss: 0.2534\n",
      "Iteration 22000/28125, Loss: 0.0598\n",
      "Iteration 22100/28125, Loss: 0.3029\n",
      "Iteration 22200/28125, Loss: 0.1173\n",
      "Iteration 22300/28125, Loss: 0.2065\n",
      "Iteration 22400/28125, Loss: 0.2953\n",
      "Iteration 22500/28125, Loss: 0.4422\n",
      "Iteration 22600/28125, Loss: 0.2076\n",
      "Iteration 22700/28125, Loss: 0.3843\n",
      "Iteration 22800/28125, Loss: 0.2302\n",
      "Iteration 22900/28125, Loss: 0.5732\n",
      "Iteration 23000/28125, Loss: 0.3170\n",
      "Iteration 23100/28125, Loss: 0.3209\n",
      "Iteration 23200/28125, Loss: 0.1096\n",
      "Iteration 23300/28125, Loss: 0.2155\n",
      "Iteration 23400/28125, Loss: 0.0823\n",
      "Iteration 23500/28125, Loss: 0.0951\n",
      "Iteration 23600/28125, Loss: 0.2259\n",
      "Iteration 23700/28125, Loss: 0.0573\n",
      "Iteration 23800/28125, Loss: 0.2264\n",
      "Iteration 23900/28125, Loss: 0.2489\n",
      "Iteration 24000/28125, Loss: 0.2096\n",
      "Iteration 24100/28125, Loss: 0.0828\n",
      "Iteration 24200/28125, Loss: 0.2007\n",
      "Iteration 24300/28125, Loss: 0.6500\n",
      "Iteration 24400/28125, Loss: 0.1716\n",
      "Iteration 24500/28125, Loss: 0.3892\n",
      "Iteration 24600/28125, Loss: 0.2125\n",
      "Iteration 24700/28125, Loss: 0.3551\n",
      "Iteration 24800/28125, Loss: 0.2341\n",
      "Iteration 24900/28125, Loss: 0.3094\n",
      "Iteration 25000/28125, Loss: 0.3193\n",
      "Iteration 25100/28125, Loss: 0.2659\n",
      "Iteration 25200/28125, Loss: 0.2170\n",
      "Iteration 25300/28125, Loss: 0.0944\n",
      "Iteration 25400/28125, Loss: 0.3286\n",
      "Iteration 25500/28125, Loss: 0.1013\n",
      "Iteration 25600/28125, Loss: 0.3454\n",
      "Iteration 25700/28125, Loss: 0.2309\n",
      "Iteration 25800/28125, Loss: 0.1772\n",
      "Iteration 25900/28125, Loss: 0.3918\n",
      "Iteration 26000/28125, Loss: 0.5789\n",
      "Iteration 26100/28125, Loss: 0.3190\n",
      "Iteration 26200/28125, Loss: 0.2260\n",
      "Iteration 26300/28125, Loss: 0.1627\n",
      "Iteration 26400/28125, Loss: 0.3699\n",
      "Iteration 26500/28125, Loss: 0.1291\n",
      "Iteration 26600/28125, Loss: 0.1679\n",
      "Iteration 26700/28125, Loss: 0.3884\n",
      "Iteration 26800/28125, Loss: 0.1563\n",
      "Iteration 26900/28125, Loss: 0.1882\n",
      "Iteration 27000/28125, Loss: 0.4280\n",
      "Iteration 27100/28125, Loss: 0.1726\n",
      "Iteration 27200/28125, Loss: 0.2227\n",
      "Iteration 27300/28125, Loss: 0.1353\n",
      "Iteration 27400/28125, Loss: 0.1444\n",
      "Iteration 27500/28125, Loss: 0.2188\n",
      "Iteration 27600/28125, Loss: 0.1160\n",
      "Iteration 27700/28125, Loss: 0.1448\n",
      "Iteration 27800/28125, Loss: 0.1521\n",
      "Iteration 27900/28125, Loss: 0.1686\n",
      "Iteration 28000/28125, Loss: 0.0807\n",
      "Iteration 28100/28125, Loss: 0.3475\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 2.1696\n",
      "Iteration 200/14025, Loss: 2.6467\n",
      "Iteration 300/14025, Loss: 1.4762\n",
      "Iteration 400/14025, Loss: 1.9526\n",
      "Iteration 500/14025, Loss: 1.8412\n",
      "Iteration 600/14025, Loss: 1.3606\n",
      "Iteration 700/14025, Loss: 1.3424\n",
      "Iteration 800/14025, Loss: 1.2163\n",
      "Iteration 900/14025, Loss: 1.0468\n",
      "Iteration 1000/14025, Loss: 2.2700\n",
      "Iteration 1100/14025, Loss: 1.1930\n",
      "Iteration 1200/14025, Loss: 1.2193\n",
      "Iteration 1300/14025, Loss: 0.6819\n",
      "Iteration 1400/14025, Loss: 1.5608\n",
      "Iteration 1500/14025, Loss: 1.8936\n",
      "Iteration 1600/14025, Loss: 1.3519\n",
      "Iteration 1700/14025, Loss: 0.7837\n",
      "Iteration 1800/14025, Loss: 1.2307\n",
      "Iteration 1900/14025, Loss: 0.8257\n",
      "Iteration 2000/14025, Loss: 1.0775\n",
      "Iteration 2100/14025, Loss: 1.2441\n",
      "Iteration 2200/14025, Loss: 1.3165\n",
      "Iteration 2300/14025, Loss: 1.1381\n",
      "Iteration 2400/14025, Loss: 1.4122\n",
      "Iteration 2500/14025, Loss: 1.0962\n",
      "Iteration 2600/14025, Loss: 1.0548\n",
      "Iteration 2700/14025, Loss: 1.0377\n",
      "Iteration 2800/14025, Loss: 0.7835\n",
      "Iteration 2900/14025, Loss: 0.6486\n",
      "Iteration 3000/14025, Loss: 0.5902\n",
      "Iteration 3100/14025, Loss: 1.2401\n",
      "Iteration 3200/14025, Loss: 0.6689\n",
      "Iteration 3300/14025, Loss: 0.6542\n",
      "Iteration 3400/14025, Loss: 1.1156\n",
      "Iteration 3500/14025, Loss: 0.8862\n",
      "Iteration 3600/14025, Loss: 0.4422\n",
      "Iteration 3700/14025, Loss: 0.7029\n",
      "Iteration 3800/14025, Loss: 0.4761\n",
      "Iteration 3900/14025, Loss: 0.7349\n",
      "Iteration 4000/14025, Loss: 0.9337\n",
      "Iteration 4100/14025, Loss: 0.6428\n",
      "Iteration 4200/14025, Loss: 0.9219\n",
      "Iteration 4300/14025, Loss: 0.4886\n",
      "Iteration 4400/14025, Loss: 0.4816\n",
      "Iteration 4500/14025, Loss: 1.1292\n",
      "Iteration 4600/14025, Loss: 1.1773\n",
      "Iteration 4700/14025, Loss: 0.7698\n",
      "Iteration 4800/14025, Loss: 0.4832\n",
      "Iteration 4900/14025, Loss: 0.3752\n",
      "Iteration 5000/14025, Loss: 0.4156\n",
      "Iteration 5100/14025, Loss: 0.4593\n",
      "Iteration 5200/14025, Loss: 0.8763\n",
      "Iteration 5300/14025, Loss: 0.7619\n",
      "Iteration 5400/14025, Loss: 0.5016\n",
      "Iteration 5500/14025, Loss: 0.6992\n",
      "Iteration 5600/14025, Loss: 0.5633\n",
      "Iteration 5700/14025, Loss: 0.4327\n",
      "Iteration 5800/14025, Loss: 0.4038\n",
      "Iteration 5900/14025, Loss: 0.5187\n",
      "Iteration 6000/14025, Loss: 0.7742\n",
      "Iteration 6100/14025, Loss: 0.3103\n",
      "Iteration 6200/14025, Loss: 0.8998\n",
      "Iteration 6300/14025, Loss: 0.4687\n",
      "Iteration 6400/14025, Loss: 0.4343\n",
      "Iteration 6500/14025, Loss: 0.6757\n",
      "Iteration 6600/14025, Loss: 0.7888\n",
      "Iteration 6700/14025, Loss: 0.3985\n",
      "Iteration 6800/14025, Loss: 0.3861\n",
      "Iteration 6900/14025, Loss: 0.2891\n",
      "Iteration 7000/14025, Loss: 0.5693\n",
      "Iteration 7100/14025, Loss: 0.2160\n",
      "Iteration 7200/14025, Loss: 0.6146\n",
      "Iteration 7300/14025, Loss: 0.3718\n",
      "Iteration 7400/14025, Loss: 0.7041\n",
      "Iteration 7500/14025, Loss: 1.0667\n",
      "Iteration 7600/14025, Loss: 0.4148\n",
      "Iteration 7700/14025, Loss: 0.5849\n",
      "Iteration 7800/14025, Loss: 0.5443\n",
      "Iteration 7900/14025, Loss: 0.2782\n",
      "Iteration 8000/14025, Loss: 0.3491\n",
      "Iteration 8100/14025, Loss: 0.5871\n",
      "Iteration 8200/14025, Loss: 0.7326\n",
      "Iteration 8300/14025, Loss: 0.3341\n",
      "Iteration 8400/14025, Loss: 0.5418\n",
      "Iteration 8500/14025, Loss: 0.3477\n",
      "Iteration 8600/14025, Loss: 0.1681\n",
      "Iteration 8700/14025, Loss: 0.2217\n",
      "Iteration 8800/14025, Loss: 0.6206\n",
      "Iteration 8900/14025, Loss: 0.1616\n",
      "Iteration 9000/14025, Loss: 0.4474\n",
      "Iteration 9100/14025, Loss: 0.5850\n",
      "Iteration 9200/14025, Loss: 0.3040\n",
      "Iteration 9300/14025, Loss: 0.3705\n",
      "Iteration 9400/14025, Loss: 0.2677\n",
      "Iteration 9500/14025, Loss: 0.3709\n",
      "Iteration 9600/14025, Loss: 0.3380\n",
      "Iteration 9700/14025, Loss: 0.3355\n",
      "Iteration 9800/14025, Loss: 0.1607\n",
      "Iteration 9900/14025, Loss: 0.3023\n",
      "Iteration 10000/14025, Loss: 0.8675\n",
      "Iteration 10100/14025, Loss: 0.3529\n",
      "Iteration 10200/14025, Loss: 0.3234\n",
      "Iteration 10300/14025, Loss: 0.3029\n",
      "Iteration 10400/14025, Loss: 0.1159\n",
      "Iteration 10500/14025, Loss: 0.2197\n",
      "Iteration 10600/14025, Loss: 0.4051\n",
      "Iteration 10700/14025, Loss: 0.2282\n",
      "Iteration 10800/14025, Loss: 0.4122\n",
      "Iteration 10900/14025, Loss: 0.3100\n",
      "Iteration 11000/14025, Loss: 0.1376\n",
      "Iteration 11100/14025, Loss: 0.2259\n",
      "Iteration 11200/14025, Loss: 0.4003\n",
      "Iteration 11300/14025, Loss: 0.2907\n",
      "Iteration 11400/14025, Loss: 0.3612\n",
      "Iteration 11500/14025, Loss: 0.4150\n",
      "Iteration 11600/14025, Loss: 0.1577\n",
      "Iteration 11700/14025, Loss: 0.1454\n",
      "Iteration 11800/14025, Loss: 0.3493\n",
      "Iteration 11900/14025, Loss: 0.1797\n",
      "Iteration 12000/14025, Loss: 0.3447\n",
      "Iteration 12100/14025, Loss: 0.2142\n",
      "Iteration 12200/14025, Loss: 0.3114\n",
      "Iteration 12300/14025, Loss: 0.2474\n",
      "Iteration 12400/14025, Loss: 0.2389\n",
      "Iteration 12500/14025, Loss: 0.2857\n",
      "Iteration 12600/14025, Loss: 0.3002\n",
      "Iteration 12700/14025, Loss: 0.2439\n",
      "Iteration 12800/14025, Loss: 0.3146\n",
      "Iteration 12900/14025, Loss: 0.3270\n",
      "Iteration 13000/14025, Loss: 0.4404\n",
      "Iteration 13100/14025, Loss: 0.2356\n",
      "Iteration 13200/14025, Loss: 0.2307\n",
      "Iteration 13300/14025, Loss: 0.1230\n",
      "Iteration 13400/14025, Loss: 0.1652\n",
      "Iteration 13500/14025, Loss: 0.3149\n",
      "Iteration 13600/14025, Loss: 0.2434\n",
      "Iteration 13700/14025, Loss: 0.1252\n",
      "Iteration 13800/14025, Loss: 0.1924\n",
      "Iteration 13900/14025, Loss: 0.1342\n",
      "Iteration 14000/14025, Loss: 0.1863\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 2.2927\n",
      "Iteration 200/6975, Loss: 2.1674\n",
      "Iteration 300/6975, Loss: 1.3739\n",
      "Iteration 400/6975, Loss: 2.0614\n",
      "Iteration 500/6975, Loss: 2.0373\n",
      "Iteration 600/6975, Loss: 1.3716\n",
      "Iteration 700/6975, Loss: 1.9825\n",
      "Iteration 800/6975, Loss: 1.5807\n",
      "Iteration 900/6975, Loss: 1.3229\n",
      "Iteration 1000/6975, Loss: 1.2069\n",
      "Iteration 1100/6975, Loss: 1.1343\n",
      "Iteration 1200/6975, Loss: 0.9132\n",
      "Iteration 1300/6975, Loss: 1.1609\n",
      "Iteration 1400/6975, Loss: 0.8709\n",
      "Iteration 1500/6975, Loss: 0.9079\n",
      "Iteration 1600/6975, Loss: 0.6134\n",
      "Iteration 1700/6975, Loss: 0.9296\n",
      "Iteration 1800/6975, Loss: 0.6272\n",
      "Iteration 1900/6975, Loss: 0.8505\n",
      "Iteration 2000/6975, Loss: 0.6597\n",
      "Iteration 2100/6975, Loss: 0.8003\n",
      "Iteration 2200/6975, Loss: 0.4714\n",
      "Iteration 2300/6975, Loss: 0.9621\n",
      "Iteration 2400/6975, Loss: 0.9164\n",
      "Iteration 2500/6975, Loss: 0.5730\n",
      "Iteration 2600/6975, Loss: 0.6427\n",
      "Iteration 2700/6975, Loss: 0.7407\n",
      "Iteration 2800/6975, Loss: 0.4557\n",
      "Iteration 2900/6975, Loss: 0.7542\n",
      "Iteration 3000/6975, Loss: 0.7515\n",
      "Iteration 3100/6975, Loss: 0.7279\n",
      "Iteration 3200/6975, Loss: 0.4561\n",
      "Iteration 3300/6975, Loss: 0.5863\n",
      "Iteration 3400/6975, Loss: 0.4022\n",
      "Iteration 3500/6975, Loss: 0.3401\n",
      "Iteration 3600/6975, Loss: 0.4090\n",
      "Iteration 3700/6975, Loss: 0.5677\n",
      "Iteration 3800/6975, Loss: 0.5912\n",
      "Iteration 3900/6975, Loss: 0.9655\n",
      "Iteration 4000/6975, Loss: 0.5509\n",
      "Iteration 4100/6975, Loss: 0.5514\n",
      "Iteration 4200/6975, Loss: 0.4206\n",
      "Iteration 4300/6975, Loss: 0.1849\n",
      "Iteration 4400/6975, Loss: 0.6049\n",
      "Iteration 4500/6975, Loss: 0.4243\n",
      "Iteration 4600/6975, Loss: 0.2976\n",
      "Iteration 4700/6975, Loss: 0.3320\n",
      "Iteration 4800/6975, Loss: 0.3377\n",
      "Iteration 4900/6975, Loss: 0.1986\n",
      "Iteration 5000/6975, Loss: 0.6678\n",
      "Iteration 5100/6975, Loss: 0.2773\n",
      "Iteration 5200/6975, Loss: 0.3122\n",
      "Iteration 5300/6975, Loss: 0.2992\n",
      "Iteration 5400/6975, Loss: 0.3607\n",
      "Iteration 5500/6975, Loss: 0.3146\n",
      "Iteration 5600/6975, Loss: 0.3714\n",
      "Iteration 5700/6975, Loss: 0.3665\n",
      "Iteration 5800/6975, Loss: 0.2058\n",
      "Iteration 5900/6975, Loss: 0.4564\n",
      "Iteration 6000/6975, Loss: 0.3081\n",
      "Iteration 6100/6975, Loss: 0.2391\n",
      "Iteration 6200/6975, Loss: 0.2591\n",
      "Iteration 6300/6975, Loss: 0.2683\n",
      "Iteration 6400/6975, Loss: 0.3376\n",
      "Iteration 6500/6975, Loss: 0.3213\n",
      "Iteration 6600/6975, Loss: 0.1579\n",
      "Iteration 6700/6975, Loss: 0.1688\n",
      "Iteration 6800/6975, Loss: 0.1734\n",
      "Iteration 6900/6975, Loss: 0.1975\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 2.9151\n",
      "Iteration 200/3450, Loss: 2.8504\n",
      "Iteration 300/3450, Loss: 1.3192\n",
      "Iteration 400/3450, Loss: 1.4175\n",
      "Iteration 500/3450, Loss: 1.5738\n",
      "Iteration 600/3450, Loss: 0.9265\n",
      "Iteration 700/3450, Loss: 3.3193\n",
      "Iteration 800/3450, Loss: 1.0164\n",
      "Iteration 900/3450, Loss: 0.5489\n",
      "Iteration 1000/3450, Loss: 1.1641\n",
      "Iteration 1100/3450, Loss: 0.5372\n",
      "Iteration 1200/3450, Loss: 0.7160\n",
      "Iteration 1300/3450, Loss: 0.7706\n",
      "Iteration 1400/3450, Loss: 0.5588\n",
      "Iteration 1500/3450, Loss: 0.6896\n",
      "Iteration 1600/3450, Loss: 0.9520\n",
      "Iteration 1700/3450, Loss: 0.4976\n",
      "Iteration 1800/3450, Loss: 0.7914\n",
      "Iteration 1900/3450, Loss: 0.5423\n",
      "Iteration 2000/3450, Loss: 0.5407\n",
      "Iteration 2100/3450, Loss: 0.4305\n",
      "Iteration 2200/3450, Loss: 1.0833\n",
      "Iteration 2300/3450, Loss: 0.4249\n",
      "Iteration 2400/3450, Loss: 0.3429\n",
      "Iteration 2500/3450, Loss: 0.5660\n",
      "Iteration 2600/3450, Loss: 0.3579\n",
      "Iteration 2700/3450, Loss: 0.4048\n",
      "Iteration 2800/3450, Loss: 0.3895\n",
      "Iteration 2900/3450, Loss: 0.2678\n",
      "Iteration 3000/3450, Loss: 0.3718\n",
      "Iteration 3100/3450, Loss: 0.2834\n",
      "Iteration 3200/3450, Loss: 0.3021\n",
      "Iteration 3300/3450, Loss: 0.1809\n",
      "Iteration 3400/3450, Loss: 0.1840\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 6.4232\n",
      "Iteration 200/1725, Loss: 4.8545\n",
      "Iteration 300/1725, Loss: 2.6258\n",
      "Iteration 400/1725, Loss: 3.2527\n",
      "Iteration 500/1725, Loss: 1.6653\n",
      "Iteration 600/1725, Loss: 2.5562\n",
      "Iteration 700/1725, Loss: 1.1049\n",
      "Iteration 800/1725, Loss: 2.4514\n",
      "Iteration 900/1725, Loss: 1.9504\n",
      "Iteration 1000/1725, Loss: 1.0689\n",
      "Iteration 1100/1725, Loss: 1.0113\n",
      "Iteration 1200/1725, Loss: 0.6387\n",
      "Iteration 1300/1725, Loss: 2.7985\n",
      "Iteration 1400/1725, Loss: 0.9846\n",
      "Iteration 1500/1725, Loss: 0.8170\n",
      "Iteration 1600/1725, Loss: 0.5055\n",
      "Iteration 1700/1725, Loss: 0.5144\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 10.9094\n",
      "Iteration 200/825, Loss: 5.3395\n",
      "Iteration 300/825, Loss: 6.3730\n",
      "Iteration 400/825, Loss: 7.2615\n",
      "Iteration 500/825, Loss: 3.6056\n",
      "Iteration 600/825, Loss: 4.4617\n",
      "Iteration 700/825, Loss: 2.0880\n",
      "Iteration 800/825, Loss: 2.8499\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 9.9932\n",
      "Iteration 200/375, Loss: 11.5692\n",
      "Iteration 300/375, Loss: 7.9046\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 16.0098\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 1.8807\n",
      "Iteration 200/28125, Loss: 2.9598\n",
      "Iteration 300/28125, Loss: 3.1271\n",
      "Iteration 400/28125, Loss: 3.1876\n",
      "Iteration 500/28125, Loss: 1.9703\n",
      "Iteration 600/28125, Loss: 1.3514\n",
      "Iteration 700/28125, Loss: 1.0084\n",
      "Iteration 800/28125, Loss: 1.8519\n",
      "Iteration 900/28125, Loss: 1.8441\n",
      "Iteration 1000/28125, Loss: 1.8826\n",
      "Iteration 1100/28125, Loss: 1.2433\n",
      "Iteration 1200/28125, Loss: 0.9328\n",
      "Iteration 1300/28125, Loss: 3.7766\n",
      "Iteration 1400/28125, Loss: 1.7554\n",
      "Iteration 1500/28125, Loss: 0.9308\n",
      "Iteration 1600/28125, Loss: 0.9403\n",
      "Iteration 1700/28125, Loss: 2.3180\n",
      "Iteration 1800/28125, Loss: 1.3237\n",
      "Iteration 1900/28125, Loss: 1.3191\n",
      "Iteration 2000/28125, Loss: 1.9721\n",
      "Iteration 2100/28125, Loss: 1.0572\n",
      "Iteration 2200/28125, Loss: 1.3420\n",
      "Iteration 2300/28125, Loss: 0.9528\n",
      "Iteration 2400/28125, Loss: 1.2897\n",
      "Iteration 2500/28125, Loss: 1.1857\n",
      "Iteration 2600/28125, Loss: 0.9674\n",
      "Iteration 2700/28125, Loss: 0.6272\n",
      "Iteration 2800/28125, Loss: 2.0055\n",
      "Iteration 2900/28125, Loss: 1.5685\n",
      "Iteration 3000/28125, Loss: 1.3387\n",
      "Iteration 3100/28125, Loss: 2.5569\n",
      "Iteration 3200/28125, Loss: 0.8295\n",
      "Iteration 3300/28125, Loss: 0.2660\n",
      "Iteration 3400/28125, Loss: 0.7173\n",
      "Iteration 3500/28125, Loss: 0.7254\n",
      "Iteration 3600/28125, Loss: 1.4173\n",
      "Iteration 3700/28125, Loss: 1.1607\n",
      "Iteration 3800/28125, Loss: 0.4882\n",
      "Iteration 3900/28125, Loss: 0.5642\n",
      "Iteration 4000/28125, Loss: 1.5732\n",
      "Iteration 4100/28125, Loss: 0.2460\n",
      "Iteration 4200/28125, Loss: 1.6236\n",
      "Iteration 4300/28125, Loss: 0.5263\n",
      "Iteration 4400/28125, Loss: 1.0470\n",
      "Iteration 4500/28125, Loss: 1.5193\n",
      "Iteration 4600/28125, Loss: 0.9629\n",
      "Iteration 4700/28125, Loss: 1.2352\n",
      "Iteration 4800/28125, Loss: 1.6568\n",
      "Iteration 4900/28125, Loss: 1.1113\n",
      "Iteration 5000/28125, Loss: 1.3448\n",
      "Iteration 5100/28125, Loss: 0.9855\n",
      "Iteration 5200/28125, Loss: 1.0731\n",
      "Iteration 5300/28125, Loss: 1.0799\n",
      "Iteration 5400/28125, Loss: 0.6737\n",
      "Iteration 5500/28125, Loss: 1.7955\n",
      "Iteration 5600/28125, Loss: 0.5187\n",
      "Iteration 5700/28125, Loss: 1.9431\n",
      "Iteration 5800/28125, Loss: 0.9304\n",
      "Iteration 5900/28125, Loss: 0.1492\n",
      "Iteration 6000/28125, Loss: 0.7819\n",
      "Iteration 6100/28125, Loss: 0.3879\n",
      "Iteration 6200/28125, Loss: 1.0386\n",
      "Iteration 6300/28125, Loss: 1.0086\n",
      "Iteration 6400/28125, Loss: 0.2711\n",
      "Iteration 6500/28125, Loss: 0.4335\n",
      "Iteration 6600/28125, Loss: 0.5727\n",
      "Iteration 6700/28125, Loss: 1.0257\n",
      "Iteration 6800/28125, Loss: 1.9938\n",
      "Iteration 6900/28125, Loss: 0.5838\n",
      "Iteration 7000/28125, Loss: 0.5284\n",
      "Iteration 7100/28125, Loss: 0.2698\n",
      "Iteration 7200/28125, Loss: 0.2805\n",
      "Iteration 7300/28125, Loss: 1.2624\n",
      "Iteration 7400/28125, Loss: 0.6920\n",
      "Iteration 7500/28125, Loss: 0.5470\n",
      "Iteration 7600/28125, Loss: 0.3790\n",
      "Iteration 7700/28125, Loss: 1.7404\n",
      "Iteration 7800/28125, Loss: 0.9246\n",
      "Iteration 7900/28125, Loss: 1.1095\n",
      "Iteration 8000/28125, Loss: 0.7698\n",
      "Iteration 8100/28125, Loss: 0.3401\n",
      "Iteration 8200/28125, Loss: 1.0318\n",
      "Iteration 8300/28125, Loss: 0.6572\n",
      "Iteration 8400/28125, Loss: 0.7976\n",
      "Iteration 8500/28125, Loss: 0.4173\n",
      "Iteration 8600/28125, Loss: 0.8588\n",
      "Iteration 8700/28125, Loss: 0.1987\n",
      "Iteration 8800/28125, Loss: 0.5820\n",
      "Iteration 8900/28125, Loss: 1.8550\n",
      "Iteration 9000/28125, Loss: 0.9085\n",
      "Iteration 9100/28125, Loss: 0.2647\n",
      "Iteration 9200/28125, Loss: 0.8296\n",
      "Iteration 9300/28125, Loss: 0.6902\n",
      "Iteration 9400/28125, Loss: 0.3198\n",
      "Iteration 9500/28125, Loss: 0.1808\n",
      "Iteration 9600/28125, Loss: 0.6816\n",
      "Iteration 9700/28125, Loss: 0.8816\n",
      "Iteration 9800/28125, Loss: 0.7444\n",
      "Iteration 9900/28125, Loss: 0.8999\n",
      "Iteration 10000/28125, Loss: 0.2215\n",
      "Iteration 10100/28125, Loss: 1.0204\n",
      "Iteration 10200/28125, Loss: 0.6633\n",
      "Iteration 10300/28125, Loss: 0.4895\n",
      "Iteration 10400/28125, Loss: 0.8869\n",
      "Iteration 10500/28125, Loss: 0.6622\n",
      "Iteration 10600/28125, Loss: 0.5486\n",
      "Iteration 10700/28125, Loss: 0.8360\n",
      "Iteration 10800/28125, Loss: 0.5931\n",
      "Iteration 10900/28125, Loss: 0.3161\n",
      "Iteration 11000/28125, Loss: 0.4531\n",
      "Iteration 11100/28125, Loss: 0.5247\n",
      "Iteration 11200/28125, Loss: 0.4639\n",
      "Iteration 11300/28125, Loss: 0.2936\n",
      "Iteration 11400/28125, Loss: 0.4785\n",
      "Iteration 11500/28125, Loss: 0.4811\n",
      "Iteration 11600/28125, Loss: 0.3713\n",
      "Iteration 11700/28125, Loss: 0.6906\n",
      "Iteration 11800/28125, Loss: 0.3097\n",
      "Iteration 11900/28125, Loss: 0.2733\n",
      "Iteration 12000/28125, Loss: 0.5595\n",
      "Iteration 12100/28125, Loss: 1.2216\n",
      "Iteration 12200/28125, Loss: 0.3454\n",
      "Iteration 12300/28125, Loss: 0.3757\n",
      "Iteration 12400/28125, Loss: 1.3988\n",
      "Iteration 12500/28125, Loss: 0.5182\n",
      "Iteration 12600/28125, Loss: 0.6117\n",
      "Iteration 12700/28125, Loss: 0.8656\n",
      "Iteration 12800/28125, Loss: 0.4054\n",
      "Iteration 12900/28125, Loss: 0.2010\n",
      "Iteration 13000/28125, Loss: 0.6750\n",
      "Iteration 13100/28125, Loss: 0.1871\n",
      "Iteration 13200/28125, Loss: 1.3816\n",
      "Iteration 13300/28125, Loss: 0.4324\n",
      "Iteration 13400/28125, Loss: 0.6499\n",
      "Iteration 13500/28125, Loss: 0.4167\n",
      "Iteration 13600/28125, Loss: 0.2896\n",
      "Iteration 13700/28125, Loss: 0.3670\n",
      "Iteration 13800/28125, Loss: 0.1853\n",
      "Iteration 13900/28125, Loss: 0.2449\n",
      "Iteration 14000/28125, Loss: 0.4467\n",
      "Iteration 14100/28125, Loss: 0.2628\n",
      "Iteration 14200/28125, Loss: 0.2051\n",
      "Iteration 14300/28125, Loss: 0.4641\n",
      "Iteration 14400/28125, Loss: 1.0075\n",
      "Iteration 14500/28125, Loss: 0.6514\n",
      "Iteration 14600/28125, Loss: 0.4981\n",
      "Iteration 14700/28125, Loss: 0.3460\n",
      "Iteration 14800/28125, Loss: 0.5509\n",
      "Iteration 14900/28125, Loss: 0.5404\n",
      "Iteration 15000/28125, Loss: 0.8173\n",
      "Iteration 15100/28125, Loss: 0.2688\n",
      "Iteration 15200/28125, Loss: 0.4190\n",
      "Iteration 15300/28125, Loss: 1.0011\n",
      "Iteration 15400/28125, Loss: 0.7171\n",
      "Iteration 15500/28125, Loss: 0.3540\n",
      "Iteration 15600/28125, Loss: 0.4964\n",
      "Iteration 15700/28125, Loss: 0.2356\n",
      "Iteration 15800/28125, Loss: 0.4186\n",
      "Iteration 15900/28125, Loss: 0.5356\n",
      "Iteration 16000/28125, Loss: 0.2868\n",
      "Iteration 16100/28125, Loss: 0.6010\n",
      "Iteration 16200/28125, Loss: 0.9071\n",
      "Iteration 16300/28125, Loss: 0.1676\n",
      "Iteration 16400/28125, Loss: 0.8228\n",
      "Iteration 16500/28125, Loss: 0.5137\n",
      "Iteration 16600/28125, Loss: 0.3579\n",
      "Iteration 16700/28125, Loss: 0.4278\n",
      "Iteration 16800/28125, Loss: 0.7402\n",
      "Iteration 16900/28125, Loss: 0.3581\n",
      "Iteration 17000/28125, Loss: 0.4577\n",
      "Iteration 17100/28125, Loss: 0.1902\n",
      "Iteration 17200/28125, Loss: 0.3047\n",
      "Iteration 17300/28125, Loss: 0.3946\n",
      "Iteration 17400/28125, Loss: 0.2838\n",
      "Iteration 17500/28125, Loss: 0.4879\n",
      "Iteration 17600/28125, Loss: 0.3533\n",
      "Iteration 17700/28125, Loss: 0.5640\n",
      "Iteration 17800/28125, Loss: 0.3314\n",
      "Iteration 17900/28125, Loss: 0.4010\n",
      "Iteration 18000/28125, Loss: 0.4336\n",
      "Iteration 18100/28125, Loss: 0.4962\n",
      "Iteration 18200/28125, Loss: 0.5632\n",
      "Iteration 18300/28125, Loss: 0.7336\n",
      "Iteration 18400/28125, Loss: 0.2217\n",
      "Iteration 18500/28125, Loss: 0.7700\n",
      "Iteration 18600/28125, Loss: 0.4224\n",
      "Iteration 18700/28125, Loss: 0.1829\n",
      "Iteration 18800/28125, Loss: 0.4121\n",
      "Iteration 18900/28125, Loss: 0.3989\n",
      "Iteration 19000/28125, Loss: 0.4530\n",
      "Iteration 19100/28125, Loss: 0.2949\n",
      "Iteration 19200/28125, Loss: 0.3433\n",
      "Iteration 19300/28125, Loss: 0.3659\n",
      "Iteration 19400/28125, Loss: 0.3385\n",
      "Iteration 19500/28125, Loss: 0.5693\n",
      "Iteration 19600/28125, Loss: 0.2983\n",
      "Iteration 19700/28125, Loss: 0.3136\n",
      "Iteration 19800/28125, Loss: 0.3614\n",
      "Iteration 19900/28125, Loss: 0.4686\n",
      "Iteration 20000/28125, Loss: 0.9408\n",
      "Iteration 20100/28125, Loss: 0.4991\n",
      "Iteration 20200/28125, Loss: 0.5203\n",
      "Iteration 20300/28125, Loss: 0.2135\n",
      "Iteration 20400/28125, Loss: 0.4869\n",
      "Iteration 20500/28125, Loss: 0.2458\n",
      "Iteration 20600/28125, Loss: 0.3296\n",
      "Iteration 20700/28125, Loss: 0.4966\n",
      "Iteration 20800/28125, Loss: 0.2029\n",
      "Iteration 20900/28125, Loss: 0.2954\n",
      "Iteration 21000/28125, Loss: 0.3219\n",
      "Iteration 21100/28125, Loss: 0.3039\n",
      "Iteration 21200/28125, Loss: 0.6376\n",
      "Iteration 21300/28125, Loss: 0.2129\n",
      "Iteration 21400/28125, Loss: 0.2688\n",
      "Iteration 21500/28125, Loss: 0.5090\n",
      "Iteration 21600/28125, Loss: 0.4788\n",
      "Iteration 21700/28125, Loss: 0.3916\n",
      "Iteration 21800/28125, Loss: 0.4769\n",
      "Iteration 21900/28125, Loss: 0.3391\n",
      "Iteration 22000/28125, Loss: 0.1455\n",
      "Iteration 22100/28125, Loss: 0.3918\n",
      "Iteration 22200/28125, Loss: 0.2047\n",
      "Iteration 22300/28125, Loss: 0.2980\n",
      "Iteration 22400/28125, Loss: 0.3830\n",
      "Iteration 22500/28125, Loss: 0.5399\n",
      "Iteration 22600/28125, Loss: 0.2987\n",
      "Iteration 22700/28125, Loss: 0.4775\n",
      "Iteration 22800/28125, Loss: 0.3207\n",
      "Iteration 22900/28125, Loss: 0.6724\n",
      "Iteration 23000/28125, Loss: 0.4081\n",
      "Iteration 23100/28125, Loss: 0.4153\n",
      "Iteration 23200/28125, Loss: 0.2012\n",
      "Iteration 23300/28125, Loss: 0.3061\n",
      "Iteration 23400/28125, Loss: 0.1726\n",
      "Iteration 23500/28125, Loss: 0.1866\n",
      "Iteration 23600/28125, Loss: 0.3221\n",
      "Iteration 23700/28125, Loss: 0.1489\n",
      "Iteration 23800/28125, Loss: 0.3216\n",
      "Iteration 23900/28125, Loss: 0.3453\n",
      "Iteration 24000/28125, Loss: 0.3063\n",
      "Iteration 24100/28125, Loss: 0.1757\n",
      "Iteration 24200/28125, Loss: 0.2957\n",
      "Iteration 24300/28125, Loss: 0.7487\n",
      "Iteration 24400/28125, Loss: 0.2680\n",
      "Iteration 24500/28125, Loss: 0.4876\n",
      "Iteration 24600/28125, Loss: 0.3127\n",
      "Iteration 24700/28125, Loss: 0.4534\n",
      "Iteration 24800/28125, Loss: 0.3324\n",
      "Iteration 24900/28125, Loss: 0.4093\n",
      "Iteration 25000/28125, Loss: 0.4180\n",
      "Iteration 25100/28125, Loss: 0.3708\n",
      "Iteration 25200/28125, Loss: 0.3180\n",
      "Iteration 25300/28125, Loss: 0.1917\n",
      "Iteration 25400/28125, Loss: 0.4355\n",
      "Iteration 25500/28125, Loss: 0.1983\n",
      "Iteration 25600/28125, Loss: 0.4528\n",
      "Iteration 25700/28125, Loss: 0.3311\n",
      "Iteration 25800/28125, Loss: 0.2783\n",
      "Iteration 25900/28125, Loss: 0.4928\n",
      "Iteration 26000/28125, Loss: 0.6884\n",
      "Iteration 26100/28125, Loss: 0.4259\n",
      "Iteration 26200/28125, Loss: 0.3284\n",
      "Iteration 26300/28125, Loss: 0.2665\n",
      "Iteration 26400/28125, Loss: 0.4749\n",
      "Iteration 26500/28125, Loss: 0.2297\n",
      "Iteration 26600/28125, Loss: 0.2703\n",
      "Iteration 26700/28125, Loss: 0.4946\n",
      "Iteration 26800/28125, Loss: 0.2603\n",
      "Iteration 26900/28125, Loss: 0.2939\n",
      "Iteration 27000/28125, Loss: 0.5364\n",
      "Iteration 27100/28125, Loss: 0.2766\n",
      "Iteration 27200/28125, Loss: 0.3289\n",
      "Iteration 27300/28125, Loss: 0.2392\n",
      "Iteration 27400/28125, Loss: 0.2471\n",
      "Iteration 27500/28125, Loss: 0.3252\n",
      "Iteration 27600/28125, Loss: 0.2197\n",
      "Iteration 27700/28125, Loss: 0.2492\n",
      "Iteration 27800/28125, Loss: 0.2567\n",
      "Iteration 27900/28125, Loss: 0.2752\n",
      "Iteration 28000/28125, Loss: 0.1847\n",
      "Iteration 28100/28125, Loss: 0.4592\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 2.1717\n",
      "Iteration 200/14025, Loss: 2.6495\n",
      "Iteration 300/14025, Loss: 1.4799\n",
      "Iteration 400/14025, Loss: 1.9568\n",
      "Iteration 500/14025, Loss: 1.8461\n",
      "Iteration 600/14025, Loss: 1.3663\n",
      "Iteration 700/14025, Loss: 1.3489\n",
      "Iteration 800/14025, Loss: 1.2235\n",
      "Iteration 900/14025, Loss: 1.0546\n",
      "Iteration 1000/14025, Loss: 2.2785\n",
      "Iteration 1100/14025, Loss: 1.2023\n",
      "Iteration 1200/14025, Loss: 1.2298\n",
      "Iteration 1300/14025, Loss: 0.6932\n",
      "Iteration 1400/14025, Loss: 1.5735\n",
      "Iteration 1500/14025, Loss: 1.9067\n",
      "Iteration 1600/14025, Loss: 1.3655\n",
      "Iteration 1700/14025, Loss: 0.7985\n",
      "Iteration 1800/14025, Loss: 1.2467\n",
      "Iteration 1900/14025, Loss: 0.8424\n",
      "Iteration 2000/14025, Loss: 1.0951\n",
      "Iteration 2100/14025, Loss: 1.2625\n",
      "Iteration 2200/14025, Loss: 1.3354\n",
      "Iteration 2300/14025, Loss: 1.1584\n",
      "Iteration 2400/14025, Loss: 1.4331\n",
      "Iteration 2500/14025, Loss: 1.1180\n",
      "Iteration 2600/14025, Loss: 1.0775\n",
      "Iteration 2700/14025, Loss: 1.0613\n",
      "Iteration 2800/14025, Loss: 0.8080\n",
      "Iteration 2900/14025, Loss: 0.6741\n",
      "Iteration 3000/14025, Loss: 0.6162\n",
      "Iteration 3100/14025, Loss: 1.2671\n",
      "Iteration 3200/14025, Loss: 0.6971\n",
      "Iteration 3300/14025, Loss: 0.6829\n",
      "Iteration 3400/14025, Loss: 1.1446\n",
      "Iteration 3500/14025, Loss: 0.9175\n",
      "Iteration 3600/14025, Loss: 0.4740\n",
      "Iteration 3700/14025, Loss: 0.7354\n",
      "Iteration 3800/14025, Loss: 0.5095\n",
      "Iteration 3900/14025, Loss: 0.7685\n",
      "Iteration 4000/14025, Loss: 0.9697\n",
      "Iteration 4100/14025, Loss: 0.6790\n",
      "Iteration 4200/14025, Loss: 0.9590\n",
      "Iteration 4300/14025, Loss: 0.5266\n",
      "Iteration 4400/14025, Loss: 0.5205\n",
      "Iteration 4500/14025, Loss: 1.1693\n",
      "Iteration 4600/14025, Loss: 1.2187\n",
      "Iteration 4700/14025, Loss: 0.8113\n",
      "Iteration 4800/14025, Loss: 0.5247\n",
      "Iteration 4900/14025, Loss: 0.4177\n",
      "Iteration 5000/14025, Loss: 0.4590\n",
      "Iteration 5100/14025, Loss: 0.5039\n",
      "Iteration 5200/14025, Loss: 0.9219\n",
      "Iteration 5300/14025, Loss: 0.8103\n",
      "Iteration 5400/14025, Loss: 0.5491\n",
      "Iteration 5500/14025, Loss: 0.7465\n",
      "Iteration 5600/14025, Loss: 0.6137\n",
      "Iteration 5700/14025, Loss: 0.4829\n",
      "Iteration 5800/14025, Loss: 0.4541\n",
      "Iteration 5900/14025, Loss: 0.5713\n",
      "Iteration 6000/14025, Loss: 0.8265\n",
      "Iteration 6100/14025, Loss: 0.3630\n",
      "Iteration 6200/14025, Loss: 0.9554\n",
      "Iteration 6300/14025, Loss: 0.5234\n",
      "Iteration 6400/14025, Loss: 0.4907\n",
      "Iteration 6500/14025, Loss: 0.7330\n",
      "Iteration 6600/14025, Loss: 0.8481\n",
      "Iteration 6700/14025, Loss: 0.4563\n",
      "Iteration 6800/14025, Loss: 0.4457\n",
      "Iteration 6900/14025, Loss: 0.3484\n",
      "Iteration 7000/14025, Loss: 0.6302\n",
      "Iteration 7100/14025, Loss: 0.2764\n",
      "Iteration 7200/14025, Loss: 0.6779\n",
      "Iteration 7300/14025, Loss: 0.4344\n",
      "Iteration 7400/14025, Loss: 0.7689\n",
      "Iteration 7500/14025, Loss: 1.1316\n",
      "Iteration 7600/14025, Loss: 0.4800\n",
      "Iteration 7700/14025, Loss: 0.6516\n",
      "Iteration 7800/14025, Loss: 0.6126\n",
      "Iteration 7900/14025, Loss: 0.3453\n",
      "Iteration 8000/14025, Loss: 0.4177\n",
      "Iteration 8100/14025, Loss: 0.6574\n",
      "Iteration 8200/14025, Loss: 0.8025\n",
      "Iteration 8300/14025, Loss: 0.4046\n",
      "Iteration 8400/14025, Loss: 0.6137\n",
      "Iteration 8500/14025, Loss: 0.4193\n",
      "Iteration 8600/14025, Loss: 0.2398\n",
      "Iteration 8700/14025, Loss: 0.2949\n",
      "Iteration 8800/14025, Loss: 0.6953\n",
      "Iteration 8900/14025, Loss: 0.2361\n",
      "Iteration 9000/14025, Loss: 0.5242\n",
      "Iteration 9100/14025, Loss: 0.6607\n",
      "Iteration 9200/14025, Loss: 0.3816\n",
      "Iteration 9300/14025, Loss: 0.4487\n",
      "Iteration 9400/14025, Loss: 0.3453\n",
      "Iteration 9500/14025, Loss: 0.4510\n",
      "Iteration 9600/14025, Loss: 0.4177\n",
      "Iteration 9700/14025, Loss: 0.4162\n",
      "Iteration 9800/14025, Loss: 0.2408\n",
      "Iteration 9900/14025, Loss: 0.3844\n",
      "Iteration 10000/14025, Loss: 0.9508\n",
      "Iteration 10100/14025, Loss: 0.4359\n",
      "Iteration 10200/14025, Loss: 0.4084\n",
      "Iteration 10300/14025, Loss: 0.3876\n",
      "Iteration 10400/14025, Loss: 0.2002\n",
      "Iteration 10500/14025, Loss: 0.3051\n",
      "Iteration 10600/14025, Loss: 0.4921\n",
      "Iteration 10700/14025, Loss: 0.3146\n",
      "Iteration 10800/14025, Loss: 0.5018\n",
      "Iteration 10900/14025, Loss: 0.3999\n",
      "Iteration 11000/14025, Loss: 0.2263\n",
      "Iteration 11100/14025, Loss: 0.3151\n",
      "Iteration 11200/14025, Loss: 0.4935\n",
      "Iteration 11300/14025, Loss: 0.3839\n",
      "Iteration 11400/14025, Loss: 0.4535\n",
      "Iteration 11500/14025, Loss: 0.5085\n",
      "Iteration 11600/14025, Loss: 0.2505\n",
      "Iteration 11700/14025, Loss: 0.2377\n",
      "Iteration 11800/14025, Loss: 0.4457\n",
      "Iteration 11900/14025, Loss: 0.2746\n",
      "Iteration 12000/14025, Loss: 0.4431\n",
      "Iteration 12100/14025, Loss: 0.3106\n",
      "Iteration 12200/14025, Loss: 0.4115\n",
      "Iteration 12300/14025, Loss: 0.3464\n",
      "Iteration 12400/14025, Loss: 0.3376\n",
      "Iteration 12500/14025, Loss: 0.3852\n",
      "Iteration 12600/14025, Loss: 0.4017\n",
      "Iteration 12700/14025, Loss: 0.3458\n",
      "Iteration 12800/14025, Loss: 0.4179\n",
      "Iteration 12900/14025, Loss: 0.4299\n",
      "Iteration 13000/14025, Loss: 0.5435\n",
      "Iteration 13100/14025, Loss: 0.3384\n",
      "Iteration 13200/14025, Loss: 0.3336\n",
      "Iteration 13300/14025, Loss: 0.2259\n",
      "Iteration 13400/14025, Loss: 0.2688\n",
      "Iteration 13500/14025, Loss: 0.4208\n",
      "Iteration 13600/14025, Loss: 0.3495\n",
      "Iteration 13700/14025, Loss: 0.2298\n",
      "Iteration 13800/14025, Loss: 0.2983\n",
      "Iteration 13900/14025, Loss: 0.2403\n",
      "Iteration 14000/14025, Loss: 0.2936\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 2.2956\n",
      "Iteration 200/6975, Loss: 2.1718\n",
      "Iteration 300/6975, Loss: 1.3797\n",
      "Iteration 400/6975, Loss: 2.0689\n",
      "Iteration 500/6975, Loss: 2.0464\n",
      "Iteration 600/6975, Loss: 1.3824\n",
      "Iteration 700/6975, Loss: 1.9952\n",
      "Iteration 800/6975, Loss: 1.5952\n",
      "Iteration 900/6975, Loss: 1.3392\n",
      "Iteration 1000/6975, Loss: 1.2251\n",
      "Iteration 1100/6975, Loss: 1.1542\n",
      "Iteration 1200/6975, Loss: 0.9349\n",
      "Iteration 1300/6975, Loss: 1.1846\n",
      "Iteration 1400/6975, Loss: 0.8968\n",
      "Iteration 1500/6975, Loss: 0.9352\n",
      "Iteration 1600/6975, Loss: 0.6428\n",
      "Iteration 1700/6975, Loss: 0.9607\n",
      "Iteration 1800/6975, Loss: 0.6603\n",
      "Iteration 1900/6975, Loss: 0.8852\n",
      "Iteration 2000/6975, Loss: 0.6964\n",
      "Iteration 2100/6975, Loss: 0.8384\n",
      "Iteration 2200/6975, Loss: 0.5115\n",
      "Iteration 2300/6975, Loss: 1.0042\n",
      "Iteration 2400/6975, Loss: 0.9601\n",
      "Iteration 2500/6975, Loss: 0.6188\n",
      "Iteration 2600/6975, Loss: 0.6901\n",
      "Iteration 2700/6975, Loss: 0.7905\n",
      "Iteration 2800/6975, Loss: 0.5070\n",
      "Iteration 2900/6975, Loss: 0.8073\n",
      "Iteration 3000/6975, Loss: 0.8066\n",
      "Iteration 3100/6975, Loss: 0.7848\n",
      "Iteration 3200/6975, Loss: 0.5143\n",
      "Iteration 3300/6975, Loss: 0.6464\n",
      "Iteration 3400/6975, Loss: 0.4637\n",
      "Iteration 3500/6975, Loss: 0.4029\n",
      "Iteration 3600/6975, Loss: 0.4739\n",
      "Iteration 3700/6975, Loss: 0.6345\n",
      "Iteration 3800/6975, Loss: 0.6597\n",
      "Iteration 3900/6975, Loss: 1.0351\n",
      "Iteration 4000/6975, Loss: 0.6215\n",
      "Iteration 4100/6975, Loss: 0.6239\n",
      "Iteration 4200/6975, Loss: 0.4946\n",
      "Iteration 4300/6975, Loss: 0.2602\n",
      "Iteration 4400/6975, Loss: 0.6837\n",
      "Iteration 4500/6975, Loss: 0.5026\n",
      "Iteration 4600/6975, Loss: 0.3778\n",
      "Iteration 4700/6975, Loss: 0.4137\n",
      "Iteration 4800/6975, Loss: 0.4212\n",
      "Iteration 4900/6975, Loss: 0.2828\n",
      "Iteration 5000/6975, Loss: 0.7537\n",
      "Iteration 5100/6975, Loss: 0.3650\n",
      "Iteration 5200/6975, Loss: 0.4003\n",
      "Iteration 5300/6975, Loss: 0.3894\n",
      "Iteration 5400/6975, Loss: 0.4521\n",
      "Iteration 5500/6975, Loss: 0.4070\n",
      "Iteration 5600/6975, Loss: 0.4673\n",
      "Iteration 5700/6975, Loss: 0.4628\n",
      "Iteration 5800/6975, Loss: 0.3025\n",
      "Iteration 5900/6975, Loss: 0.5563\n",
      "Iteration 6000/6975, Loss: 0.4084\n",
      "Iteration 6100/6975, Loss: 0.3405\n",
      "Iteration 6200/6975, Loss: 0.3616\n",
      "Iteration 6300/6975, Loss: 0.3723\n",
      "Iteration 6400/6975, Loss: 0.4432\n",
      "Iteration 6500/6975, Loss: 0.4273\n",
      "Iteration 6600/6975, Loss: 0.2643\n",
      "Iteration 6700/6975, Loss: 0.2768\n",
      "Iteration 6800/6975, Loss: 0.2826\n",
      "Iteration 6900/6975, Loss: 0.3076\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 2.9203\n",
      "Iteration 200/3450, Loss: 2.8587\n",
      "Iteration 300/3450, Loss: 1.3314\n",
      "Iteration 400/3450, Loss: 1.4349\n",
      "Iteration 500/3450, Loss: 1.5953\n",
      "Iteration 600/3450, Loss: 0.9521\n",
      "Iteration 700/3450, Loss: 3.3514\n",
      "Iteration 800/3450, Loss: 1.0511\n",
      "Iteration 900/3450, Loss: 0.5882\n",
      "Iteration 1000/3450, Loss: 1.2072\n",
      "Iteration 1100/3450, Loss: 0.5846\n",
      "Iteration 1200/3450, Loss: 0.7678\n",
      "Iteration 1300/3450, Loss: 0.8261\n",
      "Iteration 1400/3450, Loss: 0.6190\n",
      "Iteration 1500/3450, Loss: 0.7537\n",
      "Iteration 1600/3450, Loss: 1.0204\n",
      "Iteration 1700/3450, Loss: 0.5694\n",
      "Iteration 1800/3450, Loss: 0.8674\n",
      "Iteration 1900/3450, Loss: 0.6217\n",
      "Iteration 2000/3450, Loss: 0.6241\n",
      "Iteration 2100/3450, Loss: 0.5174\n",
      "Iteration 2200/3450, Loss: 1.1750\n",
      "Iteration 2300/3450, Loss: 0.5196\n",
      "Iteration 2400/3450, Loss: 0.4401\n",
      "Iteration 2500/3450, Loss: 0.6667\n",
      "Iteration 2600/3450, Loss: 0.4613\n",
      "Iteration 2700/3450, Loss: 0.5116\n",
      "Iteration 2800/3450, Loss: 0.4994\n",
      "Iteration 2900/3450, Loss: 0.3803\n",
      "Iteration 3000/3450, Loss: 0.4891\n",
      "Iteration 3100/3450, Loss: 0.4025\n",
      "Iteration 3200/3450, Loss: 0.4236\n",
      "Iteration 3300/3450, Loss: 0.3047\n",
      "Iteration 3400/3450, Loss: 0.3103\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 4.1406\n",
      "Iteration 200/1725, Loss: 2.8413\n",
      "Iteration 300/1725, Loss: 3.7533\n",
      "Iteration 400/1725, Loss: 4.0700\n",
      "Iteration 500/1725, Loss: 2.0276\n",
      "Iteration 600/1725, Loss: 2.5707\n",
      "Iteration 700/1725, Loss: 1.2580\n",
      "Iteration 800/1725, Loss: 2.4157\n",
      "Iteration 900/1725, Loss: 1.9107\n",
      "Iteration 1000/1725, Loss: 1.2241\n",
      "Iteration 1100/1725, Loss: 1.1643\n",
      "Iteration 1200/1725, Loss: 0.7096\n",
      "Iteration 1300/1725, Loss: 2.3709\n",
      "Iteration 1400/1725, Loss: 0.8994\n",
      "Iteration 1500/1725, Loss: 2.0071\n",
      "Iteration 1600/1725, Loss: 0.5351\n",
      "Iteration 1700/1725, Loss: 0.6076\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 11.0111\n",
      "Iteration 200/825, Loss: 7.1904\n",
      "Iteration 300/825, Loss: 5.4763\n",
      "Iteration 400/825, Loss: 7.6540\n",
      "Iteration 500/825, Loss: 2.0987\n",
      "Iteration 600/825, Loss: 2.4906\n",
      "Iteration 700/825, Loss: 2.8927\n",
      "Iteration 800/825, Loss: 2.5937\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 11.1920\n",
      "Iteration 200/375, Loss: 12.3598\n",
      "Iteration 300/375, Loss: 10.9904\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 15.7526\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 1.8985\n",
      "Iteration 200/28125, Loss: 2.9802\n",
      "Iteration 300/28125, Loss: 3.1515\n",
      "Iteration 400/28125, Loss: 3.2153\n",
      "Iteration 500/28125, Loss: 1.9999\n",
      "Iteration 600/28125, Loss: 1.3874\n",
      "Iteration 700/28125, Loss: 1.0460\n",
      "Iteration 800/28125, Loss: 1.8921\n",
      "Iteration 900/28125, Loss: 1.8894\n",
      "Iteration 1000/28125, Loss: 1.9308\n",
      "Iteration 1100/28125, Loss: 1.2945\n",
      "Iteration 1200/28125, Loss: 0.9886\n",
      "Iteration 1300/28125, Loss: 3.8341\n",
      "Iteration 1400/28125, Loss: 1.8196\n",
      "Iteration 1500/28125, Loss: 0.9982\n",
      "Iteration 1600/28125, Loss: 1.0108\n",
      "Iteration 1700/28125, Loss: 2.3870\n",
      "Iteration 1800/28125, Loss: 1.3963\n",
      "Iteration 1900/28125, Loss: 1.3998\n",
      "Iteration 2000/28125, Loss: 2.0522\n",
      "Iteration 2100/28125, Loss: 1.1455\n",
      "Iteration 2200/28125, Loss: 1.4321\n",
      "Iteration 2300/28125, Loss: 1.0494\n",
      "Iteration 2400/28125, Loss: 1.3946\n",
      "Iteration 2500/28125, Loss: 1.2891\n",
      "Iteration 2600/28125, Loss: 1.0794\n",
      "Iteration 2700/28125, Loss: 0.7417\n",
      "Iteration 2800/28125, Loss: 2.1310\n",
      "Iteration 2900/28125, Loss: 1.6998\n",
      "Iteration 3000/28125, Loss: 1.4642\n",
      "Iteration 3100/28125, Loss: 2.6900\n",
      "Iteration 3200/28125, Loss: 0.9614\n",
      "Iteration 3300/28125, Loss: 0.3969\n",
      "Iteration 3400/28125, Loss: 0.8637\n",
      "Iteration 3500/28125, Loss: 0.8751\n",
      "Iteration 3600/28125, Loss: 1.5809\n",
      "Iteration 3700/28125, Loss: 1.3172\n",
      "Iteration 3800/28125, Loss: 0.6437\n",
      "Iteration 3900/28125, Loss: 0.7270\n",
      "Iteration 4000/28125, Loss: 1.7521\n",
      "Iteration 4100/28125, Loss: 0.4166\n",
      "Iteration 4200/28125, Loss: 1.8016\n",
      "Iteration 4300/28125, Loss: 0.7074\n",
      "Iteration 4400/28125, Loss: 1.2348\n",
      "Iteration 4500/28125, Loss: 1.7019\n",
      "Iteration 4600/28125, Loss: 1.1535\n",
      "Iteration 4700/28125, Loss: 1.4243\n",
      "Iteration 4800/28125, Loss: 1.8601\n",
      "Iteration 4900/28125, Loss: 1.3265\n",
      "Iteration 5000/28125, Loss: 1.5598\n",
      "Iteration 5100/28125, Loss: 1.2147\n",
      "Iteration 5200/28125, Loss: 1.2918\n",
      "Iteration 5300/28125, Loss: 1.3049\n",
      "Iteration 5400/28125, Loss: 0.8987\n",
      "Iteration 5500/28125, Loss: 2.0242\n",
      "Iteration 5600/28125, Loss: 0.7589\n",
      "Iteration 5700/28125, Loss: 2.1807\n",
      "Iteration 5800/28125, Loss: 1.1757\n",
      "Iteration 5900/28125, Loss: 0.3870\n",
      "Iteration 6000/28125, Loss: 1.0392\n",
      "Iteration 6100/28125, Loss: 0.6358\n",
      "Iteration 6200/28125, Loss: 1.2984\n",
      "Iteration 6300/28125, Loss: 1.2857\n",
      "Iteration 6400/28125, Loss: 0.5338\n",
      "Iteration 6500/28125, Loss: 0.6984\n",
      "Iteration 6600/28125, Loss: 0.8371\n",
      "Iteration 6700/28125, Loss: 1.2930\n",
      "Iteration 6800/28125, Loss: 2.3052\n",
      "Iteration 6900/28125, Loss: 0.8635\n",
      "Iteration 7000/28125, Loss: 0.8250\n",
      "Iteration 7100/28125, Loss: 0.5589\n",
      "Iteration 7200/28125, Loss: 0.5776\n",
      "Iteration 7300/28125, Loss: 1.5622\n",
      "Iteration 7400/28125, Loss: 0.9993\n",
      "Iteration 7500/28125, Loss: 0.8525\n",
      "Iteration 7600/28125, Loss: 0.6992\n",
      "Iteration 7700/28125, Loss: 2.0585\n",
      "Iteration 7800/28125, Loss: 1.2355\n",
      "Iteration 7900/28125, Loss: 1.4428\n",
      "Iteration 8000/28125, Loss: 1.0908\n",
      "Iteration 8100/28125, Loss: 0.6679\n",
      "Iteration 8200/28125, Loss: 1.3746\n",
      "Iteration 8300/28125, Loss: 0.9986\n",
      "Iteration 8400/28125, Loss: 1.1410\n",
      "Iteration 8500/28125, Loss: 0.7562\n",
      "Iteration 8600/28125, Loss: 1.2223\n",
      "Iteration 8700/28125, Loss: 0.5418\n",
      "Iteration 8800/28125, Loss: 0.9436\n",
      "Iteration 8900/28125, Loss: 2.2352\n",
      "Iteration 9000/28125, Loss: 1.2769\n",
      "Iteration 9100/28125, Loss: 0.6311\n",
      "Iteration 9200/28125, Loss: 1.1811\n",
      "Iteration 9300/28125, Loss: 1.0454\n",
      "Iteration 9400/28125, Loss: 0.7008\n",
      "Iteration 9500/28125, Loss: 0.5617\n",
      "Iteration 9600/28125, Loss: 1.0569\n",
      "Iteration 9700/28125, Loss: 1.2811\n",
      "Iteration 9800/28125, Loss: 1.1481\n",
      "Iteration 9900/28125, Loss: 1.3081\n",
      "Iteration 10000/28125, Loss: 0.6112\n",
      "Iteration 10100/28125, Loss: 1.4297\n",
      "Iteration 10200/28125, Loss: 1.0817\n",
      "Iteration 10300/28125, Loss: 0.8988\n",
      "Iteration 10400/28125, Loss: 1.2946\n",
      "Iteration 10500/28125, Loss: 1.0890\n",
      "Iteration 10600/28125, Loss: 0.9901\n",
      "Iteration 10700/28125, Loss: 1.3056\n",
      "Iteration 10800/28125, Loss: 1.0153\n",
      "Iteration 10900/28125, Loss: 0.7377\n",
      "Iteration 11000/28125, Loss: 0.8911\n",
      "Iteration 11100/28125, Loss: 0.9751\n",
      "Iteration 11200/28125, Loss: 0.9077\n",
      "Iteration 11300/28125, Loss: 0.7401\n",
      "Iteration 11400/28125, Loss: 0.9479\n",
      "Iteration 11500/28125, Loss: 0.9581\n",
      "Iteration 11600/28125, Loss: 0.8392\n",
      "Iteration 11700/28125, Loss: 1.1546\n",
      "Iteration 11800/28125, Loss: 0.7767\n",
      "Iteration 11900/28125, Loss: 0.7378\n",
      "Iteration 12000/28125, Loss: 1.0549\n",
      "Iteration 12100/28125, Loss: 1.7069\n",
      "Iteration 12200/28125, Loss: 0.8273\n",
      "Iteration 12300/28125, Loss: 0.8550\n",
      "Iteration 12400/28125, Loss: 1.9376\n",
      "Iteration 12500/28125, Loss: 1.0208\n",
      "Iteration 12600/28125, Loss: 1.1058\n",
      "Iteration 12700/28125, Loss: 1.3910\n",
      "Iteration 12800/28125, Loss: 0.9076\n",
      "Iteration 12900/28125, Loss: 0.6887\n",
      "Iteration 13000/28125, Loss: 1.1932\n",
      "Iteration 13100/28125, Loss: 0.6798\n",
      "Iteration 13200/28125, Loss: 1.9412\n",
      "Iteration 13300/28125, Loss: 0.9494\n",
      "Iteration 13400/28125, Loss: 1.1848\n",
      "Iteration 13500/28125, Loss: 0.9203\n",
      "Iteration 13600/28125, Loss: 0.8115\n",
      "Iteration 13700/28125, Loss: 0.8860\n",
      "Iteration 13800/28125, Loss: 0.7140\n",
      "Iteration 13900/28125, Loss: 0.7776\n",
      "Iteration 14000/28125, Loss: 0.9745\n",
      "Iteration 14100/28125, Loss: 0.7727\n",
      "Iteration 14200/28125, Loss: 0.7353\n",
      "Iteration 14300/28125, Loss: 1.0133\n",
      "Iteration 14400/28125, Loss: 1.6131\n",
      "Iteration 14500/28125, Loss: 1.2379\n",
      "Iteration 14600/28125, Loss: 1.0555\n",
      "Iteration 14700/28125, Loss: 0.9211\n",
      "Iteration 14800/28125, Loss: 1.1194\n",
      "Iteration 14900/28125, Loss: 1.1109\n",
      "Iteration 15000/28125, Loss: 1.3972\n",
      "Iteration 15100/28125, Loss: 0.8426\n",
      "Iteration 15200/28125, Loss: 0.9865\n",
      "Iteration 15300/28125, Loss: 1.5949\n",
      "Iteration 15400/28125, Loss: 1.3154\n",
      "Iteration 15500/28125, Loss: 0.9236\n",
      "Iteration 15600/28125, Loss: 1.1100\n",
      "Iteration 15700/28125, Loss: 0.8037\n",
      "Iteration 15800/28125, Loss: 1.0289\n",
      "Iteration 15900/28125, Loss: 1.1112\n",
      "Iteration 16000/28125, Loss: 0.8904\n",
      "Iteration 16100/28125, Loss: 1.1934\n",
      "Iteration 16200/28125, Loss: 1.5631\n",
      "Iteration 16300/28125, Loss: 0.7532\n",
      "Iteration 16400/28125, Loss: 1.4095\n",
      "Iteration 16500/28125, Loss: 1.1228\n",
      "Iteration 16600/28125, Loss: 0.9798\n",
      "Iteration 16700/28125, Loss: 1.0364\n",
      "Iteration 16800/28125, Loss: 1.3927\n",
      "Iteration 16900/28125, Loss: 0.9884\n",
      "Iteration 17000/28125, Loss: 1.0839\n",
      "Iteration 17100/28125, Loss: 0.7997\n",
      "Iteration 17200/28125, Loss: 0.9252\n",
      "Iteration 17300/28125, Loss: 1.0310\n",
      "Iteration 17400/28125, Loss: 0.9121\n",
      "Iteration 17500/28125, Loss: 1.1348\n",
      "Iteration 17600/28125, Loss: 1.0096\n",
      "Iteration 17700/28125, Loss: 1.2385\n",
      "Iteration 17800/28125, Loss: 1.0309\n",
      "Iteration 17900/28125, Loss: 1.0638\n",
      "Iteration 18000/28125, Loss: 1.1065\n",
      "Iteration 18100/28125, Loss: 1.1593\n",
      "Iteration 18200/28125, Loss: 1.2314\n",
      "Iteration 18300/28125, Loss: 1.4349\n",
      "Iteration 18400/28125, Loss: 0.8986\n",
      "Iteration 18500/28125, Loss: 1.4549\n",
      "Iteration 18600/28125, Loss: 1.1060\n",
      "Iteration 18700/28125, Loss: 0.8307\n",
      "Iteration 18800/28125, Loss: 1.0938\n",
      "Iteration 18900/28125, Loss: 1.1004\n",
      "Iteration 19000/28125, Loss: 1.1522\n",
      "Iteration 19100/28125, Loss: 0.9868\n",
      "Iteration 19200/28125, Loss: 1.0372\n",
      "Iteration 19300/28125, Loss: 1.0319\n",
      "Iteration 19400/28125, Loss: 1.0182\n",
      "Iteration 19500/28125, Loss: 1.2724\n",
      "Iteration 19600/28125, Loss: 0.9835\n",
      "Iteration 19700/28125, Loss: 1.0129\n",
      "Iteration 19800/28125, Loss: 1.0681\n",
      "Iteration 19900/28125, Loss: 1.1623\n",
      "Iteration 20000/28125, Loss: 1.6985\n",
      "Iteration 20100/28125, Loss: 1.2170\n",
      "Iteration 20200/28125, Loss: 1.2743\n",
      "Iteration 20300/28125, Loss: 0.8946\n",
      "Iteration 20400/28125, Loss: 1.2216\n",
      "Iteration 20500/28125, Loss: 0.9530\n",
      "Iteration 20600/28125, Loss: 1.0253\n",
      "Iteration 20700/28125, Loss: 1.2113\n",
      "Iteration 20800/28125, Loss: 0.9098\n",
      "Iteration 20900/28125, Loss: 1.0489\n",
      "Iteration 21000/28125, Loss: 1.0520\n",
      "Iteration 21100/28125, Loss: 1.0348\n",
      "Iteration 21200/28125, Loss: 1.3898\n",
      "Iteration 21300/28125, Loss: 0.9235\n",
      "Iteration 21400/28125, Loss: 0.9967\n",
      "Iteration 21500/28125, Loss: 1.3017\n",
      "Iteration 21600/28125, Loss: 1.2312\n",
      "Iteration 21700/28125, Loss: 1.1289\n",
      "Iteration 21800/28125, Loss: 1.2441\n",
      "Iteration 21900/28125, Loss: 1.0594\n",
      "Iteration 22000/28125, Loss: 0.8632\n",
      "Iteration 22100/28125, Loss: 1.1413\n",
      "Iteration 22200/28125, Loss: 0.9381\n",
      "Iteration 22300/28125, Loss: 1.0743\n",
      "Iteration 22400/28125, Loss: 1.1119\n",
      "Iteration 22500/28125, Loss: 1.3800\n",
      "Iteration 22600/28125, Loss: 1.0670\n",
      "Iteration 22700/28125, Loss: 1.2610\n",
      "Iteration 22800/28125, Loss: 1.0777\n",
      "Iteration 22900/28125, Loss: 1.5085\n",
      "Iteration 23000/28125, Loss: 1.1695\n",
      "Iteration 23100/28125, Loss: 1.2144\n",
      "Iteration 23200/28125, Loss: 0.9674\n",
      "Iteration 23300/28125, Loss: 1.0627\n",
      "Iteration 23400/28125, Loss: 0.9222\n",
      "Iteration 23500/28125, Loss: 0.9477\n",
      "Iteration 23600/28125, Loss: 1.1359\n",
      "Iteration 23700/28125, Loss: 0.9107\n",
      "Iteration 23800/28125, Loss: 1.1180\n",
      "Iteration 23900/28125, Loss: 1.1561\n",
      "Iteration 24000/28125, Loss: 1.1140\n",
      "Iteration 24100/28125, Loss: 0.9460\n",
      "Iteration 24200/28125, Loss: 1.0846\n",
      "Iteration 24300/28125, Loss: 1.5629\n",
      "Iteration 24400/28125, Loss: 1.0654\n",
      "Iteration 24500/28125, Loss: 1.3094\n",
      "Iteration 24600/28125, Loss: 1.1670\n",
      "Iteration 24700/28125, Loss: 1.2698\n",
      "Iteration 24800/28125, Loss: 1.1493\n",
      "Iteration 24900/28125, Loss: 1.2523\n",
      "Iteration 25000/28125, Loss: 1.2365\n",
      "Iteration 25100/28125, Loss: 1.2549\n",
      "Iteration 25200/28125, Loss: 1.1564\n",
      "Iteration 25300/28125, Loss: 0.9933\n",
      "Iteration 25400/28125, Loss: 1.3362\n",
      "Iteration 25500/28125, Loss: 0.9916\n",
      "Iteration 25600/28125, Loss: 1.3628\n",
      "Iteration 25700/28125, Loss: 1.1556\n",
      "Iteration 25800/28125, Loss: 1.1133\n",
      "Iteration 25900/28125, Loss: 1.3293\n",
      "Iteration 26000/28125, Loss: 1.5974\n",
      "Iteration 26100/28125, Loss: 1.3189\n",
      "Iteration 26200/28125, Loss: 1.1635\n",
      "Iteration 26300/28125, Loss: 1.1306\n",
      "Iteration 26400/28125, Loss: 1.3352\n",
      "Iteration 26500/28125, Loss: 1.0444\n",
      "Iteration 26600/28125, Loss: 1.1082\n",
      "Iteration 26700/28125, Loss: 1.3691\n",
      "Iteration 26800/28125, Loss: 1.1214\n",
      "Iteration 26900/28125, Loss: 1.1643\n",
      "Iteration 27000/28125, Loss: 1.4297\n",
      "Iteration 27100/28125, Loss: 1.1256\n",
      "Iteration 27200/28125, Loss: 1.2045\n",
      "Iteration 27300/28125, Loss: 1.0862\n",
      "Iteration 27400/28125, Loss: 1.0771\n",
      "Iteration 27500/28125, Loss: 1.1910\n",
      "Iteration 27600/28125, Loss: 1.0575\n",
      "Iteration 27700/28125, Loss: 1.0963\n",
      "Iteration 27800/28125, Loss: 1.1063\n",
      "Iteration 27900/28125, Loss: 1.1390\n",
      "Iteration 28000/28125, Loss: 1.0236\n",
      "Iteration 28100/28125, Loss: 1.3733\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 2.1929\n",
      "Iteration 200/14025, Loss: 2.6776\n",
      "Iteration 300/14025, Loss: 1.5158\n",
      "Iteration 400/14025, Loss: 1.9992\n",
      "Iteration 500/14025, Loss: 1.8947\n",
      "Iteration 600/14025, Loss: 1.4228\n",
      "Iteration 700/14025, Loss: 1.4133\n",
      "Iteration 800/14025, Loss: 1.2951\n",
      "Iteration 900/14025, Loss: 1.1321\n",
      "Iteration 1000/14025, Loss: 2.3633\n",
      "Iteration 1100/14025, Loss: 1.2945\n",
      "Iteration 1200/14025, Loss: 1.3338\n",
      "Iteration 1300/14025, Loss: 0.8056\n",
      "Iteration 1400/14025, Loss: 1.6982\n",
      "Iteration 1500/14025, Loss: 2.0356\n",
      "Iteration 1600/14025, Loss: 1.4998\n",
      "Iteration 1700/14025, Loss: 0.9441\n",
      "Iteration 1800/14025, Loss: 1.4037\n",
      "Iteration 1900/14025, Loss: 1.0067\n",
      "Iteration 2000/14025, Loss: 1.2679\n",
      "Iteration 2100/14025, Loss: 1.4433\n",
      "Iteration 2200/14025, Loss: 1.5209\n",
      "Iteration 2300/14025, Loss: 1.3574\n",
      "Iteration 2400/14025, Loss: 1.6373\n",
      "Iteration 2500/14025, Loss: 1.3315\n",
      "Iteration 2600/14025, Loss: 1.3001\n",
      "Iteration 2700/14025, Loss: 1.2916\n",
      "Iteration 2800/14025, Loss: 1.0467\n",
      "Iteration 2900/14025, Loss: 0.9223\n",
      "Iteration 3000/14025, Loss: 0.8699\n",
      "Iteration 3100/14025, Loss: 1.5304\n",
      "Iteration 3200/14025, Loss: 0.9723\n",
      "Iteration 3300/14025, Loss: 0.9618\n",
      "Iteration 3400/14025, Loss: 1.4260\n",
      "Iteration 3500/14025, Loss: 1.2211\n",
      "Iteration 3600/14025, Loss: 0.7825\n",
      "Iteration 3700/14025, Loss: 1.0511\n",
      "Iteration 3800/14025, Loss: 0.8324\n",
      "Iteration 3900/14025, Loss: 1.0941\n",
      "Iteration 4000/14025, Loss: 1.3175\n",
      "Iteration 4100/14025, Loss: 1.0285\n",
      "Iteration 4200/14025, Loss: 1.3173\n",
      "Iteration 4300/14025, Loss: 0.8930\n",
      "Iteration 4400/14025, Loss: 0.8948\n",
      "Iteration 4500/14025, Loss: 1.5551\n",
      "Iteration 4600/14025, Loss: 1.6174\n",
      "Iteration 4700/14025, Loss: 1.2109\n",
      "Iteration 4800/14025, Loss: 0.9241\n",
      "Iteration 4900/14025, Loss: 0.8261\n",
      "Iteration 5000/14025, Loss: 0.8753\n",
      "Iteration 5100/14025, Loss: 0.9308\n",
      "Iteration 5200/14025, Loss: 1.3583\n",
      "Iteration 5300/14025, Loss: 1.2741\n",
      "Iteration 5400/14025, Loss: 1.0031\n",
      "Iteration 5500/14025, Loss: 1.1972\n",
      "Iteration 5600/14025, Loss: 1.0941\n",
      "Iteration 5700/14025, Loss: 0.9617\n",
      "Iteration 5800/14025, Loss: 0.9336\n",
      "Iteration 5900/14025, Loss: 1.0730\n",
      "Iteration 6000/14025, Loss: 1.3240\n",
      "Iteration 6100/14025, Loss: 0.8646\n",
      "Iteration 6200/14025, Loss: 1.4837\n",
      "Iteration 6300/14025, Loss: 1.0426\n",
      "Iteration 6400/14025, Loss: 1.0265\n",
      "Iteration 6500/14025, Loss: 1.2760\n",
      "Iteration 6600/14025, Loss: 1.4102\n",
      "Iteration 6700/14025, Loss: 1.0028\n",
      "Iteration 6800/14025, Loss: 1.0104\n",
      "Iteration 6900/14025, Loss: 0.9091\n",
      "Iteration 7000/14025, Loss: 1.2047\n",
      "Iteration 7100/14025, Loss: 0.8463\n",
      "Iteration 7200/14025, Loss: 1.2752\n",
      "Iteration 7300/14025, Loss: 1.0239\n",
      "Iteration 7400/14025, Loss: 1.3799\n",
      "Iteration 7500/14025, Loss: 1.7419\n",
      "Iteration 7600/14025, Loss: 1.0928\n",
      "Iteration 7700/14025, Loss: 1.2792\n",
      "Iteration 7800/14025, Loss: 1.2547\n",
      "Iteration 7900/14025, Loss: 0.9737\n",
      "Iteration 8000/14025, Loss: 1.0600\n",
      "Iteration 8100/14025, Loss: 1.3164\n",
      "Iteration 8200/14025, Loss: 1.4540\n",
      "Iteration 8300/14025, Loss: 1.0638\n",
      "Iteration 8400/14025, Loss: 1.2856\n",
      "Iteration 8500/14025, Loss: 1.0871\n",
      "Iteration 8600/14025, Loss: 0.9082\n",
      "Iteration 8700/14025, Loss: 0.9776\n",
      "Iteration 8800/14025, Loss: 1.3907\n",
      "Iteration 8900/14025, Loss: 0.9291\n",
      "Iteration 9000/14025, Loss: 1.2389\n",
      "Iteration 9100/14025, Loss: 1.3629\n",
      "Iteration 9200/14025, Loss: 1.1031\n",
      "Iteration 9300/14025, Loss: 1.1751\n",
      "Iteration 9400/14025, Loss: 1.0622\n",
      "Iteration 9500/14025, Loss: 1.1935\n",
      "Iteration 9600/14025, Loss: 1.1547\n",
      "Iteration 9700/14025, Loss: 1.1620\n",
      "Iteration 9800/14025, Loss: 0.9806\n",
      "Iteration 9900/14025, Loss: 1.1423\n",
      "Iteration 10000/14025, Loss: 1.7191\n",
      "Iteration 10100/14025, Loss: 1.2002\n",
      "Iteration 10200/14025, Loss: 1.1930\n",
      "Iteration 10300/14025, Loss: 1.1666\n",
      "Iteration 10400/14025, Loss: 0.9745\n",
      "Iteration 10500/14025, Loss: 1.0893\n",
      "Iteration 10600/14025, Loss: 1.2914\n",
      "Iteration 10700/14025, Loss: 1.1068\n",
      "Iteration 10800/14025, Loss: 1.3250\n",
      "Iteration 10900/14025, Loss: 1.2241\n",
      "Iteration 11000/14025, Loss: 1.0391\n",
      "Iteration 11100/14025, Loss: 1.1316\n",
      "Iteration 11200/14025, Loss: 1.3508\n",
      "Iteration 11300/14025, Loss: 1.2347\n",
      "Iteration 11400/14025, Loss: 1.2955\n",
      "Iteration 11500/14025, Loss: 1.3601\n",
      "Iteration 11600/14025, Loss: 1.0960\n",
      "Iteration 11700/14025, Loss: 1.0769\n",
      "Iteration 11800/14025, Loss: 1.3250\n",
      "Iteration 11900/14025, Loss: 1.1382\n",
      "Iteration 12000/14025, Loss: 1.3387\n",
      "Iteration 12100/14025, Loss: 1.1855\n",
      "Iteration 12200/14025, Loss: 1.3251\n",
      "Iteration 12300/14025, Loss: 1.2471\n",
      "Iteration 12400/14025, Loss: 1.2321\n",
      "Iteration 12500/14025, Loss: 1.2870\n",
      "Iteration 12600/14025, Loss: 1.3237\n",
      "Iteration 12700/14025, Loss: 1.2703\n",
      "Iteration 12800/14025, Loss: 1.3549\n",
      "Iteration 12900/14025, Loss: 1.3590\n",
      "Iteration 13000/14025, Loss: 1.4751\n",
      "Iteration 13100/14025, Loss: 1.2647\n",
      "Iteration 13200/14025, Loss: 1.2591\n",
      "Iteration 13300/14025, Loss: 1.1519\n",
      "Iteration 13400/14025, Loss: 1.2007\n",
      "Iteration 13500/14025, Loss: 1.3734\n",
      "Iteration 13600/14025, Loss: 1.3032\n",
      "Iteration 13700/14025, Loss: 1.1677\n",
      "Iteration 13800/14025, Loss: 1.2472\n",
      "Iteration 13900/14025, Loss: 1.1902\n",
      "Iteration 14000/14025, Loss: 1.2540\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 2.3243\n",
      "Iteration 200/6975, Loss: 2.2158\n",
      "Iteration 300/6975, Loss: 1.4381\n",
      "Iteration 400/6975, Loss: 2.1434\n",
      "Iteration 500/6975, Loss: 2.1370\n",
      "Iteration 600/6975, Loss: 1.4907\n",
      "Iteration 700/6975, Loss: 2.1209\n",
      "Iteration 800/6975, Loss: 1.7384\n",
      "Iteration 900/6975, Loss: 1.5006\n",
      "Iteration 1000/6975, Loss: 1.4063\n",
      "Iteration 1100/6975, Loss: 1.3514\n",
      "Iteration 1200/6975, Loss: 1.1500\n",
      "Iteration 1300/6975, Loss: 1.4189\n",
      "Iteration 1400/6975, Loss: 1.1531\n",
      "Iteration 1500/6975, Loss: 1.2046\n",
      "Iteration 1600/6975, Loss: 0.9327\n",
      "Iteration 1700/6975, Loss: 1.2664\n",
      "Iteration 1800/6975, Loss: 0.9855\n",
      "Iteration 1900/6975, Loss: 1.2267\n",
      "Iteration 2000/6975, Loss: 1.0570\n",
      "Iteration 2100/6975, Loss: 1.2131\n",
      "Iteration 2200/6975, Loss: 0.9053\n",
      "Iteration 2300/6975, Loss: 1.4176\n",
      "Iteration 2400/6975, Loss: 1.3887\n",
      "Iteration 2500/6975, Loss: 1.0679\n",
      "Iteration 2600/6975, Loss: 1.1537\n",
      "Iteration 2700/6975, Loss: 1.2767\n",
      "Iteration 2800/6975, Loss: 1.0090\n",
      "Iteration 2900/6975, Loss: 1.3257\n",
      "Iteration 3000/6975, Loss: 1.3440\n",
      "Iteration 3100/6975, Loss: 1.3393\n",
      "Iteration 3200/6975, Loss: 1.0817\n",
      "Iteration 3300/6975, Loss: 1.2318\n",
      "Iteration 3400/6975, Loss: 1.0608\n",
      "Iteration 3500/6975, Loss: 1.0138\n",
      "Iteration 3600/6975, Loss: 1.1037\n",
      "Iteration 3700/6975, Loss: 1.2821\n",
      "Iteration 3800/6975, Loss: 1.3242\n",
      "Iteration 3900/6975, Loss: 1.7092\n",
      "Iteration 4000/6975, Loss: 1.3047\n",
      "Iteration 4100/6975, Loss: 1.3239\n",
      "Iteration 4200/6975, Loss: 1.2091\n",
      "Iteration 4300/6975, Loss: 0.9871\n",
      "Iteration 4400/6975, Loss: 1.4451\n",
      "Iteration 4500/6975, Loss: 1.2576\n",
      "Iteration 4600/6975, Loss: 1.1513\n",
      "Iteration 4700/6975, Loss: 1.1993\n",
      "Iteration 4800/6975, Loss: 1.2251\n",
      "Iteration 4900/6975, Loss: 1.0915\n",
      "Iteration 5000/6975, Loss: 1.5767\n",
      "Iteration 5100/6975, Loss: 1.2068\n",
      "Iteration 5200/6975, Loss: 1.2450\n",
      "Iteration 5300/6975, Loss: 1.2534\n",
      "Iteration 5400/6975, Loss: 1.3267\n",
      "Iteration 5500/6975, Loss: 1.2907\n",
      "Iteration 5600/6975, Loss: 1.3848\n",
      "Iteration 5700/6975, Loss: 1.3828\n",
      "Iteration 5800/6975, Loss: 1.2249\n",
      "Iteration 5900/6975, Loss: 1.5087\n",
      "Iteration 6000/6975, Loss: 1.3642\n",
      "Iteration 6100/6975, Loss: 1.3055\n",
      "Iteration 6200/6975, Loss: 1.3367\n",
      "Iteration 6300/6975, Loss: 1.3605\n",
      "Iteration 6400/6975, Loss: 1.4465\n",
      "Iteration 6500/6975, Loss: 1.4338\n",
      "Iteration 6600/6975, Loss: 1.2727\n",
      "Iteration 6700/6975, Loss: 1.3011\n",
      "Iteration 6800/6975, Loss: 1.3159\n",
      "Iteration 6900/6975, Loss: 1.3482\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 2.9720\n",
      "Iteration 200/3450, Loss: 2.9412\n",
      "Iteration 300/3450, Loss: 1.4564\n",
      "Iteration 400/3450, Loss: 1.6075\n",
      "Iteration 500/3450, Loss: 1.8091\n",
      "Iteration 600/3450, Loss: 1.2061\n",
      "Iteration 700/3450, Loss: 3.6101\n",
      "Iteration 800/3450, Loss: 1.3982\n",
      "Iteration 900/3450, Loss: 0.9781\n",
      "Iteration 1000/3450, Loss: 1.6322\n",
      "Iteration 1100/3450, Loss: 1.0533\n",
      "Iteration 1200/3450, Loss: 1.2797\n",
      "Iteration 1300/3450, Loss: 1.3768\n",
      "Iteration 1400/3450, Loss: 1.2134\n",
      "Iteration 1500/3450, Loss: 1.3870\n",
      "Iteration 1600/3450, Loss: 1.6887\n",
      "Iteration 1700/3450, Loss: 1.2782\n",
      "Iteration 1800/3450, Loss: 1.6176\n",
      "Iteration 1900/3450, Loss: 1.4043\n",
      "Iteration 2000/3450, Loss: 1.4451\n",
      "Iteration 2100/3450, Loss: 1.3718\n",
      "Iteration 2200/3450, Loss: 2.0713\n",
      "Iteration 2300/3450, Loss: 1.4482\n",
      "Iteration 2400/3450, Loss: 1.3933\n",
      "Iteration 2500/3450, Loss: 1.6611\n",
      "Iteration 2600/3450, Loss: 1.4747\n",
      "Iteration 2700/3450, Loss: 1.5545\n",
      "Iteration 2800/3450, Loss: 1.5769\n",
      "Iteration 2900/3450, Loss: 1.4810\n",
      "Iteration 3000/3450, Loss: 1.6361\n",
      "Iteration 3100/3450, Loss: 1.5677\n",
      "Iteration 3200/3450, Loss: 1.6093\n",
      "Iteration 3300/3450, Loss: 1.5102\n",
      "Iteration 3400/3450, Loss: 1.5410\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 6.4995\n",
      "Iteration 200/1725, Loss: 2.3996\n",
      "Iteration 300/1725, Loss: 2.9785\n",
      "Iteration 400/1725, Loss: 3.4726\n",
      "Iteration 500/1725, Loss: 2.7309\n",
      "Iteration 600/1725, Loss: 4.5822\n",
      "Iteration 700/1725, Loss: 2.0942\n",
      "Iteration 800/1725, Loss: 3.6514\n",
      "Iteration 900/1725, Loss: 3.0247\n",
      "Iteration 1000/1725, Loss: 2.6476\n",
      "Iteration 1100/1725, Loss: 2.4482\n",
      "Iteration 1200/1725, Loss: 2.7156\n",
      "Iteration 1300/1725, Loss: 4.2243\n",
      "Iteration 1400/1725, Loss: 2.5396\n",
      "Iteration 1500/1725, Loss: 3.3823\n",
      "Iteration 1600/1725, Loss: 2.3463\n",
      "Iteration 1700/1725, Loss: 2.6748\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 12.3695\n",
      "Iteration 200/825, Loss: 8.5394\n",
      "Iteration 300/825, Loss: 5.0717\n",
      "Iteration 400/825, Loss: 9.4701\n",
      "Iteration 500/825, Loss: 4.5992\n",
      "Iteration 600/825, Loss: 6.1379\n",
      "Iteration 700/825, Loss: 5.7827\n",
      "Iteration 800/825, Loss: 6.5118\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 15.0968\n",
      "Iteration 200/375, Loss: 12.2725\n",
      "Iteration 300/375, Loss: 12.3369\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 22.6435\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 2.0735\n",
      "Iteration 200/28125, Loss: 3.1774\n",
      "Iteration 300/28125, Loss: 3.3851\n",
      "Iteration 400/28125, Loss: 3.4785\n",
      "Iteration 500/28125, Loss: 2.2775\n",
      "Iteration 600/28125, Loss: 1.7222\n",
      "Iteration 700/28125, Loss: 1.3922\n",
      "Iteration 800/28125, Loss: 2.2587\n",
      "Iteration 900/28125, Loss: 2.3013\n",
      "Iteration 1000/28125, Loss: 2.3625\n",
      "Iteration 1100/28125, Loss: 1.7501\n",
      "Iteration 1200/28125, Loss: 1.4811\n",
      "Iteration 1300/28125, Loss: 4.3356\n",
      "Iteration 1400/28125, Loss: 2.3797\n",
      "Iteration 1500/28125, Loss: 1.5801\n",
      "Iteration 1600/28125, Loss: 1.6146\n",
      "Iteration 1700/28125, Loss: 2.9698\n",
      "Iteration 1800/28125, Loss: 2.0032\n",
      "Iteration 1900/28125, Loss: 2.0727\n",
      "Iteration 2000/28125, Loss: 2.7160\n",
      "Iteration 2100/28125, Loss: 1.8738\n",
      "Iteration 2200/28125, Loss: 2.1701\n",
      "Iteration 2300/28125, Loss: 1.8387\n",
      "Iteration 2400/28125, Loss: 2.2546\n",
      "Iteration 2500/28125, Loss: 2.1152\n",
      "Iteration 2600/28125, Loss: 1.9776\n",
      "Iteration 2700/28125, Loss: 1.6524\n",
      "Iteration 2800/28125, Loss: 3.1274\n",
      "Iteration 2900/28125, Loss: 2.7357\n",
      "Iteration 3000/28125, Loss: 2.4377\n",
      "Iteration 3100/28125, Loss: 3.7247\n",
      "Iteration 3200/28125, Loss: 1.9689\n",
      "Iteration 3300/28125, Loss: 1.3791\n",
      "Iteration 3400/28125, Loss: 1.9758\n",
      "Iteration 3500/28125, Loss: 1.9922\n",
      "Iteration 3600/28125, Loss: 2.8242\n",
      "Iteration 3700/28125, Loss: 2.4699\n",
      "Iteration 3800/28125, Loss: 1.7650\n",
      "Iteration 3900/28125, Loss: 1.9158\n",
      "Iteration 4000/28125, Loss: 3.0677\n",
      "Iteration 4100/28125, Loss: 1.6438\n",
      "Iteration 4200/28125, Loss: 3.0631\n",
      "Iteration 4300/28125, Loss: 1.9956\n",
      "Iteration 4400/28125, Loss: 2.5873\n",
      "Iteration 4500/28125, Loss: 2.9842\n",
      "Iteration 4600/28125, Loss: 2.4759\n",
      "Iteration 4700/28125, Loss: 2.7226\n",
      "Iteration 4800/28125, Loss: 3.2602\n",
      "Iteration 4900/28125, Loss: 2.7633\n",
      "Iteration 5000/28125, Loss: 3.0591\n",
      "Iteration 5100/28125, Loss: 2.7887\n",
      "Iteration 5200/28125, Loss: 2.7550\n",
      "Iteration 5300/28125, Loss: 2.7975\n",
      "Iteration 5400/28125, Loss: 2.3709\n",
      "Iteration 5500/28125, Loss: 3.5307\n",
      "Iteration 5600/28125, Loss: 2.3579\n",
      "Iteration 5700/28125, Loss: 3.7375\n",
      "Iteration 5800/28125, Loss: 2.7789\n",
      "Iteration 5900/28125, Loss: 1.8982\n",
      "Iteration 6000/28125, Loss: 2.6971\n",
      "Iteration 6100/28125, Loss: 2.1838\n",
      "Iteration 6200/28125, Loss: 2.9506\n",
      "Iteration 6300/28125, Loss: 3.0559\n",
      "Iteration 6400/28125, Loss: 2.1652\n",
      "Iteration 6500/28125, Loss: 2.3001\n",
      "Iteration 6600/28125, Loss: 2.4294\n",
      "Iteration 6700/28125, Loss: 2.8921\n",
      "Iteration 6800/28125, Loss: 4.2342\n",
      "Iteration 6900/28125, Loss: 2.5546\n",
      "Iteration 7000/28125, Loss: 2.5907\n",
      "Iteration 7100/28125, Loss: 2.2674\n",
      "Iteration 7200/28125, Loss: 2.3446\n",
      "Iteration 7300/28125, Loss: 3.3254\n",
      "Iteration 7400/28125, Loss: 2.8153\n",
      "Iteration 7500/28125, Loss: 2.6237\n",
      "Iteration 7600/28125, Loss: 2.5924\n",
      "Iteration 7700/28125, Loss: 3.8831\n",
      "Iteration 7800/28125, Loss: 2.9929\n",
      "Iteration 7900/28125, Loss: 3.3496\n",
      "Iteration 8000/28125, Loss: 2.8749\n",
      "Iteration 8100/28125, Loss: 2.4604\n",
      "Iteration 8200/28125, Loss: 3.3259\n",
      "Iteration 8300/28125, Loss: 2.8926\n",
      "Iteration 8400/28125, Loss: 3.0093\n",
      "Iteration 8500/28125, Loss: 2.5952\n",
      "Iteration 8600/28125, Loss: 3.3286\n",
      "Iteration 8700/28125, Loss: 2.3679\n",
      "Iteration 8800/28125, Loss: 2.8677\n",
      "Iteration 8900/28125, Loss: 4.2316\n",
      "Iteration 9000/28125, Loss: 3.2083\n",
      "Iteration 9100/28125, Loss: 2.6486\n",
      "Iteration 9200/28125, Loss: 3.0703\n",
      "Iteration 9300/28125, Loss: 2.8243\n",
      "Iteration 9400/28125, Loss: 2.7308\n",
      "Iteration 9500/28125, Loss: 2.5183\n",
      "Iteration 9600/28125, Loss: 2.8467\n",
      "Iteration 9700/28125, Loss: 3.3352\n",
      "Iteration 9800/28125, Loss: 3.2368\n",
      "Iteration 9900/28125, Loss: 3.3620\n",
      "Iteration 10000/28125, Loss: 2.5195\n",
      "Iteration 10100/28125, Loss: 3.4457\n",
      "Iteration 10200/28125, Loss: 3.1549\n",
      "Iteration 10300/28125, Loss: 2.8741\n",
      "Iteration 10400/28125, Loss: 3.2233\n",
      "Iteration 10500/28125, Loss: 3.1587\n",
      "Iteration 10600/28125, Loss: 3.1796\n",
      "Iteration 10700/28125, Loss: 3.6872\n",
      "Iteration 10800/28125, Loss: 2.9975\n",
      "Iteration 10900/28125, Loss: 2.6719\n",
      "Iteration 11000/28125, Loss: 2.9300\n",
      "Iteration 11100/28125, Loss: 3.0958\n",
      "Iteration 11200/28125, Loss: 2.9981\n",
      "Iteration 11300/28125, Loss: 2.8665\n",
      "Iteration 11400/28125, Loss: 3.1321\n",
      "Iteration 11500/28125, Loss: 3.2367\n",
      "Iteration 11600/28125, Loss: 3.0698\n",
      "Iteration 11700/28125, Loss: 3.2610\n",
      "Iteration 11800/28125, Loss: 2.9176\n",
      "Iteration 11900/28125, Loss: 2.8276\n",
      "Iteration 12000/28125, Loss: 3.2365\n",
      "Iteration 12100/28125, Loss: 3.8512\n",
      "Iteration 12200/28125, Loss: 3.0859\n",
      "Iteration 12300/28125, Loss: 2.9127\n",
      "Iteration 12400/28125, Loss: 4.4526\n",
      "Iteration 12500/28125, Loss: 3.1753\n",
      "Iteration 12600/28125, Loss: 3.3740\n",
      "Iteration 12700/28125, Loss: 3.7056\n",
      "Iteration 12800/28125, Loss: 3.1913\n",
      "Iteration 12900/28125, Loss: 2.7594\n",
      "Iteration 13000/28125, Loss: 3.5948\n",
      "Iteration 13100/28125, Loss: 2.7327\n",
      "Iteration 13200/28125, Loss: 4.3887\n",
      "Iteration 13300/28125, Loss: 3.1083\n",
      "Iteration 13400/28125, Loss: 3.4846\n",
      "Iteration 13500/28125, Loss: 3.0028\n",
      "Iteration 13600/28125, Loss: 2.9868\n",
      "Iteration 13700/28125, Loss: 3.0012\n",
      "Iteration 13800/28125, Loss: 2.8870\n",
      "Iteration 13900/28125, Loss: 2.8706\n",
      "Iteration 14000/28125, Loss: 3.1403\n",
      "Iteration 14100/28125, Loss: 2.7993\n",
      "Iteration 14200/28125, Loss: 2.8141\n",
      "Iteration 14300/28125, Loss: 3.2771\n",
      "Iteration 14400/28125, Loss: 4.1455\n",
      "Iteration 14500/28125, Loss: 3.8428\n",
      "Iteration 14600/28125, Loss: 3.3530\n",
      "Iteration 14700/28125, Loss: 3.2273\n",
      "Iteration 14800/28125, Loss: 3.4044\n",
      "Iteration 14900/28125, Loss: 3.3504\n",
      "Iteration 15000/28125, Loss: 3.8449\n",
      "Iteration 15100/28125, Loss: 3.3103\n",
      "Iteration 15200/28125, Loss: 3.1843\n",
      "Iteration 15300/28125, Loss: 4.0227\n",
      "Iteration 15400/28125, Loss: 3.8368\n",
      "Iteration 15500/28125, Loss: 3.0177\n",
      "Iteration 15600/28125, Loss: 3.5118\n",
      "Iteration 15700/28125, Loss: 2.8809\n",
      "Iteration 15800/28125, Loss: 3.4946\n",
      "Iteration 15900/28125, Loss: 3.1707\n",
      "Iteration 16000/28125, Loss: 3.1922\n",
      "Iteration 16100/28125, Loss: 3.3999\n",
      "Iteration 16200/28125, Loss: 4.1776\n",
      "Iteration 16300/28125, Loss: 2.8179\n",
      "Iteration 16400/28125, Loss: 3.5038\n",
      "Iteration 16500/28125, Loss: 3.3152\n",
      "Iteration 16600/28125, Loss: 3.4129\n",
      "Iteration 16700/28125, Loss: 3.1992\n",
      "Iteration 16800/28125, Loss: 3.8094\n",
      "Iteration 16900/28125, Loss: 3.2800\n",
      "Iteration 17000/28125, Loss: 3.5889\n",
      "Iteration 17100/28125, Loss: 2.8619\n",
      "Iteration 17200/28125, Loss: 3.0245\n",
      "Iteration 17300/28125, Loss: 3.3395\n",
      "Iteration 17400/28125, Loss: 3.1974\n",
      "Iteration 17500/28125, Loss: 3.4298\n",
      "Iteration 17600/28125, Loss: 3.4220\n",
      "Iteration 17700/28125, Loss: 3.8806\n",
      "Iteration 17800/28125, Loss: 3.6749\n",
      "Iteration 17900/28125, Loss: 3.4448\n",
      "Iteration 18000/28125, Loss: 3.4672\n",
      "Iteration 18100/28125, Loss: 3.4947\n",
      "Iteration 18200/28125, Loss: 3.8405\n",
      "Iteration 18300/28125, Loss: 3.9109\n",
      "Iteration 18400/28125, Loss: 3.3420\n",
      "Iteration 18500/28125, Loss: 3.8815\n",
      "Iteration 18600/28125, Loss: 3.4967\n",
      "Iteration 18700/28125, Loss: 2.9676\n",
      "Iteration 18800/28125, Loss: 3.4345\n",
      "Iteration 18900/28125, Loss: 3.5759\n",
      "Iteration 19000/28125, Loss: 3.8009\n",
      "Iteration 19100/28125, Loss: 3.3568\n",
      "Iteration 19200/28125, Loss: 3.4823\n",
      "Iteration 19300/28125, Loss: 3.0692\n",
      "Iteration 19400/28125, Loss: 3.3893\n",
      "Iteration 19500/28125, Loss: 3.4851\n",
      "Iteration 19600/28125, Loss: 3.2276\n",
      "Iteration 19700/28125, Loss: 3.3641\n",
      "Iteration 19800/28125, Loss: 3.5044\n",
      "Iteration 19900/28125, Loss: 3.3628\n",
      "Iteration 20000/28125, Loss: 4.0225\n",
      "Iteration 20100/28125, Loss: 3.4822\n",
      "Iteration 20200/28125, Loss: 4.0126\n",
      "Iteration 20300/28125, Loss: 2.9568\n",
      "Iteration 20400/28125, Loss: 3.7660\n",
      "Iteration 20500/28125, Loss: 3.2346\n",
      "Iteration 20600/28125, Loss: 3.1933\n",
      "Iteration 20700/28125, Loss: 3.4804\n",
      "Iteration 20800/28125, Loss: 3.0819\n",
      "Iteration 20900/28125, Loss: 3.5944\n",
      "Iteration 21000/28125, Loss: 3.2537\n",
      "Iteration 21100/28125, Loss: 3.3182\n",
      "Iteration 21200/28125, Loss: 3.7460\n",
      "Iteration 21300/28125, Loss: 3.1039\n",
      "Iteration 21400/28125, Loss: 3.2926\n",
      "Iteration 21500/28125, Loss: 3.8012\n",
      "Iteration 21600/28125, Loss: 3.8229\n",
      "Iteration 21700/28125, Loss: 3.3122\n",
      "Iteration 21800/28125, Loss: 3.4716\n",
      "Iteration 21900/28125, Loss: 3.0925\n",
      "Iteration 22000/28125, Loss: 2.8806\n",
      "Iteration 22100/28125, Loss: 3.4125\n",
      "Iteration 22200/28125, Loss: 3.0565\n",
      "Iteration 22300/28125, Loss: 3.5496\n",
      "Iteration 22400/28125, Loss: 3.2669\n",
      "Iteration 22500/28125, Loss: 4.2903\n",
      "Iteration 22600/28125, Loss: 3.4162\n",
      "Iteration 22700/28125, Loss: 3.5934\n",
      "Iteration 22800/28125, Loss: 3.2890\n",
      "Iteration 22900/28125, Loss: 4.3143\n",
      "Iteration 23000/28125, Loss: 3.3787\n",
      "Iteration 23100/28125, Loss: 3.7538\n",
      "Iteration 23200/28125, Loss: 3.0804\n",
      "Iteration 23300/28125, Loss: 3.2141\n",
      "Iteration 23400/28125, Loss: 2.9188\n",
      "Iteration 23500/28125, Loss: 3.1275\n",
      "Iteration 23600/28125, Loss: 3.6571\n",
      "Iteration 23700/28125, Loss: 3.0718\n",
      "Iteration 23800/28125, Loss: 3.5117\n",
      "Iteration 23900/28125, Loss: 3.6322\n",
      "Iteration 24000/28125, Loss: 3.5921\n",
      "Iteration 24100/28125, Loss: 3.0587\n",
      "Iteration 24200/28125, Loss: 3.4326\n",
      "Iteration 24300/28125, Loss: 3.6929\n",
      "Iteration 24400/28125, Loss: 3.2332\n",
      "Iteration 24500/28125, Loss: 3.5981\n",
      "Iteration 24600/28125, Loss: 3.9386\n",
      "Iteration 24700/28125, Loss: 3.4984\n",
      "Iteration 24800/28125, Loss: 3.6024\n",
      "Iteration 24900/28125, Loss: 3.6262\n",
      "Iteration 25000/28125, Loss: 3.4055\n",
      "Iteration 25100/28125, Loss: 4.0023\n",
      "Iteration 25200/28125, Loss: 3.3998\n",
      "Iteration 25300/28125, Loss: 3.0216\n",
      "Iteration 25400/28125, Loss: 3.9466\n",
      "Iteration 25500/28125, Loss: 3.0682\n",
      "Iteration 25600/28125, Loss: 3.9155\n",
      "Iteration 25700/28125, Loss: 3.2568\n",
      "Iteration 25800/28125, Loss: 3.1880\n",
      "Iteration 25900/28125, Loss: 3.8642\n",
      "Iteration 26000/28125, Loss: 4.2674\n",
      "Iteration 26100/28125, Loss: 4.0849\n",
      "Iteration 26200/28125, Loss: 3.3686\n",
      "Iteration 26300/28125, Loss: 3.7742\n",
      "Iteration 26400/28125, Loss: 3.7424\n",
      "Iteration 26500/28125, Loss: 3.0458\n",
      "Iteration 26600/28125, Loss: 3.3651\n",
      "Iteration 26700/28125, Loss: 3.7169\n",
      "Iteration 26800/28125, Loss: 3.7013\n",
      "Iteration 26900/28125, Loss: 3.5552\n",
      "Iteration 27000/28125, Loss: 4.1825\n",
      "Iteration 27100/28125, Loss: 3.6466\n",
      "Iteration 27200/28125, Loss: 3.9100\n",
      "Iteration 27300/28125, Loss: 3.3801\n",
      "Iteration 27400/28125, Loss: 3.1005\n",
      "Iteration 27500/28125, Loss: 3.3950\n",
      "Iteration 27600/28125, Loss: 3.2053\n",
      "Iteration 27700/28125, Loss: 3.1958\n",
      "Iteration 27800/28125, Loss: 3.3564\n",
      "Iteration 27900/28125, Loss: 3.3820\n",
      "Iteration 28000/28125, Loss: 3.0212\n",
      "Iteration 28100/28125, Loss: 3.8799\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 2.4018\n",
      "Iteration 200/14025, Loss: 2.9509\n",
      "Iteration 300/14025, Loss: 1.8628\n",
      "Iteration 400/14025, Loss: 2.4037\n",
      "Iteration 500/14025, Loss: 2.3541\n",
      "Iteration 600/14025, Loss: 1.9530\n",
      "Iteration 700/14025, Loss: 2.0142\n",
      "Iteration 800/14025, Loss: 1.9562\n",
      "Iteration 900/14025, Loss: 1.8423\n",
      "Iteration 1000/14025, Loss: 3.1332\n",
      "Iteration 1100/14025, Loss: 2.1245\n",
      "Iteration 1200/14025, Loss: 2.2676\n",
      "Iteration 1300/14025, Loss: 1.8059\n",
      "Iteration 1400/14025, Loss: 2.7935\n",
      "Iteration 1500/14025, Loss: 3.1653\n",
      "Iteration 1600/14025, Loss: 2.6661\n",
      "Iteration 1700/14025, Loss: 2.1986\n",
      "Iteration 1800/14025, Loss: 2.7493\n",
      "Iteration 1900/14025, Loss: 2.4061\n",
      "Iteration 2000/14025, Loss: 2.7299\n",
      "Iteration 2100/14025, Loss: 2.9572\n",
      "Iteration 2200/14025, Loss: 3.0615\n",
      "Iteration 2300/14025, Loss: 3.0036\n",
      "Iteration 2400/14025, Loss: 3.3063\n",
      "Iteration 2500/14025, Loss: 3.0667\n",
      "Iteration 2600/14025, Loss: 3.0940\n",
      "Iteration 2700/14025, Loss: 3.1313\n",
      "Iteration 2800/14025, Loss: 2.9350\n",
      "Iteration 2900/14025, Loss: 2.8824\n",
      "Iteration 3000/14025, Loss: 2.8529\n",
      "Iteration 3100/14025, Loss: 3.5751\n",
      "Iteration 3200/14025, Loss: 3.1031\n",
      "Iteration 3300/14025, Loss: 3.0969\n",
      "Iteration 3400/14025, Loss: 3.5477\n",
      "Iteration 3500/14025, Loss: 3.5090\n",
      "Iteration 3600/14025, Loss: 3.1010\n",
      "Iteration 3700/14025, Loss: 3.3963\n",
      "Iteration 3800/14025, Loss: 3.2191\n",
      "Iteration 3900/14025, Loss: 3.4683\n",
      "Iteration 4000/14025, Loss: 3.8554\n",
      "Iteration 4100/14025, Loss: 3.5608\n",
      "Iteration 4200/14025, Loss: 3.8700\n",
      "Iteration 4300/14025, Loss: 3.5057\n",
      "Iteration 4400/14025, Loss: 3.5536\n",
      "Iteration 4500/14025, Loss: 4.2609\n",
      "Iteration 4600/14025, Loss: 4.4099\n",
      "Iteration 4700/14025, Loss: 3.9822\n",
      "Iteration 4800/14025, Loss: 3.6577\n",
      "Iteration 4900/14025, Loss: 3.6129\n",
      "Iteration 5000/14025, Loss: 3.6908\n",
      "Iteration 5100/14025, Loss: 3.8030\n",
      "Iteration 5200/14025, Loss: 4.2775\n",
      "Iteration 5300/14025, Loss: 4.4378\n",
      "Iteration 5400/14025, Loss: 4.0118\n",
      "Iteration 5500/14025, Loss: 4.1495\n",
      "Iteration 5600/14025, Loss: 4.2424\n",
      "Iteration 5700/14025, Loss: 4.0469\n",
      "Iteration 5800/14025, Loss: 4.0326\n",
      "Iteration 5900/14025, Loss: 4.3486\n",
      "Iteration 6000/14025, Loss: 4.4919\n",
      "Iteration 6100/14025, Loss: 4.0429\n",
      "Iteration 6200/14025, Loss: 4.8306\n",
      "Iteration 6300/14025, Loss: 4.2907\n",
      "Iteration 6400/14025, Loss: 4.4546\n",
      "Iteration 6500/14025, Loss: 4.6230\n",
      "Iteration 6600/14025, Loss: 4.8541\n",
      "Iteration 6700/14025, Loss: 4.2980\n",
      "Iteration 6800/14025, Loss: 4.5434\n",
      "Iteration 6900/14025, Loss: 4.2905\n",
      "Iteration 7000/14025, Loss: 4.6473\n",
      "Iteration 7100/14025, Loss: 4.2105\n",
      "Iteration 7200/14025, Loss: 4.8152\n",
      "Iteration 7300/14025, Loss: 4.4669\n",
      "Iteration 7400/14025, Loss: 4.9775\n",
      "Iteration 7500/14025, Loss: 5.2922\n",
      "Iteration 7600/14025, Loss: 4.6310\n",
      "Iteration 7700/14025, Loss: 4.9518\n",
      "Iteration 7800/14025, Loss: 4.9750\n",
      "Iteration 7900/14025, Loss: 4.5116\n",
      "Iteration 8000/14025, Loss: 4.7099\n",
      "Iteration 8100/14025, Loss: 5.0383\n",
      "Iteration 8200/14025, Loss: 5.0327\n",
      "Iteration 8300/14025, Loss: 4.8132\n",
      "Iteration 8400/14025, Loss: 4.9762\n",
      "Iteration 8500/14025, Loss: 4.7724\n",
      "Iteration 8600/14025, Loss: 4.5091\n",
      "Iteration 8700/14025, Loss: 4.6877\n",
      "Iteration 8800/14025, Loss: 5.1400\n",
      "Iteration 8900/14025, Loss: 4.6097\n",
      "Iteration 9000/14025, Loss: 5.0914\n",
      "Iteration 9100/14025, Loss: 5.0820\n",
      "Iteration 9200/14025, Loss: 4.9524\n",
      "Iteration 9300/14025, Loss: 5.0117\n",
      "Iteration 9400/14025, Loss: 4.7233\n",
      "Iteration 9500/14025, Loss: 5.1684\n",
      "Iteration 9600/14025, Loss: 4.9746\n",
      "Iteration 9700/14025, Loss: 4.9677\n",
      "Iteration 9800/14025, Loss: 4.7339\n",
      "Iteration 9900/14025, Loss: 4.9997\n",
      "Iteration 10000/14025, Loss: 5.5753\n",
      "Iteration 10100/14025, Loss: 5.0430\n",
      "Iteration 10200/14025, Loss: 5.2403\n",
      "Iteration 10300/14025, Loss: 5.0220\n",
      "Iteration 10400/14025, Loss: 4.7613\n",
      "Iteration 10500/14025, Loss: 4.9044\n",
      "Iteration 10600/14025, Loss: 5.2332\n",
      "Iteration 10700/14025, Loss: 4.9630\n",
      "Iteration 10800/14025, Loss: 5.3448\n",
      "Iteration 10900/14025, Loss: 5.2443\n",
      "Iteration 11000/14025, Loss: 4.9235\n",
      "Iteration 11100/14025, Loss: 5.0231\n",
      "Iteration 11200/14025, Loss: 5.4929\n",
      "Iteration 11300/14025, Loss: 5.2786\n",
      "Iteration 11400/14025, Loss: 5.2600\n",
      "Iteration 11500/14025, Loss: 5.2888\n",
      "Iteration 11600/14025, Loss: 5.0484\n",
      "Iteration 11700/14025, Loss: 4.8905\n",
      "Iteration 11800/14025, Loss: 5.5358\n",
      "Iteration 11900/14025, Loss: 5.1782\n",
      "Iteration 12000/14025, Loss: 5.5682\n",
      "Iteration 12100/14025, Loss: 5.1332\n",
      "Iteration 12200/14025, Loss: 5.8422\n",
      "Iteration 12300/14025, Loss: 5.4432\n",
      "Iteration 12400/14025, Loss: 5.2950\n",
      "Iteration 12500/14025, Loss: 5.3074\n",
      "Iteration 12600/14025, Loss: 5.4110\n",
      "Iteration 12700/14025, Loss: 5.4948\n",
      "Iteration 12800/14025, Loss: 5.6832\n",
      "Iteration 12900/14025, Loss: 5.4449\n",
      "Iteration 13000/14025, Loss: 5.6485\n",
      "Iteration 13100/14025, Loss: 5.2698\n",
      "Iteration 13200/14025, Loss: 5.2550\n",
      "Iteration 13300/14025, Loss: 5.1944\n",
      "Iteration 13400/14025, Loss: 5.2250\n",
      "Iteration 13500/14025, Loss: 5.4946\n",
      "Iteration 13600/14025, Loss: 5.5509\n",
      "Iteration 13700/14025, Loss: 5.1509\n",
      "Iteration 13800/14025, Loss: 5.2600\n",
      "Iteration 13900/14025, Loss: 5.1669\n",
      "Iteration 14000/14025, Loss: 5.3492\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 2.6080\n",
      "Iteration 200/6975, Loss: 2.6463\n",
      "Iteration 300/6975, Loss: 2.0031\n",
      "Iteration 400/6975, Loss: 2.8594\n",
      "Iteration 500/6975, Loss: 3.0008\n",
      "Iteration 600/6975, Loss: 2.5147\n",
      "Iteration 700/6975, Loss: 3.3007\n",
      "Iteration 800/6975, Loss: 3.0725\n",
      "Iteration 900/6975, Loss: 2.9918\n",
      "Iteration 1000/6975, Loss: 3.0675\n",
      "Iteration 1100/6975, Loss: 3.1448\n",
      "Iteration 1200/6975, Loss: 3.0901\n",
      "Iteration 1300/6975, Loss: 3.5180\n",
      "Iteration 1400/6975, Loss: 3.4324\n",
      "Iteration 1500/6975, Loss: 3.5763\n",
      "Iteration 1600/6975, Loss: 3.4676\n",
      "Iteration 1700/6975, Loss: 3.9172\n",
      "Iteration 1800/6975, Loss: 3.7853\n",
      "Iteration 1900/6975, Loss: 4.1469\n",
      "Iteration 2000/6975, Loss: 4.1183\n",
      "Iteration 2100/6975, Loss: 4.3560\n",
      "Iteration 2200/6975, Loss: 4.1897\n",
      "Iteration 2300/6975, Loss: 4.8499\n",
      "Iteration 2400/6975, Loss: 4.9011\n",
      "Iteration 2500/6975, Loss: 4.7370\n",
      "Iteration 2600/6975, Loss: 4.9053\n",
      "Iteration 2700/6975, Loss: 5.2000\n",
      "Iteration 2800/6975, Loss: 5.0382\n",
      "Iteration 2900/6975, Loss: 5.4582\n",
      "Iteration 3000/6975, Loss: 5.6038\n",
      "Iteration 3100/6975, Loss: 5.6839\n",
      "Iteration 3200/6975, Loss: 5.4966\n",
      "Iteration 3300/6975, Loss: 5.7598\n",
      "Iteration 3400/6975, Loss: 5.6388\n",
      "Iteration 3500/6975, Loss: 5.6670\n",
      "Iteration 3600/6975, Loss: 5.8705\n",
      "Iteration 3700/6975, Loss: 6.1471\n",
      "Iteration 3800/6975, Loss: 6.2939\n",
      "Iteration 3900/6975, Loss: 6.7889\n",
      "Iteration 4000/6975, Loss: 6.3226\n",
      "Iteration 4100/6975, Loss: 6.3991\n",
      "Iteration 4200/6975, Loss: 6.3852\n",
      "Iteration 4300/6975, Loss: 6.2129\n",
      "Iteration 4400/6975, Loss: 6.8946\n",
      "Iteration 4500/6975, Loss: 6.6385\n",
      "Iteration 4600/6975, Loss: 6.6306\n",
      "Iteration 4700/6975, Loss: 6.6989\n",
      "Iteration 4800/6975, Loss: 6.8588\n",
      "Iteration 4900/6975, Loss: 6.6940\n",
      "Iteration 5000/6975, Loss: 7.2911\n",
      "Iteration 5100/6975, Loss: 6.9742\n",
      "Iteration 5200/6975, Loss: 6.9731\n",
      "Iteration 5300/6975, Loss: 7.0933\n",
      "Iteration 5400/6975, Loss: 7.1686\n",
      "Iteration 5500/6975, Loss: 7.2018\n",
      "Iteration 5600/6975, Loss: 7.4144\n",
      "Iteration 5700/6975, Loss: 7.4443\n",
      "Iteration 5800/6975, Loss: 7.2297\n",
      "Iteration 5900/6975, Loss: 7.6748\n",
      "Iteration 6000/6975, Loss: 7.5893\n",
      "Iteration 6100/6975, Loss: 7.5333\n",
      "Iteration 6200/6975, Loss: 7.6123\n",
      "Iteration 6300/6975, Loss: 7.6702\n",
      "Iteration 6400/6975, Loss: 7.8502\n",
      "Iteration 6500/6975, Loss: 7.7575\n",
      "Iteration 6600/6975, Loss: 7.5515\n",
      "Iteration 6700/6975, Loss: 7.6762\n",
      "Iteration 6800/6975, Loss: 7.6801\n",
      "Iteration 6900/6975, Loss: 7.7083\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 3.4876\n",
      "Iteration 200/3450, Loss: 3.7555\n",
      "Iteration 300/3450, Loss: 2.7099\n",
      "Iteration 400/3450, Loss: 3.2804\n",
      "Iteration 500/3450, Loss: 3.8491\n",
      "Iteration 600/3450, Loss: 3.6129\n",
      "Iteration 700/3450, Loss: 5.9804\n",
      "Iteration 800/3450, Loss: 4.6710\n",
      "Iteration 900/3450, Loss: 4.5840\n",
      "Iteration 1000/3450, Loss: 5.5702\n",
      "Iteration 1100/3450, Loss: 5.3394\n",
      "Iteration 1200/3450, Loss: 5.9372\n",
      "Iteration 1300/3450, Loss: 6.3660\n",
      "Iteration 1400/3450, Loss: 6.5594\n",
      "Iteration 1500/3450, Loss: 7.0090\n",
      "Iteration 1600/3450, Loss: 7.6225\n",
      "Iteration 1700/3450, Loss: 7.4768\n",
      "Iteration 1800/3450, Loss: 8.1383\n",
      "Iteration 1900/3450, Loss: 8.1587\n",
      "Iteration 2000/3450, Loss: 8.4797\n",
      "Iteration 2100/3450, Loss: 8.6598\n",
      "Iteration 2200/3450, Loss: 9.7539\n",
      "Iteration 2300/3450, Loss: 9.3079\n",
      "Iteration 2400/3450, Loss: 9.3392\n",
      "Iteration 2500/3450, Loss: 9.8287\n",
      "Iteration 2600/3450, Loss: 9.7779\n",
      "Iteration 2700/3450, Loss: 10.3241\n",
      "Iteration 2800/3450, Loss: 10.1894\n",
      "Iteration 2900/3450, Loss: 10.3303\n",
      "Iteration 3000/3450, Loss: 10.7663\n",
      "Iteration 3100/3450, Loss: 10.7946\n",
      "Iteration 3200/3450, Loss: 10.9501\n",
      "Iteration 3300/3450, Loss: 10.9855\n",
      "Iteration 3400/3450, Loss: 11.1635\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 7.9648\n",
      "Iteration 200/1725, Loss: 5.0369\n",
      "Iteration 300/1725, Loss: 6.5386\n",
      "Iteration 400/1725, Loss: 9.1813\n",
      "Iteration 500/1725, Loss: 8.9597\n",
      "Iteration 600/1725, Loss: 11.4036\n",
      "Iteration 700/1725, Loss: 10.7678\n",
      "Iteration 800/1725, Loss: 13.1229\n",
      "Iteration 900/1725, Loss: 14.7849\n",
      "Iteration 1000/1725, Loss: 14.5207\n",
      "Iteration 1100/1725, Loss: 15.2900\n",
      "Iteration 1200/1725, Loss: 16.9829\n",
      "Iteration 1300/1725, Loss: 18.3554\n",
      "Iteration 1400/1725, Loss: 17.8548\n",
      "Iteration 1500/1725, Loss: 19.5731\n",
      "Iteration 1600/1725, Loss: 19.2512\n",
      "Iteration 1700/1725, Loss: 19.9434\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 15.7607\n",
      "Iteration 200/825, Loss: 17.1257\n",
      "Iteration 300/825, Loss: 21.7258\n",
      "Iteration 400/825, Loss: 26.4238\n",
      "Iteration 500/825, Loss: 26.6148\n",
      "Iteration 600/825, Loss: 32.5150\n",
      "Iteration 700/825, Loss: 35.7643\n",
      "Iteration 800/825, Loss: 40.6074\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 27.9535\n",
      "Iteration 200/375, Loss: 46.8534\n",
      "Iteration 300/375, Loss: 61.2790\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 77.9793\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 10.9467\n",
      "Iteration 200/28125, Loss: 12.2081\n",
      "Iteration 300/28125, Loss: 21.2353\n",
      "Iteration 400/28125, Loss: 21.4427\n",
      "Iteration 500/28125, Loss: 13.6863\n",
      "Iteration 600/28125, Loss: 10.2226\n",
      "Iteration 700/28125, Loss: 6.1386\n",
      "Iteration 800/28125, Loss: 13.9807\n",
      "Iteration 900/28125, Loss: 14.6126\n",
      "Iteration 1000/28125, Loss: 10.4078\n",
      "Iteration 1100/28125, Loss: 4.8500\n",
      "Iteration 1200/28125, Loss: 3.8460\n",
      "Iteration 1300/28125, Loss: 16.8011\n",
      "Iteration 1400/28125, Loss: 12.4810\n",
      "Iteration 1500/28125, Loss: 8.8379\n",
      "Iteration 1600/28125, Loss: 10.0825\n",
      "Iteration 1700/28125, Loss: 9.3451\n",
      "Iteration 1800/28125, Loss: 7.1772\n",
      "Iteration 1900/28125, Loss: 6.9470\n",
      "Iteration 2000/28125, Loss: 10.9022\n",
      "Iteration 2100/28125, Loss: 8.4766\n",
      "Iteration 2200/28125, Loss: 7.2195\n",
      "Iteration 2300/28125, Loss: 5.4989\n",
      "Iteration 2400/28125, Loss: 5.4310\n",
      "Iteration 2500/28125, Loss: 6.4991\n",
      "Iteration 2600/28125, Loss: 4.6587\n",
      "Iteration 2700/28125, Loss: 4.9644\n",
      "Iteration 2800/28125, Loss: 17.4846\n",
      "Iteration 2900/28125, Loss: 9.4113\n",
      "Iteration 3000/28125, Loss: 10.5514\n",
      "Iteration 3100/28125, Loss: 21.5880\n",
      "Iteration 3200/28125, Loss: 4.7537\n",
      "Iteration 3300/28125, Loss: 0.6682\n",
      "Iteration 3400/28125, Loss: 6.2211\n",
      "Iteration 3500/28125, Loss: 5.7728\n",
      "Iteration 3600/28125, Loss: 13.0481\n",
      "Iteration 3700/28125, Loss: 10.3596\n",
      "Iteration 3800/28125, Loss: 1.6377\n",
      "Iteration 3900/28125, Loss: 7.7096\n",
      "Iteration 4000/28125, Loss: 11.7524\n",
      "Iteration 4100/28125, Loss: 0.1636\n",
      "Iteration 4200/28125, Loss: 6.7706\n",
      "Iteration 4300/28125, Loss: 6.2593\n",
      "Iteration 4400/28125, Loss: 4.9642\n",
      "Iteration 4500/28125, Loss: 8.7432\n",
      "Iteration 4600/28125, Loss: 4.3916\n",
      "Iteration 4700/28125, Loss: 5.8292\n",
      "Iteration 4800/28125, Loss: 6.6254\n",
      "Iteration 4900/28125, Loss: 6.4266\n",
      "Iteration 5000/28125, Loss: 14.6528\n",
      "Iteration 5100/28125, Loss: 5.8767\n",
      "Iteration 5200/28125, Loss: 4.0815\n",
      "Iteration 5300/28125, Loss: 4.5108\n",
      "Iteration 5400/28125, Loss: 2.4904\n",
      "Iteration 5500/28125, Loss: 12.0672\n",
      "Iteration 5600/28125, Loss: 1.9601\n",
      "Iteration 5700/28125, Loss: 8.3311\n",
      "Iteration 5800/28125, Loss: 4.3434\n",
      "Iteration 5900/28125, Loss: 0.2242\n",
      "Iteration 6000/28125, Loss: 7.0246\n",
      "Iteration 6100/28125, Loss: 0.2336\n",
      "Iteration 6200/28125, Loss: 5.2622\n",
      "Iteration 6300/28125, Loss: 5.3959\n",
      "Iteration 6400/28125, Loss: 0.2444\n",
      "Iteration 6500/28125, Loss: 3.4282\n",
      "Iteration 6600/28125, Loss: 4.9273\n",
      "Iteration 6700/28125, Loss: 14.0384\n",
      "Iteration 6800/28125, Loss: 10.9017\n",
      "Iteration 6900/28125, Loss: 2.1969\n",
      "Iteration 7000/28125, Loss: 4.5830\n",
      "Iteration 7100/28125, Loss: 3.2579\n",
      "Iteration 7200/28125, Loss: 0.2723\n",
      "Iteration 7300/28125, Loss: 8.1556\n",
      "Iteration 7400/28125, Loss: 0.2896\n",
      "Iteration 7500/28125, Loss: 4.5998\n",
      "Iteration 7600/28125, Loss: 0.2874\n",
      "Iteration 7700/28125, Loss: 15.6353\n",
      "Iteration 7800/28125, Loss: 6.0776\n",
      "Iteration 7900/28125, Loss: 9.2591\n",
      "Iteration 8000/28125, Loss: 8.3859\n",
      "Iteration 8100/28125, Loss: 4.6216\n",
      "Iteration 8200/28125, Loss: 6.9720\n",
      "Iteration 8300/28125, Loss: 5.7843\n",
      "Iteration 8400/28125, Loss: 6.2695\n",
      "Iteration 8500/28125, Loss: 6.4732\n",
      "Iteration 8600/28125, Loss: 9.7387\n",
      "Iteration 8700/28125, Loss: 0.9748\n",
      "Iteration 8800/28125, Loss: 0.4523\n",
      "Iteration 8900/28125, Loss: 13.0991\n",
      "Iteration 9000/28125, Loss: 5.9136\n",
      "Iteration 9100/28125, Loss: 1.7395\n",
      "Iteration 9200/28125, Loss: 6.3374\n",
      "Iteration 9300/28125, Loss: 2.4849\n",
      "Iteration 9400/28125, Loss: 1.2025\n",
      "Iteration 9500/28125, Loss: 0.5528\n",
      "Iteration 9600/28125, Loss: 5.4416\n",
      "Iteration 9700/28125, Loss: 5.6137\n",
      "Iteration 9800/28125, Loss: 3.7445\n",
      "Iteration 9900/28125, Loss: 6.6881\n",
      "Iteration 10000/28125, Loss: 1.2646\n",
      "Iteration 10100/28125, Loss: 4.9900\n",
      "Iteration 10200/28125, Loss: 7.5537\n",
      "Iteration 10300/28125, Loss: 4.7009\n",
      "Iteration 10400/28125, Loss: 5.2062\n",
      "Iteration 10500/28125, Loss: 3.7015\n",
      "Iteration 10600/28125, Loss: 2.1763\n",
      "Iteration 10700/28125, Loss: 8.6274\n",
      "Iteration 10800/28125, Loss: 9.4174\n",
      "Iteration 10900/28125, Loss: 1.4043\n",
      "Iteration 11000/28125, Loss: 3.0140\n",
      "Iteration 11100/28125, Loss: 5.0636\n",
      "Iteration 11200/28125, Loss: 2.3467\n",
      "Iteration 11300/28125, Loss: 0.4196\n",
      "Iteration 11400/28125, Loss: 2.0511\n",
      "Iteration 11500/28125, Loss: 1.0994\n",
      "Iteration 11600/28125, Loss: 2.5644\n",
      "Iteration 11700/28125, Loss: 6.1946\n",
      "Iteration 11800/28125, Loss: 1.5088\n",
      "Iteration 11900/28125, Loss: 0.8644\n",
      "Iteration 12000/28125, Loss: 0.7966\n",
      "Iteration 12100/28125, Loss: 2.5667\n",
      "Iteration 12200/28125, Loss: 3.4333\n",
      "Iteration 12300/28125, Loss: 1.1155\n",
      "Iteration 12400/28125, Loss: 14.4673\n",
      "Iteration 12500/28125, Loss: 3.3437\n",
      "Iteration 12600/28125, Loss: 5.8626\n",
      "Iteration 12700/28125, Loss: 6.5554\n",
      "Iteration 12800/28125, Loss: 3.2186\n",
      "Iteration 12900/28125, Loss: 0.4952\n",
      "Iteration 13000/28125, Loss: 2.2904\n",
      "Iteration 13100/28125, Loss: 0.6971\n",
      "Iteration 13200/28125, Loss: 8.1066\n",
      "Iteration 13300/28125, Loss: 3.5679\n",
      "Iteration 13400/28125, Loss: 3.1630\n",
      "Iteration 13500/28125, Loss: 3.8465\n",
      "Iteration 13600/28125, Loss: 1.1783\n",
      "Iteration 13700/28125, Loss: 0.6157\n",
      "Iteration 13800/28125, Loss: 4.7961\n",
      "Iteration 13900/28125, Loss: 1.3061\n",
      "Iteration 14000/28125, Loss: 7.8116\n",
      "Iteration 14100/28125, Loss: 0.6221\n",
      "Iteration 14200/28125, Loss: 1.2772\n",
      "Iteration 14300/28125, Loss: 1.0426\n",
      "Iteration 14400/28125, Loss: 9.5220\n",
      "Iteration 14500/28125, Loss: 3.5492\n",
      "Iteration 14600/28125, Loss: 2.0567\n",
      "Iteration 14700/28125, Loss: 0.7912\n",
      "Iteration 14800/28125, Loss: 4.2374\n",
      "Iteration 14900/28125, Loss: 0.8622\n",
      "Iteration 15000/28125, Loss: 1.9289\n",
      "Iteration 15100/28125, Loss: 2.2904\n",
      "Iteration 15200/28125, Loss: 0.5384\n",
      "Iteration 15300/28125, Loss: 9.4793\n",
      "Iteration 15400/28125, Loss: 3.3604\n",
      "Iteration 15500/28125, Loss: 1.0768\n",
      "Iteration 15600/28125, Loss: 3.5649\n",
      "Iteration 15700/28125, Loss: 0.5521\n",
      "Iteration 15800/28125, Loss: 3.0603\n",
      "Iteration 15900/28125, Loss: 0.7000\n",
      "Iteration 16000/28125, Loss: 1.8112\n",
      "Iteration 16100/28125, Loss: 3.7747\n",
      "Iteration 16200/28125, Loss: 8.3787\n",
      "Iteration 16300/28125, Loss: 0.5741\n",
      "Iteration 16400/28125, Loss: 1.7586\n",
      "Iteration 16500/28125, Loss: 4.7209\n",
      "Iteration 16600/28125, Loss: 0.5898\n",
      "Iteration 16700/28125, Loss: 2.9202\n",
      "Iteration 16800/28125, Loss: 6.0129\n",
      "Iteration 16900/28125, Loss: 1.5209\n",
      "Iteration 17000/28125, Loss: 6.7350\n",
      "Iteration 17100/28125, Loss: 0.7116\n",
      "Iteration 17200/28125, Loss: 0.7628\n",
      "Iteration 17300/28125, Loss: 4.8172\n",
      "Iteration 17400/28125, Loss: 1.4192\n",
      "Iteration 17500/28125, Loss: 3.3620\n",
      "Iteration 17600/28125, Loss: 1.2825\n",
      "Iteration 17700/28125, Loss: 5.8828\n",
      "Iteration 17800/28125, Loss: 0.7197\n",
      "Iteration 17900/28125, Loss: 1.1583\n",
      "Iteration 18000/28125, Loss: 0.6450\n",
      "Iteration 18100/28125, Loss: 1.5965\n",
      "Iteration 18200/28125, Loss: 5.2565\n",
      "Iteration 18300/28125, Loss: 13.9147\n",
      "Iteration 18400/28125, Loss: 0.9471\n",
      "Iteration 18500/28125, Loss: 7.1705\n",
      "Iteration 18600/28125, Loss: 1.7724\n",
      "Iteration 18700/28125, Loss: 1.3084\n",
      "Iteration 18800/28125, Loss: 0.6329\n",
      "Iteration 18900/28125, Loss: 4.9667\n",
      "Iteration 19000/28125, Loss: 3.3261\n",
      "Iteration 19100/28125, Loss: 0.9677\n",
      "Iteration 19200/28125, Loss: 2.7850\n",
      "Iteration 19300/28125, Loss: 0.6744\n",
      "Iteration 19400/28125, Loss: 3.2557\n",
      "Iteration 19500/28125, Loss: 3.1743\n",
      "Iteration 19600/28125, Loss: 0.7188\n",
      "Iteration 19700/28125, Loss: 0.7030\n",
      "Iteration 19800/28125, Loss: 2.3664\n",
      "Iteration 19900/28125, Loss: 2.2755\n",
      "Iteration 20000/28125, Loss: 4.8049\n",
      "Iteration 20100/28125, Loss: 1.4046\n",
      "Iteration 20200/28125, Loss: 2.3979\n",
      "Iteration 20300/28125, Loss: 0.6637\n",
      "Iteration 20400/28125, Loss: 1.4961\n",
      "Iteration 20500/28125, Loss: 0.6673\n",
      "Iteration 20600/28125, Loss: 1.2634\n",
      "Iteration 20700/28125, Loss: 2.5089\n",
      "Iteration 20800/28125, Loss: 0.6735\n",
      "Iteration 20900/28125, Loss: 0.7379\n",
      "Iteration 21000/28125, Loss: 1.8951\n",
      "Iteration 21100/28125, Loss: 0.7503\n",
      "Iteration 21200/28125, Loss: 1.5827\n",
      "Iteration 21300/28125, Loss: 2.1100\n",
      "Iteration 21400/28125, Loss: 0.6923\n",
      "Iteration 21500/28125, Loss: 2.6280\n",
      "Iteration 21600/28125, Loss: 2.4713\n",
      "Iteration 21700/28125, Loss: 2.0188\n",
      "Iteration 21800/28125, Loss: 2.3953\n",
      "Iteration 21900/28125, Loss: 2.2086\n",
      "Iteration 22000/28125, Loss: 0.7032\n",
      "Iteration 22100/28125, Loss: 0.7046\n",
      "Iteration 22200/28125, Loss: 0.7072\n",
      "Iteration 22300/28125, Loss: 0.7380\n",
      "Iteration 22400/28125, Loss: 1.7609\n",
      "Iteration 22500/28125, Loss: 0.7150\n",
      "Iteration 22600/28125, Loss: 3.3159\n",
      "Iteration 22700/28125, Loss: 4.9258\n",
      "Iteration 22800/28125, Loss: 0.8367\n",
      "Iteration 22900/28125, Loss: 5.0361\n",
      "Iteration 23000/28125, Loss: 0.7366\n",
      "Iteration 23100/28125, Loss: 2.4769\n",
      "Iteration 23200/28125, Loss: 1.1928\n",
      "Iteration 23300/28125, Loss: 3.0920\n",
      "Iteration 23400/28125, Loss: 0.7291\n",
      "Iteration 23500/28125, Loss: 1.1236\n",
      "Iteration 23600/28125, Loss: 0.7327\n",
      "Iteration 23700/28125, Loss: 2.0077\n",
      "Iteration 23800/28125, Loss: 0.8645\n",
      "Iteration 23900/28125, Loss: 2.3648\n",
      "Iteration 24000/28125, Loss: 0.7583\n",
      "Iteration 24100/28125, Loss: 0.7492\n",
      "Iteration 24200/28125, Loss: 0.8004\n",
      "Iteration 24300/28125, Loss: 5.0570\n",
      "Iteration 24400/28125, Loss: 0.7466\n",
      "Iteration 24500/28125, Loss: 2.7768\n",
      "Iteration 24600/28125, Loss: 1.0617\n",
      "Iteration 24700/28125, Loss: 0.7573\n",
      "Iteration 24800/28125, Loss: 1.9409\n",
      "Iteration 24900/28125, Loss: 0.7797\n",
      "Iteration 25000/28125, Loss: 3.2921\n",
      "Iteration 25100/28125, Loss: 1.4818\n",
      "Iteration 25200/28125, Loss: 1.4828\n",
      "Iteration 25300/28125, Loss: 1.3899\n",
      "Iteration 25400/28125, Loss: 0.8576\n",
      "Iteration 25500/28125, Loss: 1.3848\n",
      "Iteration 25600/28125, Loss: 2.0931\n",
      "Iteration 25700/28125, Loss: 0.9342\n",
      "Iteration 25800/28125, Loss: 0.7711\n",
      "Iteration 25900/28125, Loss: 1.1033\n",
      "Iteration 26000/28125, Loss: 1.8163\n",
      "Iteration 26100/28125, Loss: 2.0220\n",
      "Iteration 26200/28125, Loss: 3.7440\n",
      "Iteration 26300/28125, Loss: 7.1355\n",
      "Iteration 26400/28125, Loss: 1.1451\n",
      "Iteration 26500/28125, Loss: 2.6542\n",
      "Iteration 26600/28125, Loss: 0.7829\n",
      "Iteration 26700/28125, Loss: 1.4113\n",
      "Iteration 26800/28125, Loss: 0.8496\n",
      "Iteration 26900/28125, Loss: 0.9241\n",
      "Iteration 27000/28125, Loss: 0.9517\n",
      "Iteration 27100/28125, Loss: 1.1777\n",
      "Iteration 27200/28125, Loss: 0.7919\n",
      "Iteration 27300/28125, Loss: 0.9813\n",
      "Iteration 27400/28125, Loss: 5.0481\n",
      "Iteration 27500/28125, Loss: 0.7951\n",
      "Iteration 27600/28125, Loss: 0.7965\n",
      "Iteration 27700/28125, Loss: 0.8017\n",
      "Iteration 27800/28125, Loss: 0.7993\n",
      "Iteration 27900/28125, Loss: 0.8061\n",
      "Iteration 28000/28125, Loss: 0.8015\n",
      "Iteration 28100/28125, Loss: 0.8164\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 15.2931\n",
      "Iteration 200/14025, Loss: 15.8888\n",
      "Iteration 300/14025, Loss: 15.8054\n",
      "Iteration 400/14025, Loss: 15.1627\n",
      "Iteration 500/14025, Loss: 9.2206\n",
      "Iteration 600/14025, Loss: 9.2542\n",
      "Iteration 700/14025, Loss: 10.5093\n",
      "Iteration 800/14025, Loss: 9.7405\n",
      "Iteration 900/14025, Loss: 8.0012\n",
      "Iteration 1000/14025, Loss: 19.5086\n",
      "Iteration 1100/14025, Loss: 5.7201\n",
      "Iteration 1200/14025, Loss: 14.4713\n",
      "Iteration 1300/14025, Loss: 4.4888\n",
      "Iteration 1400/14025, Loss: 10.3725\n",
      "Iteration 1500/14025, Loss: 13.7914\n",
      "Iteration 1600/14025, Loss: 11.4553\n",
      "Iteration 1700/14025, Loss: 7.5596\n",
      "Iteration 1800/14025, Loss: 10.3013\n",
      "Iteration 1900/14025, Loss: 5.2714\n",
      "Iteration 2000/14025, Loss: 8.6188\n",
      "Iteration 2100/14025, Loss: 8.1684\n",
      "Iteration 2200/14025, Loss: 10.0765\n",
      "Iteration 2300/14025, Loss: 5.8995\n",
      "Iteration 2400/14025, Loss: 7.9877\n",
      "Iteration 2500/14025, Loss: 8.2550\n",
      "Iteration 2600/14025, Loss: 10.2808\n",
      "Iteration 2700/14025, Loss: 6.1924\n",
      "Iteration 2800/14025, Loss: 4.5530\n",
      "Iteration 2900/14025, Loss: 4.6705\n",
      "Iteration 3000/14025, Loss: 3.5177\n",
      "Iteration 3100/14025, Loss: 7.3318\n",
      "Iteration 3200/14025, Loss: 4.0761\n",
      "Iteration 3300/14025, Loss: 4.6488\n",
      "Iteration 3400/14025, Loss: 6.3135\n",
      "Iteration 3500/14025, Loss: 8.3719\n",
      "Iteration 3600/14025, Loss: 3.1124\n",
      "Iteration 3700/14025, Loss: 2.8915\n",
      "Iteration 3800/14025, Loss: 3.6742\n",
      "Iteration 3900/14025, Loss: 4.9462\n",
      "Iteration 4000/14025, Loss: 9.5994\n",
      "Iteration 4100/14025, Loss: 6.5826\n",
      "Iteration 4200/14025, Loss: 7.8836\n",
      "Iteration 4300/14025, Loss: 4.3472\n",
      "Iteration 4400/14025, Loss: 5.3594\n",
      "Iteration 4500/14025, Loss: 8.6839\n",
      "Iteration 4600/14025, Loss: 7.2910\n",
      "Iteration 4700/14025, Loss: 3.1195\n",
      "Iteration 4800/14025, Loss: 5.4502\n",
      "Iteration 4900/14025, Loss: 2.4777\n",
      "Iteration 5000/14025, Loss: 3.1107\n",
      "Iteration 5100/14025, Loss: 1.5614\n",
      "Iteration 5200/14025, Loss: 5.4814\n",
      "Iteration 5300/14025, Loss: 4.7943\n",
      "Iteration 5400/14025, Loss: 2.3971\n",
      "Iteration 5500/14025, Loss: 4.2384\n",
      "Iteration 5600/14025, Loss: 6.7337\n",
      "Iteration 5700/14025, Loss: 4.1519\n",
      "Iteration 5800/14025, Loss: 2.2172\n",
      "Iteration 5900/14025, Loss: 3.1628\n",
      "Iteration 6000/14025, Loss: 7.1112\n",
      "Iteration 6100/14025, Loss: 2.3863\n",
      "Iteration 6200/14025, Loss: 7.3937\n",
      "Iteration 6300/14025, Loss: 5.8583\n",
      "Iteration 6400/14025, Loss: 1.5148\n",
      "Iteration 6500/14025, Loss: 5.4542\n",
      "Iteration 6600/14025, Loss: 6.5469\n",
      "Iteration 6700/14025, Loss: 4.2151\n",
      "Iteration 6800/14025, Loss: 4.9557\n",
      "Iteration 6900/14025, Loss: 2.9397\n",
      "Iteration 7000/14025, Loss: 4.8350\n",
      "Iteration 7100/14025, Loss: 3.4543\n",
      "Iteration 7200/14025, Loss: 4.4046\n",
      "Iteration 7300/14025, Loss: 3.5320\n",
      "Iteration 7400/14025, Loss: 2.3254\n",
      "Iteration 7500/14025, Loss: 7.4372\n",
      "Iteration 7600/14025, Loss: 0.9154\n",
      "Iteration 7700/14025, Loss: 9.8865\n",
      "Iteration 7800/14025, Loss: 4.7224\n",
      "Iteration 7900/14025, Loss: 3.7871\n",
      "Iteration 8000/14025, Loss: 2.1273\n",
      "Iteration 8100/14025, Loss: 3.7509\n",
      "Iteration 8200/14025, Loss: 5.0546\n",
      "Iteration 8300/14025, Loss: 1.8087\n",
      "Iteration 8400/14025, Loss: 3.1990\n",
      "Iteration 8500/14025, Loss: 2.2361\n",
      "Iteration 8600/14025, Loss: 0.9362\n",
      "Iteration 8700/14025, Loss: 2.2773\n",
      "Iteration 8800/14025, Loss: 4.2677\n",
      "Iteration 8900/14025, Loss: 1.0176\n",
      "Iteration 9000/14025, Loss: 1.1949\n",
      "Iteration 9100/14025, Loss: 3.2946\n",
      "Iteration 9200/14025, Loss: 4.4146\n",
      "Iteration 9300/14025, Loss: 1.0586\n",
      "Iteration 9400/14025, Loss: 1.5484\n",
      "Iteration 9500/14025, Loss: 1.7808\n",
      "Iteration 9600/14025, Loss: 4.1705\n",
      "Iteration 9700/14025, Loss: 4.9592\n",
      "Iteration 9800/14025, Loss: 1.1218\n",
      "Iteration 9900/14025, Loss: 4.6616\n",
      "Iteration 10000/14025, Loss: 5.3706\n",
      "Iteration 10100/14025, Loss: 4.1996\n",
      "Iteration 10200/14025, Loss: 0.9366\n",
      "Iteration 10300/14025, Loss: 1.3729\n",
      "Iteration 10400/14025, Loss: 0.7221\n",
      "Iteration 10500/14025, Loss: 2.9115\n",
      "Iteration 10600/14025, Loss: 2.9500\n",
      "Iteration 10700/14025, Loss: 1.5742\n",
      "Iteration 10800/14025, Loss: 2.0621\n",
      "Iteration 10900/14025, Loss: 0.7473\n",
      "Iteration 11000/14025, Loss: 0.8932\n",
      "Iteration 11100/14025, Loss: 0.9731\n",
      "Iteration 11200/14025, Loss: 2.7217\n",
      "Iteration 11300/14025, Loss: 2.7282\n",
      "Iteration 11400/14025, Loss: 1.3696\n",
      "Iteration 11500/14025, Loss: 2.3827\n",
      "Iteration 11600/14025, Loss: 0.9234\n",
      "Iteration 11700/14025, Loss: 1.1368\n",
      "Iteration 11800/14025, Loss: 5.8874\n",
      "Iteration 11900/14025, Loss: 1.1143\n",
      "Iteration 12000/14025, Loss: 2.1920\n",
      "Iteration 12100/14025, Loss: 0.7985\n",
      "Iteration 12200/14025, Loss: 4.0364\n",
      "Iteration 12300/14025, Loss: 0.8138\n",
      "Iteration 12400/14025, Loss: 1.7100\n",
      "Iteration 12500/14025, Loss: 1.8729\n",
      "Iteration 12600/14025, Loss: 2.7155\n",
      "Iteration 12700/14025, Loss: 2.1819\n",
      "Iteration 12800/14025, Loss: 2.6020\n",
      "Iteration 12900/14025, Loss: 3.8353\n",
      "Iteration 13000/14025, Loss: 2.6981\n",
      "Iteration 13100/14025, Loss: 1.9124\n",
      "Iteration 13200/14025, Loss: 1.4740\n",
      "Iteration 13300/14025, Loss: 2.0563\n",
      "Iteration 13400/14025, Loss: 3.0735\n",
      "Iteration 13500/14025, Loss: 2.3994\n",
      "Iteration 13600/14025, Loss: 1.4353\n",
      "Iteration 13700/14025, Loss: 3.0261\n",
      "Iteration 13800/14025, Loss: 1.0086\n",
      "Iteration 13900/14025, Loss: 1.2521\n",
      "Iteration 14000/14025, Loss: 1.5676\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 14.0796\n",
      "Iteration 200/6975, Loss: 15.9833\n",
      "Iteration 300/6975, Loss: 8.3717\n",
      "Iteration 400/6975, Loss: 13.5524\n",
      "Iteration 500/6975, Loss: 11.9220\n",
      "Iteration 600/6975, Loss: 9.5240\n",
      "Iteration 700/6975, Loss: 14.0102\n",
      "Iteration 800/6975, Loss: 12.2057\n",
      "Iteration 900/6975, Loss: 14.8482\n",
      "Iteration 1000/6975, Loss: 8.1329\n",
      "Iteration 1100/6975, Loss: 8.9022\n",
      "Iteration 1200/6975, Loss: 5.8275\n",
      "Iteration 1300/6975, Loss: 7.7268\n",
      "Iteration 1400/6975, Loss: 9.2009\n",
      "Iteration 1500/6975, Loss: 7.7832\n",
      "Iteration 1600/6975, Loss: 4.2287\n",
      "Iteration 1700/6975, Loss: 6.1121\n",
      "Iteration 1800/6975, Loss: 5.1594\n",
      "Iteration 1900/6975, Loss: 3.9476\n",
      "Iteration 2000/6975, Loss: 5.5467\n",
      "Iteration 2100/6975, Loss: 4.8625\n",
      "Iteration 2200/6975, Loss: 3.6697\n",
      "Iteration 2300/6975, Loss: 6.6747\n",
      "Iteration 2400/6975, Loss: 3.9865\n",
      "Iteration 2500/6975, Loss: 4.0349\n",
      "Iteration 2600/6975, Loss: 5.1237\n",
      "Iteration 2700/6975, Loss: 4.9142\n",
      "Iteration 2800/6975, Loss: 3.1730\n",
      "Iteration 2900/6975, Loss: 6.3158\n",
      "Iteration 3000/6975, Loss: 5.9470\n",
      "Iteration 3100/6975, Loss: 6.4708\n",
      "Iteration 3200/6975, Loss: 5.8070\n",
      "Iteration 3300/6975, Loss: 5.5247\n",
      "Iteration 3400/6975, Loss: 6.6158\n",
      "Iteration 3500/6975, Loss: 2.9409\n",
      "Iteration 3600/6975, Loss: 2.7323\n",
      "Iteration 3700/6975, Loss: 5.3891\n",
      "Iteration 3800/6975, Loss: 3.0765\n",
      "Iteration 3900/6975, Loss: 7.4981\n",
      "Iteration 4000/6975, Loss: 4.0692\n",
      "Iteration 4100/6975, Loss: 5.3675\n",
      "Iteration 4200/6975, Loss: 3.7023\n",
      "Iteration 4300/6975, Loss: 0.7496\n",
      "Iteration 4400/6975, Loss: 3.9995\n",
      "Iteration 4500/6975, Loss: 3.0038\n",
      "Iteration 4600/6975, Loss: 4.0036\n",
      "Iteration 4700/6975, Loss: 3.6842\n",
      "Iteration 4800/6975, Loss: 4.6463\n",
      "Iteration 4900/6975, Loss: 1.5654\n",
      "Iteration 5000/6975, Loss: 6.1253\n",
      "Iteration 5100/6975, Loss: 2.0440\n",
      "Iteration 5200/6975, Loss: 2.5693\n",
      "Iteration 5300/6975, Loss: 1.7479\n",
      "Iteration 5400/6975, Loss: 3.2837\n",
      "Iteration 5500/6975, Loss: 4.2239\n",
      "Iteration 5600/6975, Loss: 5.1012\n",
      "Iteration 5700/6975, Loss: 3.9461\n",
      "Iteration 5800/6975, Loss: 2.6999\n",
      "Iteration 5900/6975, Loss: 4.4248\n",
      "Iteration 6000/6975, Loss: 1.6923\n",
      "Iteration 6100/6975, Loss: 1.5590\n",
      "Iteration 6200/6975, Loss: 1.1376\n",
      "Iteration 6300/6975, Loss: 2.0792\n",
      "Iteration 6400/6975, Loss: 6.0911\n",
      "Iteration 6500/6975, Loss: 2.4186\n",
      "Iteration 6600/6975, Loss: 1.0710\n",
      "Iteration 6700/6975, Loss: 2.1497\n",
      "Iteration 6800/6975, Loss: 1.8379\n",
      "Iteration 6900/6975, Loss: 1.4612\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 18.0547\n",
      "Iteration 200/3450, Loss: 17.9760\n",
      "Iteration 300/3450, Loss: 12.7997\n",
      "Iteration 400/3450, Loss: 9.6214\n",
      "Iteration 500/3450, Loss: 9.1020\n",
      "Iteration 600/3450, Loss: 7.2006\n",
      "Iteration 700/3450, Loss: 13.9943\n",
      "Iteration 800/3450, Loss: 7.1226\n",
      "Iteration 900/3450, Loss: 5.2078\n",
      "Iteration 1000/3450, Loss: 7.2301\n",
      "Iteration 1100/3450, Loss: 3.7558\n",
      "Iteration 1200/3450, Loss: 7.4714\n",
      "Iteration 1300/3450, Loss: 4.3865\n",
      "Iteration 1400/3450, Loss: 4.9914\n",
      "Iteration 1500/3450, Loss: 6.8742\n",
      "Iteration 1600/3450, Loss: 10.0540\n",
      "Iteration 1700/3450, Loss: 5.4278\n",
      "Iteration 1800/3450, Loss: 7.8605\n",
      "Iteration 1900/3450, Loss: 4.8000\n",
      "Iteration 2000/3450, Loss: 6.6662\n",
      "Iteration 2100/3450, Loss: 2.7527\n",
      "Iteration 2200/3450, Loss: 10.2481\n",
      "Iteration 2300/3450, Loss: 3.8070\n",
      "Iteration 2400/3450, Loss: 3.8243\n",
      "Iteration 2500/3450, Loss: 7.9775\n",
      "Iteration 2600/3450, Loss: 3.1484\n",
      "Iteration 2700/3450, Loss: 6.6565\n",
      "Iteration 2800/3450, Loss: 5.5565\n",
      "Iteration 2900/3450, Loss: 4.7578\n",
      "Iteration 3000/3450, Loss: 6.7253\n",
      "Iteration 3100/3450, Loss: 2.4967\n",
      "Iteration 3200/3450, Loss: 4.0592\n",
      "Iteration 3300/3450, Loss: 2.2950\n",
      "Iteration 3400/3450, Loss: 2.3287\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 19.8539\n",
      "Iteration 200/1725, Loss: 15.4125\n",
      "Iteration 300/1725, Loss: 14.5410\n",
      "Iteration 400/1725, Loss: 16.3582\n",
      "Iteration 500/1725, Loss: 11.1553\n",
      "Iteration 600/1725, Loss: 10.2671\n",
      "Iteration 700/1725, Loss: 11.5727\n",
      "Iteration 800/1725, Loss: 11.5686\n",
      "Iteration 900/1725, Loss: 13.9316\n",
      "Iteration 1000/1725, Loss: 8.3727\n",
      "Iteration 1100/1725, Loss: 7.3538\n",
      "Iteration 1200/1725, Loss: 4.5954\n",
      "Iteration 1300/1725, Loss: 10.1033\n",
      "Iteration 1400/1725, Loss: 6.5041\n",
      "Iteration 1500/1725, Loss: 10.1424\n",
      "Iteration 1600/1725, Loss: 7.8871\n",
      "Iteration 1700/1725, Loss: 7.5825\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 20.7348\n",
      "Iteration 200/825, Loss: 15.2043\n",
      "Iteration 300/825, Loss: 17.7596\n",
      "Iteration 400/825, Loss: 19.0467\n",
      "Iteration 500/825, Loss: 11.8686\n",
      "Iteration 600/825, Loss: 14.7937\n",
      "Iteration 700/825, Loss: 12.6133\n",
      "Iteration 800/825, Loss: 14.4091\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 21.8598\n",
      "Iteration 200/375, Loss: 22.0087\n",
      "Iteration 300/375, Loss: 20.6266\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 27.1978\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 8.0881\n",
      "Iteration 200/28125, Loss: 15.4550\n",
      "Iteration 300/28125, Loss: 21.5289\n",
      "Iteration 400/28125, Loss: 20.5446\n",
      "Iteration 500/28125, Loss: 13.5350\n",
      "Iteration 600/28125, Loss: 8.6624\n",
      "Iteration 700/28125, Loss: 4.2676\n",
      "Iteration 800/28125, Loss: 9.5033\n",
      "Iteration 900/28125, Loss: 17.5528\n",
      "Iteration 1000/28125, Loss: 6.8709\n",
      "Iteration 1100/28125, Loss: 7.1768\n",
      "Iteration 1200/28125, Loss: 6.9034\n",
      "Iteration 1300/28125, Loss: 22.0528\n",
      "Iteration 1400/28125, Loss: 14.0525\n",
      "Iteration 1500/28125, Loss: 5.4593\n",
      "Iteration 1600/28125, Loss: 13.5388\n",
      "Iteration 1700/28125, Loss: 9.2361\n",
      "Iteration 1800/28125, Loss: 11.9450\n",
      "Iteration 1900/28125, Loss: 8.1359\n",
      "Iteration 2000/28125, Loss: 12.0451\n",
      "Iteration 2100/28125, Loss: 5.9600\n",
      "Iteration 2200/28125, Loss: 5.9728\n",
      "Iteration 2300/28125, Loss: 3.2050\n",
      "Iteration 2400/28125, Loss: 11.4296\n",
      "Iteration 2500/28125, Loss: 9.8077\n",
      "Iteration 2600/28125, Loss: 6.5546\n",
      "Iteration 2700/28125, Loss: 9.8738\n",
      "Iteration 2800/28125, Loss: 16.8699\n",
      "Iteration 2900/28125, Loss: 12.0978\n",
      "Iteration 3000/28125, Loss: 4.5990\n",
      "Iteration 3100/28125, Loss: 22.8973\n",
      "Iteration 3200/28125, Loss: 2.3843\n",
      "Iteration 3300/28125, Loss: 4.8235\n",
      "Iteration 3400/28125, Loss: 8.2024\n",
      "Iteration 3500/28125, Loss: 4.6554\n",
      "Iteration 3600/28125, Loss: 13.8738\n",
      "Iteration 3700/28125, Loss: 7.0001\n",
      "Iteration 3800/28125, Loss: 2.7856\n",
      "Iteration 3900/28125, Loss: 1.5870\n",
      "Iteration 4000/28125, Loss: 13.5653\n",
      "Iteration 4100/28125, Loss: 2.5239\n",
      "Iteration 4200/28125, Loss: 10.1961\n",
      "Iteration 4300/28125, Loss: 3.9962\n",
      "Iteration 4400/28125, Loss: 8.1127\n",
      "Iteration 4500/28125, Loss: 11.6064\n",
      "Iteration 4600/28125, Loss: 6.8928\n",
      "Iteration 4700/28125, Loss: 7.3329\n",
      "Iteration 4800/28125, Loss: 7.9615\n",
      "Iteration 4900/28125, Loss: 8.9379\n",
      "Iteration 5000/28125, Loss: 11.2420\n",
      "Iteration 5100/28125, Loss: 9.1277\n",
      "Iteration 5200/28125, Loss: 6.7218\n",
      "Iteration 5300/28125, Loss: 11.5185\n",
      "Iteration 5400/28125, Loss: 4.5824\n",
      "Iteration 5500/28125, Loss: 10.7338\n",
      "Iteration 5600/28125, Loss: 7.3909\n",
      "Iteration 5700/28125, Loss: 15.6487\n",
      "Iteration 5800/28125, Loss: 8.4956\n",
      "Iteration 5900/28125, Loss: 2.1364\n",
      "Iteration 6000/28125, Loss: 8.0514\n",
      "Iteration 6100/28125, Loss: 2.2913\n",
      "Iteration 6200/28125, Loss: 8.1625\n",
      "Iteration 6300/28125, Loss: 10.3229\n",
      "Iteration 6400/28125, Loss: 2.5957\n",
      "Iteration 6500/28125, Loss: 4.7856\n",
      "Iteration 6600/28125, Loss: 5.1612\n",
      "Iteration 6700/28125, Loss: 7.8474\n",
      "Iteration 6800/28125, Loss: 9.2392\n",
      "Iteration 6900/28125, Loss: 4.8165\n",
      "Iteration 7000/28125, Loss: 4.9135\n",
      "Iteration 7100/28125, Loss: 5.3947\n",
      "Iteration 7200/28125, Loss: 3.5383\n",
      "Iteration 7300/28125, Loss: 11.1243\n",
      "Iteration 7400/28125, Loss: 6.0189\n",
      "Iteration 7500/28125, Loss: 4.6634\n",
      "Iteration 7600/28125, Loss: 9.3238\n",
      "Iteration 7700/28125, Loss: 6.1978\n",
      "Iteration 7800/28125, Loss: 5.6734\n",
      "Iteration 7900/28125, Loss: 5.3791\n",
      "Iteration 8000/28125, Loss: 6.5609\n",
      "Iteration 8100/28125, Loss: 4.0091\n",
      "Iteration 8200/28125, Loss: 13.9652\n",
      "Iteration 8300/28125, Loss: 5.1800\n",
      "Iteration 8400/28125, Loss: 6.5535\n",
      "Iteration 8500/28125, Loss: 4.8502\n",
      "Iteration 8600/28125, Loss: 8.3830\n",
      "Iteration 8700/28125, Loss: 3.4788\n",
      "Iteration 8800/28125, Loss: 6.8329\n",
      "Iteration 8900/28125, Loss: 16.8603\n",
      "Iteration 9000/28125, Loss: 8.1430\n",
      "Iteration 9100/28125, Loss: 4.0231\n",
      "Iteration 9200/28125, Loss: 9.5262\n",
      "Iteration 9300/28125, Loss: 7.9719\n",
      "Iteration 9400/28125, Loss: 5.2305\n",
      "Iteration 9500/28125, Loss: 6.5050\n",
      "Iteration 9600/28125, Loss: 6.9440\n",
      "Iteration 9700/28125, Loss: 8.0080\n",
      "Iteration 9800/28125, Loss: 5.3436\n",
      "Iteration 9900/28125, Loss: 10.8890\n",
      "Iteration 10000/28125, Loss: 3.5359\n",
      "Iteration 10100/28125, Loss: 8.4067\n",
      "Iteration 10200/28125, Loss: 10.8307\n",
      "Iteration 10300/28125, Loss: 7.8523\n",
      "Iteration 10400/28125, Loss: 11.1896\n",
      "Iteration 10500/28125, Loss: 6.6004\n",
      "Iteration 10600/28125, Loss: 6.8341\n",
      "Iteration 10700/28125, Loss: 10.8091\n",
      "Iteration 10800/28125, Loss: 7.2736\n",
      "Iteration 10900/28125, Loss: 3.9624\n",
      "Iteration 11000/28125, Loss: 8.4918\n",
      "Iteration 11100/28125, Loss: 4.9292\n",
      "Iteration 11200/28125, Loss: 6.2846\n",
      "Iteration 11300/28125, Loss: 5.2216\n",
      "Iteration 11400/28125, Loss: 3.8481\n",
      "Iteration 11500/28125, Loss: 4.7561\n",
      "Iteration 11600/28125, Loss: 3.8948\n",
      "Iteration 11700/28125, Loss: 7.9790\n",
      "Iteration 11800/28125, Loss: 4.1154\n",
      "Iteration 11900/28125, Loss: 3.9760\n",
      "Iteration 12000/28125, Loss: 8.9324\n",
      "Iteration 12100/28125, Loss: 8.7628\n",
      "Iteration 12200/28125, Loss: 4.1073\n",
      "Iteration 12300/28125, Loss: 4.2659\n",
      "Iteration 12400/28125, Loss: 11.0799\n",
      "Iteration 12500/28125, Loss: 6.0926\n",
      "Iteration 12600/28125, Loss: 10.4582\n",
      "Iteration 12700/28125, Loss: 11.2608\n",
      "Iteration 12800/28125, Loss: 9.4069\n",
      "Iteration 12900/28125, Loss: 5.8165\n",
      "Iteration 13000/28125, Loss: 10.7647\n",
      "Iteration 13100/28125, Loss: 5.1631\n",
      "Iteration 13200/28125, Loss: 7.3199\n",
      "Iteration 13300/28125, Loss: 6.6350\n",
      "Iteration 13400/28125, Loss: 5.1985\n",
      "Iteration 13500/28125, Loss: 5.3374\n",
      "Iteration 13600/28125, Loss: 5.7530\n",
      "Iteration 13700/28125, Loss: 7.2432\n",
      "Iteration 13800/28125, Loss: 4.5774\n",
      "Iteration 13900/28125, Loss: 5.0193\n",
      "Iteration 14000/28125, Loss: 6.6724\n",
      "Iteration 14100/28125, Loss: 4.5217\n",
      "Iteration 14200/28125, Loss: 5.8886\n",
      "Iteration 14300/28125, Loss: 8.1648\n",
      "Iteration 14400/28125, Loss: 7.6893\n",
      "Iteration 14500/28125, Loss: 12.9998\n",
      "Iteration 14600/28125, Loss: 5.9575\n",
      "Iteration 14700/28125, Loss: 4.6530\n",
      "Iteration 14800/28125, Loss: 9.0235\n",
      "Iteration 14900/28125, Loss: 7.1736\n",
      "Iteration 15000/28125, Loss: 8.6245\n",
      "Iteration 15100/28125, Loss: 5.3724\n",
      "Iteration 15200/28125, Loss: 7.1913\n",
      "Iteration 15300/28125, Loss: 8.6385\n",
      "Iteration 15400/28125, Loss: 4.8051\n",
      "Iteration 15500/28125, Loss: 4.9392\n",
      "Iteration 15600/28125, Loss: 7.7849\n",
      "Iteration 15700/28125, Loss: 4.8869\n",
      "Iteration 15800/28125, Loss: 5.4490\n",
      "Iteration 15900/28125, Loss: 6.4095\n",
      "Iteration 16000/28125, Loss: 6.1869\n",
      "Iteration 16100/28125, Loss: 8.5267\n",
      "Iteration 16200/28125, Loss: 19.3778\n",
      "Iteration 16300/28125, Loss: 5.0008\n",
      "Iteration 16400/28125, Loss: 11.8712\n",
      "Iteration 16500/28125, Loss: 6.0946\n",
      "Iteration 16600/28125, Loss: 9.2528\n",
      "Iteration 16700/28125, Loss: 7.0796\n",
      "Iteration 16800/28125, Loss: 14.0022\n",
      "Iteration 16900/28125, Loss: 5.9474\n",
      "Iteration 17000/28125, Loss: 5.3567\n",
      "Iteration 17100/28125, Loss: 5.6522\n",
      "Iteration 17200/28125, Loss: 5.1782\n",
      "Iteration 17300/28125, Loss: 5.6996\n",
      "Iteration 17400/28125, Loss: 5.2138\n",
      "Iteration 17500/28125, Loss: 6.5120\n",
      "Iteration 17600/28125, Loss: 10.6347\n",
      "Iteration 17700/28125, Loss: 7.5351\n",
      "Iteration 17800/28125, Loss: 6.9852\n",
      "Iteration 17900/28125, Loss: 8.6336\n",
      "Iteration 18000/28125, Loss: 5.7239\n",
      "Iteration 18100/28125, Loss: 7.6954\n",
      "Iteration 18200/28125, Loss: 10.4653\n",
      "Iteration 18300/28125, Loss: 9.2628\n",
      "Iteration 18400/28125, Loss: 7.4510\n",
      "Iteration 18500/28125, Loss: 10.9374\n",
      "Iteration 18600/28125, Loss: 10.2156\n",
      "Iteration 18700/28125, Loss: 5.4387\n",
      "Iteration 18800/28125, Loss: 6.5708\n",
      "Iteration 18900/28125, Loss: 5.8361\n",
      "Iteration 19000/28125, Loss: 7.6488\n",
      "Iteration 19100/28125, Loss: 6.9309\n",
      "Iteration 19200/28125, Loss: 10.2368\n",
      "Iteration 19300/28125, Loss: 6.1485\n",
      "Iteration 19400/28125, Loss: 7.1101\n",
      "Iteration 19500/28125, Loss: 6.7898\n",
      "Iteration 19600/28125, Loss: 6.8123\n",
      "Iteration 19700/28125, Loss: 8.1461\n",
      "Iteration 19800/28125, Loss: 7.5295\n",
      "Iteration 19900/28125, Loss: 6.8487\n",
      "Iteration 20000/28125, Loss: 12.7060\n",
      "Iteration 20100/28125, Loss: 8.3098\n",
      "Iteration 20200/28125, Loss: 10.1184\n",
      "Iteration 20300/28125, Loss: 5.6857\n",
      "Iteration 20400/28125, Loss: 7.7298\n",
      "Iteration 20500/28125, Loss: 5.7206\n",
      "Iteration 20600/28125, Loss: 7.6377\n",
      "Iteration 20700/28125, Loss: 8.9209\n",
      "Iteration 20800/28125, Loss: 5.7754\n",
      "Iteration 20900/28125, Loss: 6.4896\n",
      "Iteration 21000/28125, Loss: 5.8613\n",
      "Iteration 21100/28125, Loss: 5.8135\n",
      "Iteration 21200/28125, Loss: 9.2441\n",
      "Iteration 21300/28125, Loss: 7.7831\n",
      "Iteration 21400/28125, Loss: 5.8604\n",
      "Iteration 21500/28125, Loss: 7.5810\n",
      "Iteration 21600/28125, Loss: 8.5525\n",
      "Iteration 21700/28125, Loss: 7.2439\n",
      "Iteration 21800/28125, Loss: 9.1891\n",
      "Iteration 21900/28125, Loss: 6.2881\n",
      "Iteration 22000/28125, Loss: 5.9457\n",
      "Iteration 22100/28125, Loss: 5.9607\n",
      "Iteration 22200/28125, Loss: 5.9777\n",
      "Iteration 22300/28125, Loss: 6.1208\n",
      "Iteration 22400/28125, Loss: 7.3090\n",
      "Iteration 22500/28125, Loss: 11.4892\n",
      "Iteration 22600/28125, Loss: 6.6984\n",
      "Iteration 22700/28125, Loss: 10.3760\n",
      "Iteration 22800/28125, Loss: 10.8270\n",
      "Iteration 22900/28125, Loss: 13.6980\n",
      "Iteration 23000/28125, Loss: 8.6059\n",
      "Iteration 23100/28125, Loss: 6.8151\n",
      "Iteration 23200/28125, Loss: 7.4174\n",
      "Iteration 23300/28125, Loss: 7.7722\n",
      "Iteration 23400/28125, Loss: 7.6413\n",
      "Iteration 23500/28125, Loss: 7.0904\n",
      "Iteration 23600/28125, Loss: 9.1092\n",
      "Iteration 23700/28125, Loss: 6.1803\n",
      "Iteration 23800/28125, Loss: 10.2154\n",
      "Iteration 23900/28125, Loss: 7.6089\n",
      "Iteration 24000/28125, Loss: 6.2226\n",
      "Iteration 24100/28125, Loss: 6.2484\n",
      "Iteration 24200/28125, Loss: 6.2894\n",
      "Iteration 24300/28125, Loss: 9.1015\n",
      "Iteration 24400/28125, Loss: 9.1829\n",
      "Iteration 24500/28125, Loss: 6.7495\n",
      "Iteration 24600/28125, Loss: 8.3834\n",
      "Iteration 24700/28125, Loss: 6.3010\n",
      "Iteration 24800/28125, Loss: 6.3172\n",
      "Iteration 24900/28125, Loss: 8.1261\n",
      "Iteration 25000/28125, Loss: 10.4127\n",
      "Iteration 25100/28125, Loss: 8.9258\n",
      "Iteration 25200/28125, Loss: 10.1221\n",
      "Iteration 25300/28125, Loss: 6.3680\n",
      "Iteration 25400/28125, Loss: 7.7115\n",
      "Iteration 25500/28125, Loss: 6.4930\n",
      "Iteration 25600/28125, Loss: 6.3972\n",
      "Iteration 25700/28125, Loss: 6.4121\n",
      "Iteration 25800/28125, Loss: 6.4220\n",
      "Iteration 25900/28125, Loss: 8.3065\n",
      "Iteration 26000/28125, Loss: 9.1148\n",
      "Iteration 26100/28125, Loss: 11.2267\n",
      "Iteration 26200/28125, Loss: 8.2848\n",
      "Iteration 26300/28125, Loss: 6.4861\n",
      "Iteration 26400/28125, Loss: 10.8887\n",
      "Iteration 26500/28125, Loss: 6.5580\n",
      "Iteration 26600/28125, Loss: 8.6799\n",
      "Iteration 26700/28125, Loss: 6.8223\n",
      "Iteration 26800/28125, Loss: 7.0022\n",
      "Iteration 26900/28125, Loss: 7.6348\n",
      "Iteration 27000/28125, Loss: 8.9343\n",
      "Iteration 27100/28125, Loss: 13.5110\n",
      "Iteration 27200/28125, Loss: 12.5810\n",
      "Iteration 27300/28125, Loss: 6.9594\n",
      "Iteration 27400/28125, Loss: 7.3510\n",
      "Iteration 27500/28125, Loss: 7.4489\n",
      "Iteration 27600/28125, Loss: 6.5704\n",
      "Iteration 27700/28125, Loss: 6.5797\n",
      "Iteration 27800/28125, Loss: 6.5899\n",
      "Iteration 27900/28125, Loss: 6.6421\n",
      "Iteration 28000/28125, Loss: 6.6029\n",
      "Iteration 28100/28125, Loss: 7.7719\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 13.7499\n",
      "Iteration 200/14025, Loss: 18.6521\n",
      "Iteration 300/14025, Loss: 12.3873\n",
      "Iteration 400/14025, Loss: 16.7606\n",
      "Iteration 500/14025, Loss: 10.1438\n",
      "Iteration 600/14025, Loss: 10.9681\n",
      "Iteration 700/14025, Loss: 11.5031\n",
      "Iteration 800/14025, Loss: 11.8316\n",
      "Iteration 900/14025, Loss: 10.4001\n",
      "Iteration 1000/14025, Loss: 13.8483\n",
      "Iteration 1100/14025, Loss: 7.2713\n",
      "Iteration 1200/14025, Loss: 14.0280\n",
      "Iteration 1300/14025, Loss: 5.4841\n",
      "Iteration 1400/14025, Loss: 14.3724\n",
      "Iteration 1500/14025, Loss: 16.4727\n",
      "Iteration 1600/14025, Loss: 8.3060\n",
      "Iteration 1700/14025, Loss: 8.7080\n",
      "Iteration 1800/14025, Loss: 11.1643\n",
      "Iteration 1900/14025, Loss: 10.1301\n",
      "Iteration 2000/14025, Loss: 8.1340\n",
      "Iteration 2100/14025, Loss: 10.7572\n",
      "Iteration 2200/14025, Loss: 10.8913\n",
      "Iteration 2300/14025, Loss: 9.5436\n",
      "Iteration 2400/14025, Loss: 12.1482\n",
      "Iteration 2500/14025, Loss: 9.6938\n",
      "Iteration 2600/14025, Loss: 9.3732\n",
      "Iteration 2700/14025, Loss: 6.8802\n",
      "Iteration 2800/14025, Loss: 13.2561\n",
      "Iteration 2900/14025, Loss: 4.9065\n",
      "Iteration 3000/14025, Loss: 8.7299\n",
      "Iteration 3100/14025, Loss: 10.2713\n",
      "Iteration 3200/14025, Loss: 5.5570\n",
      "Iteration 3300/14025, Loss: 8.2233\n",
      "Iteration 3400/14025, Loss: 9.2284\n",
      "Iteration 3500/14025, Loss: 11.8254\n",
      "Iteration 3600/14025, Loss: 4.6877\n",
      "Iteration 3700/14025, Loss: 4.4432\n",
      "Iteration 3800/14025, Loss: 4.9689\n",
      "Iteration 3900/14025, Loss: 8.4388\n",
      "Iteration 4000/14025, Loss: 8.6897\n",
      "Iteration 4100/14025, Loss: 11.6751\n",
      "Iteration 4200/14025, Loss: 9.1168\n",
      "Iteration 4300/14025, Loss: 5.0330\n",
      "Iteration 4400/14025, Loss: 5.9287\n",
      "Iteration 4500/14025, Loss: 14.4742\n",
      "Iteration 4600/14025, Loss: 10.2547\n",
      "Iteration 4700/14025, Loss: 8.4536\n",
      "Iteration 4800/14025, Loss: 5.8691\n",
      "Iteration 4900/14025, Loss: 4.4640\n",
      "Iteration 5000/14025, Loss: 5.9183\n",
      "Iteration 5100/14025, Loss: 5.7290\n",
      "Iteration 5200/14025, Loss: 9.3642\n",
      "Iteration 5300/14025, Loss: 6.9557\n",
      "Iteration 5400/14025, Loss: 4.8826\n",
      "Iteration 5500/14025, Loss: 7.0543\n",
      "Iteration 5600/14025, Loss: 6.7159\n",
      "Iteration 5700/14025, Loss: 7.3840\n",
      "Iteration 5800/14025, Loss: 9.1659\n",
      "Iteration 5900/14025, Loss: 6.8810\n",
      "Iteration 6000/14025, Loss: 6.5554\n",
      "Iteration 6100/14025, Loss: 5.5365\n",
      "Iteration 6200/14025, Loss: 8.9470\n",
      "Iteration 6300/14025, Loss: 6.7617\n",
      "Iteration 6400/14025, Loss: 9.6510\n",
      "Iteration 6500/14025, Loss: 10.8312\n",
      "Iteration 6600/14025, Loss: 9.2656\n",
      "Iteration 6700/14025, Loss: 9.6931\n",
      "Iteration 6800/14025, Loss: 5.7004\n",
      "Iteration 6900/14025, Loss: 6.7295\n",
      "Iteration 7000/14025, Loss: 10.0472\n",
      "Iteration 7100/14025, Loss: 8.0115\n",
      "Iteration 7200/14025, Loss: 8.0735\n",
      "Iteration 7300/14025, Loss: 6.3575\n",
      "Iteration 7400/14025, Loss: 11.5376\n",
      "Iteration 7500/14025, Loss: 14.0882\n",
      "Iteration 7600/14025, Loss: 8.3455\n",
      "Iteration 7700/14025, Loss: 6.6801\n",
      "Iteration 7800/14025, Loss: 8.3031\n",
      "Iteration 7900/14025, Loss: 6.4463\n",
      "Iteration 8000/14025, Loss: 7.1406\n",
      "Iteration 8100/14025, Loss: 7.8797\n",
      "Iteration 8200/14025, Loss: 9.1045\n",
      "Iteration 8300/14025, Loss: 5.8448\n",
      "Iteration 8400/14025, Loss: 10.6353\n",
      "Iteration 8500/14025, Loss: 6.4431\n",
      "Iteration 8600/14025, Loss: 5.9908\n",
      "Iteration 8700/14025, Loss: 6.2060\n",
      "Iteration 8800/14025, Loss: 10.1349\n",
      "Iteration 8900/14025, Loss: 6.1831\n",
      "Iteration 9000/14025, Loss: 9.7353\n",
      "Iteration 9100/14025, Loss: 9.0957\n",
      "Iteration 9200/14025, Loss: 7.4715\n",
      "Iteration 9300/14025, Loss: 9.8772\n",
      "Iteration 9400/14025, Loss: 6.8475\n",
      "Iteration 9500/14025, Loss: 6.6741\n",
      "Iteration 9600/14025, Loss: 7.5431\n",
      "Iteration 9700/14025, Loss: 7.7531\n",
      "Iteration 9800/14025, Loss: 7.5316\n",
      "Iteration 9900/14025, Loss: 7.4643\n",
      "Iteration 10000/14025, Loss: 12.9452\n",
      "Iteration 10100/14025, Loss: 7.3695\n",
      "Iteration 10200/14025, Loss: 8.0064\n",
      "Iteration 10300/14025, Loss: 8.3039\n",
      "Iteration 10400/14025, Loss: 8.9104\n",
      "Iteration 10500/14025, Loss: 7.3207\n",
      "Iteration 10600/14025, Loss: 9.5984\n",
      "Iteration 10700/14025, Loss: 8.3800\n",
      "Iteration 10800/14025, Loss: 7.5700\n",
      "Iteration 10900/14025, Loss: 9.0213\n",
      "Iteration 11000/14025, Loss: 6.9635\n",
      "Iteration 11100/14025, Loss: 7.6524\n",
      "Iteration 11200/14025, Loss: 8.3407\n",
      "Iteration 11300/14025, Loss: 7.8018\n",
      "Iteration 11400/14025, Loss: 10.2444\n",
      "Iteration 11500/14025, Loss: 7.9923\n",
      "Iteration 11600/14025, Loss: 8.9613\n",
      "Iteration 11700/14025, Loss: 10.4075\n",
      "Iteration 11800/14025, Loss: 8.6613\n",
      "Iteration 11900/14025, Loss: 8.1075\n",
      "Iteration 12000/14025, Loss: 11.2413\n",
      "Iteration 12100/14025, Loss: 8.9590\n",
      "Iteration 12200/14025, Loss: 9.1585\n",
      "Iteration 12300/14025, Loss: 8.8094\n",
      "Iteration 12400/14025, Loss: 8.0551\n",
      "Iteration 12500/14025, Loss: 8.4016\n",
      "Iteration 12600/14025, Loss: 8.7528\n",
      "Iteration 12700/14025, Loss: 10.0985\n",
      "Iteration 12800/14025, Loss: 8.3076\n",
      "Iteration 12900/14025, Loss: 9.6115\n",
      "Iteration 13000/14025, Loss: 10.1279\n",
      "Iteration 13100/14025, Loss: 8.5628\n",
      "Iteration 13200/14025, Loss: 9.3646\n",
      "Iteration 13300/14025, Loss: 7.7845\n",
      "Iteration 13400/14025, Loss: 8.0826\n",
      "Iteration 13500/14025, Loss: 9.4684\n",
      "Iteration 13600/14025, Loss: 9.3687\n",
      "Iteration 13700/14025, Loss: 8.4700\n",
      "Iteration 13800/14025, Loss: 7.9376\n",
      "Iteration 13900/14025, Loss: 8.7618\n",
      "Iteration 14000/14025, Loss: 8.3798\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 15.5537\n",
      "Iteration 200/6975, Loss: 16.9374\n",
      "Iteration 300/6975, Loss: 11.3074\n",
      "Iteration 400/6975, Loss: 13.8049\n",
      "Iteration 500/6975, Loss: 13.3124\n",
      "Iteration 600/6975, Loss: 10.4104\n",
      "Iteration 700/6975, Loss: 16.0981\n",
      "Iteration 800/6975, Loss: 12.3747\n",
      "Iteration 900/6975, Loss: 10.0884\n",
      "Iteration 1000/6975, Loss: 10.1696\n",
      "Iteration 1100/6975, Loss: 10.8726\n",
      "Iteration 1200/6975, Loss: 9.1009\n",
      "Iteration 1300/6975, Loss: 6.4682\n",
      "Iteration 1400/6975, Loss: 7.0512\n",
      "Iteration 1500/6975, Loss: 10.6596\n",
      "Iteration 1600/6975, Loss: 8.4270\n",
      "Iteration 1700/6975, Loss: 9.0252\n",
      "Iteration 1800/6975, Loss: 5.6358\n",
      "Iteration 1900/6975, Loss: 8.9657\n",
      "Iteration 2000/6975, Loss: 10.7550\n",
      "Iteration 2100/6975, Loss: 8.4617\n",
      "Iteration 2200/6975, Loss: 5.6134\n",
      "Iteration 2300/6975, Loss: 10.8658\n",
      "Iteration 2400/6975, Loss: 10.2320\n",
      "Iteration 2500/6975, Loss: 8.7558\n",
      "Iteration 2600/6975, Loss: 10.4081\n",
      "Iteration 2700/6975, Loss: 7.1026\n",
      "Iteration 2800/6975, Loss: 10.2670\n",
      "Iteration 2900/6975, Loss: 9.4912\n",
      "Iteration 3000/6975, Loss: 10.0914\n",
      "Iteration 3100/6975, Loss: 9.9760\n",
      "Iteration 3200/6975, Loss: 9.6887\n",
      "Iteration 3300/6975, Loss: 12.1774\n",
      "Iteration 3400/6975, Loss: 9.5935\n",
      "Iteration 3500/6975, Loss: 8.0146\n",
      "Iteration 3600/6975, Loss: 10.5324\n",
      "Iteration 3700/6975, Loss: 10.6111\n",
      "Iteration 3800/6975, Loss: 10.1539\n",
      "Iteration 3900/6975, Loss: 14.7947\n",
      "Iteration 4000/6975, Loss: 11.9935\n",
      "Iteration 4100/6975, Loss: 9.7767\n",
      "Iteration 4200/6975, Loss: 9.1101\n",
      "Iteration 4300/6975, Loss: 8.6095\n",
      "Iteration 4400/6975, Loss: 12.3387\n",
      "Iteration 4500/6975, Loss: 9.0523\n",
      "Iteration 4600/6975, Loss: 10.1982\n",
      "Iteration 4700/6975, Loss: 8.6505\n",
      "Iteration 4800/6975, Loss: 11.6540\n",
      "Iteration 4900/6975, Loss: 8.5464\n",
      "Iteration 5000/6975, Loss: 10.8087\n",
      "Iteration 5100/6975, Loss: 9.3537\n",
      "Iteration 5200/6975, Loss: 10.4949\n",
      "Iteration 5300/6975, Loss: 9.5386\n",
      "Iteration 5400/6975, Loss: 11.2811\n",
      "Iteration 5500/6975, Loss: 9.7651\n",
      "Iteration 5600/6975, Loss: 9.6723\n",
      "Iteration 5700/6975, Loss: 10.4901\n",
      "Iteration 5800/6975, Loss: 10.4660\n",
      "Iteration 5900/6975, Loss: 12.4368\n",
      "Iteration 6000/6975, Loss: 10.3437\n",
      "Iteration 6100/6975, Loss: 11.4893\n",
      "Iteration 6200/6975, Loss: 10.2473\n",
      "Iteration 6300/6975, Loss: 12.1373\n",
      "Iteration 6400/6975, Loss: 11.7897\n",
      "Iteration 6500/6975, Loss: 11.3905\n",
      "Iteration 6600/6975, Loss: 9.5315\n",
      "Iteration 6700/6975, Loss: 10.5312\n",
      "Iteration 6800/6975, Loss: 10.3377\n",
      "Iteration 6900/6975, Loss: 9.5225\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 19.3352\n",
      "Iteration 200/3450, Loss: 18.2286\n",
      "Iteration 300/3450, Loss: 11.6786\n",
      "Iteration 400/3450, Loss: 11.5844\n",
      "Iteration 500/3450, Loss: 13.5138\n",
      "Iteration 600/3450, Loss: 8.7732\n",
      "Iteration 700/3450, Loss: 18.8121\n",
      "Iteration 800/3450, Loss: 11.8213\n",
      "Iteration 900/3450, Loss: 9.3869\n",
      "Iteration 1000/3450, Loss: 14.1326\n",
      "Iteration 1100/3450, Loss: 10.8102\n",
      "Iteration 1200/3450, Loss: 10.5101\n",
      "Iteration 1300/3450, Loss: 12.9400\n",
      "Iteration 1400/3450, Loss: 13.1448\n",
      "Iteration 1500/3450, Loss: 12.7329\n",
      "Iteration 1600/3450, Loss: 14.3870\n",
      "Iteration 1700/3450, Loss: 12.8882\n",
      "Iteration 1800/3450, Loss: 15.4525\n",
      "Iteration 1900/3450, Loss: 12.6951\n",
      "Iteration 2000/3450, Loss: 15.2934\n",
      "Iteration 2100/3450, Loss: 13.4962\n",
      "Iteration 2200/3450, Loss: 17.7530\n",
      "Iteration 2300/3450, Loss: 12.2062\n",
      "Iteration 2400/3450, Loss: 13.6156\n",
      "Iteration 2500/3450, Loss: 15.1799\n",
      "Iteration 2600/3450, Loss: 13.2356\n",
      "Iteration 2700/3450, Loss: 19.0368\n",
      "Iteration 2800/3450, Loss: 13.4819\n",
      "Iteration 2900/3450, Loss: 13.6572\n",
      "Iteration 3000/3450, Loss: 19.3390\n",
      "Iteration 3100/3450, Loss: 15.0024\n",
      "Iteration 3200/3450, Loss: 14.4171\n",
      "Iteration 3300/3450, Loss: 13.1387\n",
      "Iteration 3400/3450, Loss: 13.7031\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 20.8345\n",
      "Iteration 200/1725, Loss: 18.0734\n",
      "Iteration 300/1725, Loss: 20.9541\n",
      "Iteration 400/1725, Loss: 17.6173\n",
      "Iteration 500/1725, Loss: 15.5849\n",
      "Iteration 600/1725, Loss: 18.6543\n",
      "Iteration 700/1725, Loss: 18.6923\n",
      "Iteration 800/1725, Loss: 20.4987\n",
      "Iteration 900/1725, Loss: 31.2270\n",
      "Iteration 1000/1725, Loss: 19.3110\n",
      "Iteration 1100/1725, Loss: 21.2585\n",
      "Iteration 1200/1725, Loss: 22.5027\n",
      "Iteration 1300/1725, Loss: 26.8984\n",
      "Iteration 1400/1725, Loss: 21.7590\n",
      "Iteration 1500/1725, Loss: 24.8381\n",
      "Iteration 1600/1725, Loss: 23.7221\n",
      "Iteration 1700/1725, Loss: 25.8820\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 25.3584\n",
      "Iteration 200/825, Loss: 26.9691\n",
      "Iteration 300/825, Loss: 29.4704\n",
      "Iteration 400/825, Loss: 36.9000\n",
      "Iteration 500/825, Loss: 34.1857\n",
      "Iteration 600/825, Loss: 42.6396\n",
      "Iteration 700/825, Loss: 48.2782\n",
      "Iteration 800/825, Loss: 48.1552\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 38.1371\n",
      "Iteration 200/375, Loss: 54.7703\n",
      "Iteration 300/375, Loss: 69.9216\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 81.7809\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 14.4291\n",
      "Iteration 200/28125, Loss: 17.2715\n",
      "Iteration 300/28125, Loss: 21.7861\n",
      "Iteration 400/28125, Loss: 18.1135\n",
      "Iteration 500/28125, Loss: 10.9961\n",
      "Iteration 600/28125, Loss: 13.5941\n",
      "Iteration 700/28125, Loss: 6.9163\n",
      "Iteration 800/28125, Loss: 10.7207\n",
      "Iteration 900/28125, Loss: 14.3724\n",
      "Iteration 1000/28125, Loss: 13.2530\n",
      "Iteration 1100/28125, Loss: 10.2761\n",
      "Iteration 1200/28125, Loss: 12.7895\n",
      "Iteration 1300/28125, Loss: 22.8537\n",
      "Iteration 1400/28125, Loss: 13.5220\n",
      "Iteration 1500/28125, Loss: 13.2310\n",
      "Iteration 1600/28125, Loss: 9.1733\n",
      "Iteration 1700/28125, Loss: 16.5822\n",
      "Iteration 1800/28125, Loss: 11.9991\n",
      "Iteration 1900/28125, Loss: 18.8235\n",
      "Iteration 2000/28125, Loss: 18.2980\n",
      "Iteration 2100/28125, Loss: 15.9363\n",
      "Iteration 2200/28125, Loss: 12.6899\n",
      "Iteration 2300/28125, Loss: 18.5443\n",
      "Iteration 2400/28125, Loss: 13.9048\n",
      "Iteration 2500/28125, Loss: 17.7996\n",
      "Iteration 2600/28125, Loss: 14.9684\n",
      "Iteration 2700/28125, Loss: 10.4229\n",
      "Iteration 2800/28125, Loss: 21.3712\n",
      "Iteration 2900/28125, Loss: 15.4388\n",
      "Iteration 3000/28125, Loss: 15.5170\n",
      "Iteration 3100/28125, Loss: 20.2849\n",
      "Iteration 3200/28125, Loss: 13.7251\n",
      "Iteration 3300/28125, Loss: 15.6509\n",
      "Iteration 3400/28125, Loss: 13.2736\n",
      "Iteration 3500/28125, Loss: 16.8132\n",
      "Iteration 3600/28125, Loss: 26.6164\n",
      "Iteration 3700/28125, Loss: 17.8245\n",
      "Iteration 3800/28125, Loss: 10.5370\n",
      "Iteration 3900/28125, Loss: 14.4659\n",
      "Iteration 4000/28125, Loss: 25.0318\n",
      "Iteration 4100/28125, Loss: 11.6350\n",
      "Iteration 4200/28125, Loss: 23.1838\n",
      "Iteration 4300/28125, Loss: 15.7283\n",
      "Iteration 4400/28125, Loss: 16.9261\n",
      "Iteration 4500/28125, Loss: 20.9527\n",
      "Iteration 4600/28125, Loss: 16.1976\n",
      "Iteration 4700/28125, Loss: 14.8239\n",
      "Iteration 4800/28125, Loss: 19.7553\n",
      "Iteration 4900/28125, Loss: 20.3792\n",
      "Iteration 5000/28125, Loss: 26.0192\n",
      "Iteration 5100/28125, Loss: 22.7765\n",
      "Iteration 5200/28125, Loss: 17.3316\n",
      "Iteration 5300/28125, Loss: 25.7144\n",
      "Iteration 5400/28125, Loss: 15.5428\n",
      "Iteration 5500/28125, Loss: 23.0194\n",
      "Iteration 5600/28125, Loss: 20.6189\n",
      "Iteration 5700/28125, Loss: 25.9982\n",
      "Iteration 5800/28125, Loss: 17.6589\n",
      "Iteration 5900/28125, Loss: 14.2475\n",
      "Iteration 6000/28125, Loss: 16.9633\n",
      "Iteration 6100/28125, Loss: 14.6070\n",
      "Iteration 6200/28125, Loss: 19.0385\n",
      "Iteration 6300/28125, Loss: 20.1098\n",
      "Iteration 6400/28125, Loss: 17.8973\n",
      "Iteration 6500/28125, Loss: 18.6837\n",
      "Iteration 6600/28125, Loss: 19.2623\n",
      "Iteration 6700/28125, Loss: 21.9556\n",
      "Iteration 6800/28125, Loss: 26.4342\n",
      "Iteration 6900/28125, Loss: 18.1705\n",
      "Iteration 7000/28125, Loss: 21.2729\n",
      "Iteration 7100/28125, Loss: 17.5167\n",
      "Iteration 7200/28125, Loss: 18.0163\n",
      "Iteration 7300/28125, Loss: 25.2966\n",
      "Iteration 7400/28125, Loss: 26.7039\n",
      "Iteration 7500/28125, Loss: 18.6299\n",
      "Iteration 7600/28125, Loss: 23.1910\n",
      "Iteration 7700/28125, Loss: 26.7380\n",
      "Iteration 7800/28125, Loss: 19.5109\n",
      "Iteration 7900/28125, Loss: 26.8164\n",
      "Iteration 8000/28125, Loss: 20.6075\n",
      "Iteration 8100/28125, Loss: 19.5211\n",
      "Iteration 8200/28125, Loss: 26.4063\n",
      "Iteration 8300/28125, Loss: 21.6863\n",
      "Iteration 8400/28125, Loss: 24.6319\n",
      "Iteration 8500/28125, Loss: 21.0953\n",
      "Iteration 8600/28125, Loss: 25.9241\n",
      "Iteration 8700/28125, Loss: 18.0149\n",
      "Iteration 8800/28125, Loss: 20.5848\n",
      "Iteration 8900/28125, Loss: 28.2647\n",
      "Iteration 9000/28125, Loss: 28.1399\n",
      "Iteration 9100/28125, Loss: 25.8200\n",
      "Iteration 9200/28125, Loss: 18.4144\n",
      "Iteration 9300/28125, Loss: 19.6404\n",
      "Iteration 9400/28125, Loss: 22.2204\n",
      "Iteration 9500/28125, Loss: 21.4419\n",
      "Iteration 9600/28125, Loss: 18.5447\n",
      "Iteration 9700/28125, Loss: 29.7661\n",
      "Iteration 9800/28125, Loss: 27.0803\n",
      "Iteration 9900/28125, Loss: 25.6742\n",
      "Iteration 10000/28125, Loss: 19.4888\n",
      "Iteration 10100/28125, Loss: 28.9029\n",
      "Iteration 10200/28125, Loss: 25.0309\n",
      "Iteration 10300/28125, Loss: 24.9244\n",
      "Iteration 10400/28125, Loss: 25.2354\n",
      "Iteration 10500/28125, Loss: 20.3240\n",
      "Iteration 10600/28125, Loss: 24.5116\n",
      "Iteration 10700/28125, Loss: 31.3778\n",
      "Iteration 10800/28125, Loss: 30.6062\n",
      "Iteration 10900/28125, Loss: 26.3683\n",
      "Iteration 11000/28125, Loss: 24.3503\n",
      "Iteration 11100/28125, Loss: 25.2946\n",
      "Iteration 11200/28125, Loss: 20.4789\n",
      "Iteration 11300/28125, Loss: 23.8157\n",
      "Iteration 11400/28125, Loss: 23.3601\n",
      "Iteration 11500/28125, Loss: 24.0504\n",
      "Iteration 11600/28125, Loss: 26.8603\n",
      "Iteration 11700/28125, Loss: 27.4703\n",
      "Iteration 11800/28125, Loss: 21.1976\n",
      "Iteration 11900/28125, Loss: 23.6632\n",
      "Iteration 12000/28125, Loss: 29.6538\n",
      "Iteration 12100/28125, Loss: 31.1057\n",
      "Iteration 12200/28125, Loss: 25.5548\n",
      "Iteration 12300/28125, Loss: 19.7505\n",
      "Iteration 12400/28125, Loss: 36.0068\n",
      "Iteration 12500/28125, Loss: 22.3927\n",
      "Iteration 12600/28125, Loss: 24.5143\n",
      "Iteration 12700/28125, Loss: 28.5119\n",
      "Iteration 12800/28125, Loss: 24.2567\n",
      "Iteration 12900/28125, Loss: 24.9320\n",
      "Iteration 13000/28125, Loss: 29.1703\n",
      "Iteration 13100/28125, Loss: 20.1842\n",
      "Iteration 13200/28125, Loss: 29.5476\n",
      "Iteration 13300/28125, Loss: 22.3044\n",
      "Iteration 13400/28125, Loss: 27.0984\n",
      "Iteration 13500/28125, Loss: 23.5736\n",
      "Iteration 13600/28125, Loss: 24.8018\n",
      "Iteration 13700/28125, Loss: 21.2930\n",
      "Iteration 13800/28125, Loss: 21.3053\n",
      "Iteration 13900/28125, Loss: 21.8379\n",
      "Iteration 14000/28125, Loss: 23.2690\n",
      "Iteration 14100/28125, Loss: 20.3793\n",
      "Iteration 14200/28125, Loss: 20.3804\n",
      "Iteration 14300/28125, Loss: 20.7781\n",
      "Iteration 14400/28125, Loss: 30.6235\n",
      "Iteration 14500/28125, Loss: 29.2960\n",
      "Iteration 14600/28125, Loss: 21.4392\n",
      "Iteration 14700/28125, Loss: 24.9126\n",
      "Iteration 14800/28125, Loss: 27.3028\n",
      "Iteration 14900/28125, Loss: 23.2326\n",
      "Iteration 15000/28125, Loss: 31.2168\n",
      "Iteration 15100/28125, Loss: 27.6893\n",
      "Iteration 15200/28125, Loss: 22.5886\n",
      "Iteration 15300/28125, Loss: 28.5456\n",
      "Iteration 15400/28125, Loss: 31.2050\n",
      "Iteration 15500/28125, Loss: 22.6727\n",
      "Iteration 15600/28125, Loss: 26.3796\n",
      "Iteration 15700/28125, Loss: 26.1935\n",
      "Iteration 15800/28125, Loss: 26.5616\n",
      "Iteration 15900/28125, Loss: 26.0145\n",
      "Iteration 16000/28125, Loss: 28.0411\n",
      "Iteration 16100/28125, Loss: 26.4379\n",
      "Iteration 16200/28125, Loss: 33.3333\n",
      "Iteration 16300/28125, Loss: 22.6434\n",
      "Iteration 16400/28125, Loss: 28.1924\n",
      "Iteration 16500/28125, Loss: 23.7968\n",
      "Iteration 16600/28125, Loss: 22.9070\n",
      "Iteration 16700/28125, Loss: 22.9709\n",
      "Iteration 16800/28125, Loss: 26.4428\n",
      "Iteration 16900/28125, Loss: 27.0112\n",
      "Iteration 17000/28125, Loss: 26.3802\n",
      "Iteration 17100/28125, Loss: 22.3571\n",
      "Iteration 17200/28125, Loss: 23.2338\n",
      "Iteration 17300/28125, Loss: 22.4409\n",
      "Iteration 17400/28125, Loss: 23.2910\n",
      "Iteration 17500/28125, Loss: 25.5080\n",
      "Iteration 17600/28125, Loss: 25.0121\n",
      "Iteration 17700/28125, Loss: 28.3119\n",
      "Iteration 17800/28125, Loss: 27.7235\n",
      "Iteration 17900/28125, Loss: 29.8770\n",
      "Iteration 18000/28125, Loss: 26.2532\n",
      "Iteration 18100/28125, Loss: 23.8755\n",
      "Iteration 18200/28125, Loss: 24.7166\n",
      "Iteration 18300/28125, Loss: 32.5215\n",
      "Iteration 18400/28125, Loss: 36.0898\n",
      "Iteration 18500/28125, Loss: 29.6516\n",
      "Iteration 18600/28125, Loss: 30.5901\n",
      "Iteration 18700/28125, Loss: 25.1411\n",
      "Iteration 18800/28125, Loss: 23.4934\n",
      "Iteration 18900/28125, Loss: 25.6007\n",
      "Iteration 19000/28125, Loss: 28.3984\n",
      "Iteration 19100/28125, Loss: 27.0957\n",
      "Iteration 19200/28125, Loss: 25.8070\n",
      "Iteration 19300/28125, Loss: 24.3578\n",
      "Iteration 19400/28125, Loss: 33.7373\n",
      "Iteration 19500/28125, Loss: 27.9948\n",
      "Iteration 19600/28125, Loss: 25.2877\n",
      "Iteration 19700/28125, Loss: 24.8378\n",
      "Iteration 19800/28125, Loss: 26.5617\n",
      "Iteration 19900/28125, Loss: 27.2165\n",
      "Iteration 20000/28125, Loss: 29.2463\n",
      "Iteration 20100/28125, Loss: 24.9142\n",
      "Iteration 20200/28125, Loss: 28.3472\n",
      "Iteration 20300/28125, Loss: 21.6520\n",
      "Iteration 20400/28125, Loss: 25.9945\n",
      "Iteration 20500/28125, Loss: 27.2044\n",
      "Iteration 20600/28125, Loss: 25.4227\n",
      "Iteration 20700/28125, Loss: 28.9287\n",
      "Iteration 20800/28125, Loss: 23.9578\n",
      "Iteration 20900/28125, Loss: 30.8122\n",
      "Iteration 21000/28125, Loss: 24.8352\n",
      "Iteration 21100/28125, Loss: 25.7129\n",
      "Iteration 21200/28125, Loss: 26.8048\n",
      "Iteration 21300/28125, Loss: 27.3697\n",
      "Iteration 21400/28125, Loss: 27.4186\n",
      "Iteration 21500/28125, Loss: 26.2182\n",
      "Iteration 21600/28125, Loss: 32.4641\n",
      "Iteration 21700/28125, Loss: 25.1285\n",
      "Iteration 21800/28125, Loss: 23.0914\n",
      "Iteration 21900/28125, Loss: 23.1577\n",
      "Iteration 22000/28125, Loss: 21.8064\n",
      "Iteration 22100/28125, Loss: 23.7508\n",
      "Iteration 22200/28125, Loss: 22.4262\n",
      "Iteration 22300/28125, Loss: 23.9995\n",
      "Iteration 22400/28125, Loss: 25.0239\n",
      "Iteration 22500/28125, Loss: 32.4056\n",
      "Iteration 22600/28125, Loss: 25.9967\n",
      "Iteration 22700/28125, Loss: 27.0918\n",
      "Iteration 22800/28125, Loss: 29.8615\n",
      "Iteration 22900/28125, Loss: 30.1741\n",
      "Iteration 23000/28125, Loss: 27.5373\n",
      "Iteration 23100/28125, Loss: 30.8547\n",
      "Iteration 23200/28125, Loss: 25.9298\n",
      "Iteration 23300/28125, Loss: 25.8637\n",
      "Iteration 23400/28125, Loss: 25.5040\n",
      "Iteration 23500/28125, Loss: 25.3812\n",
      "Iteration 23600/28125, Loss: 27.3532\n",
      "Iteration 23700/28125, Loss: 32.7352\n",
      "Iteration 23800/28125, Loss: 27.3981\n",
      "Iteration 23900/28125, Loss: 29.5052\n",
      "Iteration 24000/28125, Loss: 28.9211\n",
      "Iteration 24100/28125, Loss: 25.3386\n",
      "Iteration 24200/28125, Loss: 24.4259\n",
      "Iteration 24300/28125, Loss: 28.1764\n",
      "Iteration 24400/28125, Loss: 24.4498\n",
      "Iteration 24500/28125, Loss: 30.4427\n",
      "Iteration 24600/28125, Loss: 27.1373\n",
      "Iteration 24700/28125, Loss: 21.9976\n",
      "Iteration 24800/28125, Loss: 31.9830\n",
      "Iteration 24900/28125, Loss: 25.9081\n",
      "Iteration 25000/28125, Loss: 23.6665\n",
      "Iteration 25100/28125, Loss: 30.2363\n",
      "Iteration 25200/28125, Loss: 27.6564\n",
      "Iteration 25300/28125, Loss: 21.7607\n",
      "Iteration 25400/28125, Loss: 29.1273\n",
      "Iteration 25500/28125, Loss: 26.2949\n",
      "Iteration 25600/28125, Loss: 30.2324\n",
      "Iteration 25700/28125, Loss: 26.4940\n",
      "Iteration 25800/28125, Loss: 25.6207\n",
      "Iteration 25900/28125, Loss: 32.5044\n",
      "Iteration 26000/28125, Loss: 28.0413\n",
      "Iteration 26100/28125, Loss: 38.8452\n",
      "Iteration 26200/28125, Loss: 25.4694\n",
      "Iteration 26300/28125, Loss: 28.0125\n",
      "Iteration 26400/28125, Loss: 29.2715\n",
      "Iteration 26500/28125, Loss: 22.0384\n",
      "Iteration 26600/28125, Loss: 26.3048\n",
      "Iteration 26700/28125, Loss: 31.5805\n",
      "Iteration 26800/28125, Loss: 33.7577\n",
      "Iteration 26900/28125, Loss: 26.1543\n",
      "Iteration 27000/28125, Loss: 30.7125\n",
      "Iteration 27100/28125, Loss: 26.6817\n",
      "Iteration 27200/28125, Loss: 31.3159\n",
      "Iteration 27300/28125, Loss: 28.0052\n",
      "Iteration 27400/28125, Loss: 26.5499\n",
      "Iteration 27500/28125, Loss: 24.1648\n",
      "Iteration 27600/28125, Loss: 23.4833\n",
      "Iteration 27700/28125, Loss: 22.6956\n",
      "Iteration 27800/28125, Loss: 31.5170\n",
      "Iteration 27900/28125, Loss: 25.3762\n",
      "Iteration 28000/28125, Loss: 21.8014\n",
      "Iteration 28100/28125, Loss: 28.9577\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 13.2566\n",
      "Iteration 200/14025, Loss: 16.6702\n",
      "Iteration 300/14025, Loss: 12.0335\n",
      "Iteration 400/14025, Loss: 16.9452\n",
      "Iteration 500/14025, Loss: 15.7708\n",
      "Iteration 600/14025, Loss: 15.3367\n",
      "Iteration 700/14025, Loss: 12.0383\n",
      "Iteration 800/14025, Loss: 14.7148\n",
      "Iteration 900/14025, Loss: 14.5062\n",
      "Iteration 1000/14025, Loss: 19.2722\n",
      "Iteration 1100/14025, Loss: 14.8299\n",
      "Iteration 1200/14025, Loss: 17.5486\n",
      "Iteration 1300/14025, Loss: 15.9907\n",
      "Iteration 1400/14025, Loss: 22.4147\n",
      "Iteration 1500/14025, Loss: 23.7927\n",
      "Iteration 1600/14025, Loss: 22.9469\n",
      "Iteration 1700/14025, Loss: 20.6911\n",
      "Iteration 1800/14025, Loss: 22.6006\n",
      "Iteration 1900/14025, Loss: 20.7518\n",
      "Iteration 2000/14025, Loss: 20.5770\n",
      "Iteration 2100/14025, Loss: 20.2248\n",
      "Iteration 2200/14025, Loss: 28.9028\n",
      "Iteration 2300/14025, Loss: 22.3863\n",
      "Iteration 2400/14025, Loss: 24.0990\n",
      "Iteration 2500/14025, Loss: 25.3969\n",
      "Iteration 2600/14025, Loss: 25.5639\n",
      "Iteration 2700/14025, Loss: 24.7307\n",
      "Iteration 2800/14025, Loss: 24.0340\n",
      "Iteration 2900/14025, Loss: 23.0332\n",
      "Iteration 3000/14025, Loss: 22.8279\n",
      "Iteration 3100/14025, Loss: 29.9311\n",
      "Iteration 3200/14025, Loss: 29.6294\n",
      "Iteration 3300/14025, Loss: 25.0257\n",
      "Iteration 3400/14025, Loss: 28.3193\n",
      "Iteration 3500/14025, Loss: 25.2767\n",
      "Iteration 3600/14025, Loss: 26.1509\n",
      "Iteration 3700/14025, Loss: 26.1338\n",
      "Iteration 3800/14025, Loss: 24.4049\n",
      "Iteration 3900/14025, Loss: 29.7298\n",
      "Iteration 4000/14025, Loss: 32.7115\n",
      "Iteration 4100/14025, Loss: 31.2317\n",
      "Iteration 4200/14025, Loss: 30.8750\n",
      "Iteration 4300/14025, Loss: 30.1248\n",
      "Iteration 4400/14025, Loss: 30.2871\n",
      "Iteration 4500/14025, Loss: 36.8193\n",
      "Iteration 4600/14025, Loss: 29.8382\n",
      "Iteration 4700/14025, Loss: 31.3828\n",
      "Iteration 4800/14025, Loss: 28.7921\n",
      "Iteration 4900/14025, Loss: 32.1156\n",
      "Iteration 5000/14025, Loss: 29.4167\n",
      "Iteration 5100/14025, Loss: 33.5082\n",
      "Iteration 5200/14025, Loss: 34.7602\n",
      "Iteration 5300/14025, Loss: 35.6991\n",
      "Iteration 5400/14025, Loss: 30.7271\n",
      "Iteration 5500/14025, Loss: 36.3521\n",
      "Iteration 5600/14025, Loss: 36.1468\n",
      "Iteration 5700/14025, Loss: 36.7342\n",
      "Iteration 5800/14025, Loss: 34.6902\n",
      "Iteration 5900/14025, Loss: 30.7119\n",
      "Iteration 6000/14025, Loss: 35.6246\n",
      "Iteration 6100/14025, Loss: 32.6090\n",
      "Iteration 6200/14025, Loss: 41.1112\n",
      "Iteration 6300/14025, Loss: 37.8593\n",
      "Iteration 6400/14025, Loss: 34.5460\n",
      "Iteration 6500/14025, Loss: 36.1749\n",
      "Iteration 6600/14025, Loss: 37.6547\n",
      "Iteration 6700/14025, Loss: 35.4700\n",
      "Iteration 6800/14025, Loss: 38.0747\n",
      "Iteration 6900/14025, Loss: 35.0019\n",
      "Iteration 7000/14025, Loss: 38.8906\n",
      "Iteration 7100/14025, Loss: 33.7118\n",
      "Iteration 7200/14025, Loss: 38.3522\n",
      "Iteration 7300/14025, Loss: 35.4801\n",
      "Iteration 7400/14025, Loss: 40.1487\n",
      "Iteration 7500/14025, Loss: 43.6576\n",
      "Iteration 7600/14025, Loss: 36.1334\n",
      "Iteration 7700/14025, Loss: 38.7723\n",
      "Iteration 7800/14025, Loss: 39.5970\n",
      "Iteration 7900/14025, Loss: 38.9218\n",
      "Iteration 8000/14025, Loss: 35.8122\n",
      "Iteration 8100/14025, Loss: 39.7726\n",
      "Iteration 8200/14025, Loss: 40.0401\n",
      "Iteration 8300/14025, Loss: 40.9094\n",
      "Iteration 8400/14025, Loss: 40.0878\n",
      "Iteration 8500/14025, Loss: 38.8236\n",
      "Iteration 8600/14025, Loss: 34.9644\n",
      "Iteration 8700/14025, Loss: 37.2106\n",
      "Iteration 8800/14025, Loss: 40.2676\n",
      "Iteration 8900/14025, Loss: 36.4052\n",
      "Iteration 9000/14025, Loss: 43.2644\n",
      "Iteration 9100/14025, Loss: 38.0062\n",
      "Iteration 9200/14025, Loss: 37.3220\n",
      "Iteration 9300/14025, Loss: 36.5938\n",
      "Iteration 9400/14025, Loss: 37.3205\n",
      "Iteration 9500/14025, Loss: 40.0800\n",
      "Iteration 9600/14025, Loss: 38.8781\n",
      "Iteration 9700/14025, Loss: 37.2448\n",
      "Iteration 9800/14025, Loss: 40.7386\n",
      "Iteration 9900/14025, Loss: 40.6024\n",
      "Iteration 10000/14025, Loss: 41.6839\n",
      "Iteration 10100/14025, Loss: 39.6072\n",
      "Iteration 10200/14025, Loss: 40.2362\n",
      "Iteration 10300/14025, Loss: 39.2612\n",
      "Iteration 10400/14025, Loss: 37.9025\n",
      "Iteration 10500/14025, Loss: 40.9652\n",
      "Iteration 10600/14025, Loss: 37.9166\n",
      "Iteration 10700/14025, Loss: 38.4029\n",
      "Iteration 10800/14025, Loss: 41.7215\n",
      "Iteration 10900/14025, Loss: 42.4077\n",
      "Iteration 11000/14025, Loss: 38.6560\n",
      "Iteration 11100/14025, Loss: 38.2707\n",
      "Iteration 11200/14025, Loss: 42.4602\n",
      "Iteration 11300/14025, Loss: 42.8660\n",
      "Iteration 11400/14025, Loss: 40.8599\n",
      "Iteration 11500/14025, Loss: 39.2919\n",
      "Iteration 11600/14025, Loss: 39.6131\n",
      "Iteration 11700/14025, Loss: 39.2016\n",
      "Iteration 11800/14025, Loss: 42.8416\n",
      "Iteration 11900/14025, Loss: 40.4067\n",
      "Iteration 12000/14025, Loss: 46.5377\n",
      "Iteration 12100/14025, Loss: 41.9880\n",
      "Iteration 12200/14025, Loss: 46.5426\n",
      "Iteration 12300/14025, Loss: 45.6727\n",
      "Iteration 12400/14025, Loss: 39.9485\n",
      "Iteration 12500/14025, Loss: 40.2542\n",
      "Iteration 12600/14025, Loss: 43.5777\n",
      "Iteration 12700/14025, Loss: 44.2359\n",
      "Iteration 12800/14025, Loss: 45.9278\n",
      "Iteration 12900/14025, Loss: 43.7736\n",
      "Iteration 13000/14025, Loss: 43.8532\n",
      "Iteration 13100/14025, Loss: 39.9860\n",
      "Iteration 13200/14025, Loss: 43.1549\n",
      "Iteration 13300/14025, Loss: 43.0329\n",
      "Iteration 13400/14025, Loss: 43.4026\n",
      "Iteration 13500/14025, Loss: 40.2255\n",
      "Iteration 13600/14025, Loss: 41.0569\n",
      "Iteration 13700/14025, Loss: 40.9586\n",
      "Iteration 13800/14025, Loss: 40.3648\n",
      "Iteration 13900/14025, Loss: 40.8475\n",
      "Iteration 14000/14025, Loss: 41.5547\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 15.1503\n",
      "Iteration 200/6975, Loss: 17.1040\n",
      "Iteration 300/6975, Loss: 13.4682\n",
      "Iteration 400/6975, Loss: 18.1076\n",
      "Iteration 500/6975, Loss: 18.6630\n",
      "Iteration 600/6975, Loss: 21.1196\n",
      "Iteration 700/6975, Loss: 24.6768\n",
      "Iteration 800/6975, Loss: 26.1412\n",
      "Iteration 900/6975, Loss: 25.6123\n",
      "Iteration 1000/6975, Loss: 26.4655\n",
      "Iteration 1100/6975, Loss: 28.5081\n",
      "Iteration 1200/6975, Loss: 23.6825\n",
      "Iteration 1300/6975, Loss: 28.4968\n",
      "Iteration 1400/6975, Loss: 32.0171\n",
      "Iteration 1500/6975, Loss: 31.4475\n",
      "Iteration 1600/6975, Loss: 29.6691\n",
      "Iteration 1700/6975, Loss: 35.3979\n",
      "Iteration 1800/6975, Loss: 34.2883\n",
      "Iteration 1900/6975, Loss: 35.8488\n",
      "Iteration 2000/6975, Loss: 34.9348\n",
      "Iteration 2100/6975, Loss: 38.5103\n",
      "Iteration 2200/6975, Loss: 37.0513\n",
      "Iteration 2300/6975, Loss: 42.2476\n",
      "Iteration 2400/6975, Loss: 40.6689\n",
      "Iteration 2500/6975, Loss: 40.8270\n",
      "Iteration 2600/6975, Loss: 43.8881\n",
      "Iteration 2700/6975, Loss: 44.5091\n",
      "Iteration 2800/6975, Loss: 43.2353\n",
      "Iteration 2900/6975, Loss: 45.6616\n",
      "Iteration 3000/6975, Loss: 48.7472\n",
      "Iteration 3100/6975, Loss: 48.8830\n",
      "Iteration 3200/6975, Loss: 49.9030\n",
      "Iteration 3300/6975, Loss: 49.7173\n",
      "Iteration 3400/6975, Loss: 52.2662\n",
      "Iteration 3500/6975, Loss: 49.4366\n",
      "Iteration 3600/6975, Loss: 54.1146\n",
      "Iteration 3700/6975, Loss: 55.2818\n",
      "Iteration 3800/6975, Loss: 56.4602\n",
      "Iteration 3900/6975, Loss: 57.2819\n",
      "Iteration 4000/6975, Loss: 56.3887\n",
      "Iteration 4100/6975, Loss: 55.2510\n",
      "Iteration 4200/6975, Loss: 53.7177\n",
      "Iteration 4300/6975, Loss: 53.6650\n",
      "Iteration 4400/6975, Loss: 61.4482\n",
      "Iteration 4500/6975, Loss: 57.7333\n",
      "Iteration 4600/6975, Loss: 60.4022\n",
      "Iteration 4700/6975, Loss: 57.5139\n",
      "Iteration 4800/6975, Loss: 61.5143\n",
      "Iteration 4900/6975, Loss: 58.7666\n",
      "Iteration 5000/6975, Loss: 63.4760\n",
      "Iteration 5100/6975, Loss: 62.0426\n",
      "Iteration 5200/6975, Loss: 61.1951\n",
      "Iteration 5300/6975, Loss: 61.9429\n",
      "Iteration 5400/6975, Loss: 62.5918\n",
      "Iteration 5500/6975, Loss: 61.4328\n",
      "Iteration 5600/6975, Loss: 62.6372\n",
      "Iteration 5700/6975, Loss: 63.3143\n",
      "Iteration 5800/6975, Loss: 62.1433\n",
      "Iteration 5900/6975, Loss: 64.8442\n",
      "Iteration 6000/6975, Loss: 65.1034\n",
      "Iteration 6100/6975, Loss: 64.0767\n",
      "Iteration 6200/6975, Loss: 65.5949\n",
      "Iteration 6300/6975, Loss: 65.6580\n",
      "Iteration 6400/6975, Loss: 65.7238\n",
      "Iteration 6500/6975, Loss: 64.3290\n",
      "Iteration 6600/6975, Loss: 64.3597\n",
      "Iteration 6700/6975, Loss: 64.1328\n",
      "Iteration 6800/6975, Loss: 65.4081\n",
      "Iteration 6900/6975, Loss: 64.3433\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 24.3949\n",
      "Iteration 200/3450, Loss: 25.3123\n",
      "Iteration 300/3450, Loss: 21.5943\n",
      "Iteration 400/3450, Loss: 29.4640\n",
      "Iteration 500/3450, Loss: 33.9456\n",
      "Iteration 600/3450, Loss: 34.6143\n",
      "Iteration 700/3450, Loss: 44.0867\n",
      "Iteration 800/3450, Loss: 42.4642\n",
      "Iteration 900/3450, Loss: 44.1039\n",
      "Iteration 1000/3450, Loss: 50.8813\n",
      "Iteration 1100/3450, Loss: 51.7220\n",
      "Iteration 1200/3450, Loss: 56.8553\n",
      "Iteration 1300/3450, Loss: 57.8339\n",
      "Iteration 1400/3450, Loss: 65.7199\n",
      "Iteration 1500/3450, Loss: 68.1115\n",
      "Iteration 1600/3450, Loss: 72.1205\n",
      "Iteration 1700/3450, Loss: 76.2954\n",
      "Iteration 1800/3450, Loss: 79.3663\n",
      "Iteration 1900/3450, Loss: 78.3741\n",
      "Iteration 2000/3450, Loss: 80.7779\n",
      "Iteration 2100/3450, Loss: 83.5529\n",
      "Iteration 2200/3450, Loss: 94.9879\n",
      "Iteration 2300/3450, Loss: 91.6023\n",
      "Iteration 2400/3450, Loss: 92.4270\n",
      "Iteration 2500/3450, Loss: 95.8389\n",
      "Iteration 2600/3450, Loss: 94.4797\n",
      "Iteration 2700/3450, Loss: 98.4343\n",
      "Iteration 2800/3450, Loss: 99.7105\n",
      "Iteration 2900/3450, Loss: 101.9400\n",
      "Iteration 3000/3450, Loss: 104.8140\n",
      "Iteration 3100/3450, Loss: 104.3567\n",
      "Iteration 3200/3450, Loss: 106.5910\n",
      "Iteration 3300/3450, Loss: 107.0516\n",
      "Iteration 3400/3450, Loss: 106.7322\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 33.8132\n",
      "Iteration 200/1725, Loss: 41.9121\n",
      "Iteration 300/1725, Loss: 56.0481\n",
      "Iteration 400/1725, Loss: 66.5425\n",
      "Iteration 500/1725, Loss: 78.7488\n",
      "Iteration 600/1725, Loss: 95.2886\n",
      "Iteration 700/1725, Loss: 104.4128\n",
      "Iteration 800/1725, Loss: 116.9690\n",
      "Iteration 900/1725, Loss: 135.2130\n",
      "Iteration 1000/1725, Loss: 138.7344\n",
      "Iteration 1100/1725, Loss: 152.3280\n",
      "Iteration 1200/1725, Loss: 163.7035\n",
      "Iteration 1300/1725, Loss: 173.1375\n",
      "Iteration 1400/1725, Loss: 178.6965\n",
      "Iteration 1500/1725, Loss: 189.2972\n",
      "Iteration 1600/1725, Loss: 192.7374\n",
      "Iteration 1700/1725, Loss: 199.1346\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 66.5488\n",
      "Iteration 200/825, Loss: 114.6413\n",
      "Iteration 300/825, Loss: 157.7159\n",
      "Iteration 400/825, Loss: 213.3910\n",
      "Iteration 500/825, Loss: 254.7483\n",
      "Iteration 600/825, Loss: 302.8144\n",
      "Iteration 700/825, Loss: 345.6271\n",
      "Iteration 800/825, Loss: 388.9978\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 188.6324\n",
      "Iteration 200/375, Loss: 370.5955\n",
      "Iteration 300/375, Loss: 559.1727\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 626.7957\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 14.4613\n",
      "Iteration 200/28125, Loss: 26.4839\n",
      "Iteration 300/28125, Loss: 33.1511\n",
      "Iteration 400/28125, Loss: 33.0073\n",
      "Iteration 500/28125, Loss: 24.5454\n",
      "Iteration 600/28125, Loss: 28.0481\n",
      "Iteration 700/28125, Loss: 22.8414\n",
      "Iteration 800/28125, Loss: 31.0873\n",
      "Iteration 900/28125, Loss: 36.6719\n",
      "Iteration 1000/28125, Loss: 29.8685\n",
      "Iteration 1100/28125, Loss: 25.9794\n",
      "Iteration 1200/28125, Loss: 25.0015\n",
      "Iteration 1300/28125, Loss: 45.9618\n",
      "Iteration 1400/28125, Loss: 31.9603\n",
      "Iteration 1500/28125, Loss: 31.9578\n",
      "Iteration 1600/28125, Loss: 28.7247\n",
      "Iteration 1700/28125, Loss: 30.1095\n",
      "Iteration 1800/28125, Loss: 34.5787\n",
      "Iteration 1900/28125, Loss: 31.5827\n",
      "Iteration 2000/28125, Loss: 36.0554\n",
      "Iteration 2100/28125, Loss: 32.4594\n",
      "Iteration 2200/28125, Loss: 30.7668\n",
      "Iteration 2300/28125, Loss: 30.6497\n",
      "Iteration 2400/28125, Loss: 40.8713\n",
      "Iteration 2500/28125, Loss: 34.9119\n",
      "Iteration 2600/28125, Loss: 34.1571\n",
      "Iteration 2700/28125, Loss: 29.4161\n",
      "Iteration 2800/28125, Loss: 39.0311\n",
      "Iteration 2900/28125, Loss: 37.2153\n",
      "Iteration 3000/28125, Loss: 40.4693\n",
      "Iteration 3100/28125, Loss: 46.0389\n",
      "Iteration 3200/28125, Loss: 27.7026\n",
      "Iteration 3300/28125, Loss: 28.3380\n",
      "Iteration 3400/28125, Loss: 35.9184\n",
      "Iteration 3500/28125, Loss: 35.9443\n",
      "Iteration 3600/28125, Loss: 37.9617\n",
      "Iteration 3700/28125, Loss: 34.6619\n",
      "Iteration 3800/28125, Loss: 26.1269\n",
      "Iteration 3900/28125, Loss: 33.8527\n",
      "Iteration 4000/28125, Loss: 37.5774\n",
      "Iteration 4100/28125, Loss: 29.2328\n",
      "Iteration 4200/28125, Loss: 39.1631\n",
      "Iteration 4300/28125, Loss: 35.7308\n",
      "Iteration 4400/28125, Loss: 37.4054\n",
      "Iteration 4500/28125, Loss: 36.3563\n",
      "Iteration 4600/28125, Loss: 31.2003\n",
      "Iteration 4700/28125, Loss: 33.2499\n",
      "Iteration 4800/28125, Loss: 34.1546\n",
      "Iteration 4900/28125, Loss: 37.1695\n",
      "Iteration 5000/28125, Loss: 43.6240\n",
      "Iteration 5100/28125, Loss: 38.3475\n",
      "Iteration 5200/28125, Loss: 42.0795\n",
      "Iteration 5300/28125, Loss: 33.2843\n",
      "Iteration 5400/28125, Loss: 38.4333\n",
      "Iteration 5500/28125, Loss: 40.3366\n",
      "Iteration 5600/28125, Loss: 36.9708\n",
      "Iteration 5700/28125, Loss: 40.5650\n",
      "Iteration 5800/28125, Loss: 38.8695\n",
      "Iteration 5900/28125, Loss: 30.7320\n",
      "Iteration 6000/28125, Loss: 37.8817\n",
      "Iteration 6100/28125, Loss: 30.0538\n",
      "Iteration 6200/28125, Loss: 44.0643\n",
      "Iteration 6300/28125, Loss: 34.8266\n",
      "Iteration 6400/28125, Loss: 40.5271\n",
      "Iteration 6500/28125, Loss: 32.2644\n",
      "Iteration 6600/28125, Loss: 28.9373\n",
      "Iteration 6700/28125, Loss: 31.0354\n",
      "Iteration 6800/28125, Loss: 43.7440\n",
      "Iteration 6900/28125, Loss: 39.9438\n",
      "Iteration 7000/28125, Loss: 38.5089\n",
      "Iteration 7100/28125, Loss: 36.5805\n",
      "Iteration 7200/28125, Loss: 44.0134\n",
      "Iteration 7300/28125, Loss: 42.9619\n",
      "Iteration 7400/28125, Loss: 34.0775\n",
      "Iteration 7500/28125, Loss: 35.5923\n",
      "Iteration 7600/28125, Loss: 33.7939\n",
      "Iteration 7700/28125, Loss: 41.1063\n",
      "Iteration 7800/28125, Loss: 36.0995\n",
      "Iteration 7900/28125, Loss: 40.0781\n",
      "Iteration 8000/28125, Loss: 31.3518\n",
      "Iteration 8100/28125, Loss: 30.0491\n",
      "Iteration 8200/28125, Loss: 40.7770\n",
      "Iteration 8300/28125, Loss: 32.8248\n",
      "Iteration 8400/28125, Loss: 35.0808\n",
      "Iteration 8500/28125, Loss: 34.1633\n",
      "Iteration 8600/28125, Loss: 38.7552\n",
      "Iteration 8700/28125, Loss: 32.4169\n",
      "Iteration 8800/28125, Loss: 31.3922\n",
      "Iteration 8900/28125, Loss: 40.7982\n",
      "Iteration 9000/28125, Loss: 33.5680\n",
      "Iteration 9100/28125, Loss: 39.1762\n",
      "Iteration 9200/28125, Loss: 43.0105\n",
      "Iteration 9300/28125, Loss: 32.4161\n",
      "Iteration 9400/28125, Loss: 31.0259\n",
      "Iteration 9500/28125, Loss: 31.7145\n",
      "Iteration 9600/28125, Loss: 30.1363\n",
      "Iteration 9700/28125, Loss: 37.2265\n",
      "Iteration 9800/28125, Loss: 37.2735\n",
      "Iteration 9900/28125, Loss: 37.7683\n",
      "Iteration 10000/28125, Loss: 28.0896\n",
      "Iteration 10100/28125, Loss: 31.8982\n",
      "Iteration 10200/28125, Loss: 29.8421\n",
      "Iteration 10300/28125, Loss: 35.1652\n",
      "Iteration 10400/28125, Loss: 33.4878\n",
      "Iteration 10500/28125, Loss: 29.2942\n",
      "Iteration 10600/28125, Loss: 40.1738\n",
      "Iteration 10700/28125, Loss: 35.6544\n",
      "Iteration 10800/28125, Loss: 31.9770\n",
      "Iteration 10900/28125, Loss: 34.0367\n",
      "Iteration 11000/28125, Loss: 28.8595\n",
      "Iteration 11100/28125, Loss: 36.6723\n",
      "Iteration 11200/28125, Loss: 32.4658\n",
      "Iteration 11300/28125, Loss: 35.7323\n",
      "Iteration 11400/28125, Loss: 34.1759\n",
      "Iteration 11500/28125, Loss: 35.6464\n",
      "Iteration 11600/28125, Loss: 37.4447\n",
      "Iteration 11700/28125, Loss: 36.5557\n",
      "Iteration 11800/28125, Loss: 33.7466\n",
      "Iteration 11900/28125, Loss: 32.8067\n",
      "Iteration 12000/28125, Loss: 30.9988\n",
      "Iteration 12100/28125, Loss: 38.9707\n",
      "Iteration 12200/28125, Loss: 36.5178\n",
      "Iteration 12300/28125, Loss: 24.3041\n",
      "Iteration 12400/28125, Loss: 43.8550\n",
      "Iteration 12500/28125, Loss: 33.5134\n",
      "Iteration 12600/28125, Loss: 34.0525\n",
      "Iteration 12700/28125, Loss: 37.8440\n",
      "Iteration 12800/28125, Loss: 38.3686\n",
      "Iteration 12900/28125, Loss: 26.0454\n",
      "Iteration 13000/28125, Loss: 37.8372\n",
      "Iteration 13100/28125, Loss: 35.0119\n",
      "Iteration 13200/28125, Loss: 37.7202\n",
      "Iteration 13300/28125, Loss: 35.3660\n",
      "Iteration 13400/28125, Loss: 40.4913\n",
      "Iteration 13500/28125, Loss: 34.1968\n",
      "Iteration 13600/28125, Loss: 35.9574\n",
      "Iteration 13700/28125, Loss: 30.8870\n",
      "Iteration 13800/28125, Loss: 26.4264\n",
      "Iteration 13900/28125, Loss: 27.5945\n",
      "Iteration 14000/28125, Loss: 30.4982\n",
      "Iteration 14100/28125, Loss: 26.9765\n",
      "Iteration 14200/28125, Loss: 30.7248\n",
      "Iteration 14300/28125, Loss: 42.2734\n",
      "Iteration 14400/28125, Loss: 46.1740\n",
      "Iteration 14500/28125, Loss: 48.0320\n",
      "Iteration 14600/28125, Loss: 35.5224\n",
      "Iteration 14700/28125, Loss: 37.8238\n",
      "Iteration 14800/28125, Loss: 36.1908\n",
      "Iteration 14900/28125, Loss: 36.1076\n",
      "Iteration 15000/28125, Loss: 34.6243\n",
      "Iteration 15100/28125, Loss: 42.7166\n",
      "Iteration 15200/28125, Loss: 32.2179\n",
      "Iteration 15300/28125, Loss: 44.8187\n",
      "Iteration 15400/28125, Loss: 44.7806\n",
      "Iteration 15500/28125, Loss: 26.6390\n",
      "Iteration 15600/28125, Loss: 42.7965\n",
      "Iteration 15700/28125, Loss: 30.1780\n",
      "Iteration 15800/28125, Loss: 39.7079\n",
      "Iteration 15900/28125, Loss: 29.7687\n",
      "Iteration 16000/28125, Loss: 34.7791\n",
      "Iteration 16100/28125, Loss: 38.9927\n",
      "Iteration 16200/28125, Loss: 35.1250\n",
      "Iteration 16300/28125, Loss: 31.6885\n",
      "Iteration 16400/28125, Loss: 46.4836\n",
      "Iteration 16500/28125, Loss: 35.8461\n",
      "Iteration 16600/28125, Loss: 37.8387\n",
      "Iteration 16700/28125, Loss: 33.7408\n",
      "Iteration 16800/28125, Loss: 44.5890\n",
      "Iteration 16900/28125, Loss: 35.5223\n",
      "Iteration 17000/28125, Loss: 38.1376\n",
      "Iteration 17100/28125, Loss: 27.2670\n",
      "Iteration 17200/28125, Loss: 27.3134\n",
      "Iteration 17300/28125, Loss: 38.3920\n",
      "Iteration 17400/28125, Loss: 33.5493\n",
      "Iteration 17500/28125, Loss: 34.8743\n",
      "Iteration 17600/28125, Loss: 35.2543\n",
      "Iteration 17700/28125, Loss: 41.9107\n",
      "Iteration 17800/28125, Loss: 32.3262\n",
      "Iteration 17900/28125, Loss: 34.1844\n",
      "Iteration 18000/28125, Loss: 31.0910\n",
      "Iteration 18100/28125, Loss: 32.0140\n",
      "Iteration 18200/28125, Loss: 38.3780\n",
      "Iteration 18300/28125, Loss: 40.8835\n",
      "Iteration 18400/28125, Loss: 44.6005\n",
      "Iteration 18500/28125, Loss: 34.8431\n",
      "Iteration 18600/28125, Loss: 38.9006\n",
      "Iteration 18700/28125, Loss: 30.0843\n",
      "Iteration 18800/28125, Loss: 37.7511\n",
      "Iteration 18900/28125, Loss: 37.8981\n",
      "Iteration 19000/28125, Loss: 39.5432\n",
      "Iteration 19100/28125, Loss: 35.7002\n",
      "Iteration 19200/28125, Loss: 44.5911\n",
      "Iteration 19300/28125, Loss: 29.8612\n",
      "Iteration 19400/28125, Loss: 30.4826\n",
      "Iteration 19500/28125, Loss: 28.0445\n",
      "Iteration 19600/28125, Loss: 34.4879\n",
      "Iteration 19700/28125, Loss: 34.7523\n",
      "Iteration 19800/28125, Loss: 35.9999\n",
      "Iteration 19900/28125, Loss: 32.0241\n",
      "Iteration 20000/28125, Loss: 37.8233\n",
      "Iteration 20100/28125, Loss: 35.3350\n",
      "Iteration 20200/28125, Loss: 32.8331\n",
      "Iteration 20300/28125, Loss: 27.6502\n",
      "Iteration 20400/28125, Loss: 31.4038\n",
      "Iteration 20500/28125, Loss: 37.0305\n",
      "Iteration 20600/28125, Loss: 30.8483\n",
      "Iteration 20700/28125, Loss: 33.7569\n",
      "Iteration 20800/28125, Loss: 34.5426\n",
      "Iteration 20900/28125, Loss: 33.5154\n",
      "Iteration 21000/28125, Loss: 36.6423\n",
      "Iteration 21100/28125, Loss: 32.8302\n",
      "Iteration 21200/28125, Loss: 38.0223\n",
      "Iteration 21300/28125, Loss: 38.0990\n",
      "Iteration 21400/28125, Loss: 28.8923\n",
      "Iteration 21500/28125, Loss: 43.4692\n",
      "Iteration 21600/28125, Loss: 30.4720\n",
      "Iteration 21700/28125, Loss: 32.2111\n",
      "Iteration 21800/28125, Loss: 41.7259\n",
      "Iteration 21900/28125, Loss: 30.8183\n",
      "Iteration 22000/28125, Loss: 28.3545\n",
      "Iteration 22100/28125, Loss: 28.4272\n",
      "Iteration 22200/28125, Loss: 30.3472\n",
      "Iteration 22300/28125, Loss: 31.5602\n",
      "Iteration 22400/28125, Loss: 29.3137\n",
      "Iteration 22500/28125, Loss: 45.7489\n",
      "Iteration 22600/28125, Loss: 31.4146\n",
      "Iteration 22700/28125, Loss: 33.2996\n",
      "Iteration 22800/28125, Loss: 33.2429\n",
      "Iteration 22900/28125, Loss: 32.6808\n",
      "Iteration 23000/28125, Loss: 30.7897\n",
      "Iteration 23100/28125, Loss: 37.3998\n",
      "Iteration 23200/28125, Loss: 27.7967\n",
      "Iteration 23300/28125, Loss: 34.2700\n",
      "Iteration 23400/28125, Loss: 32.8763\n",
      "Iteration 23500/28125, Loss: 39.0971\n",
      "Iteration 23600/28125, Loss: 40.4184\n",
      "Iteration 23700/28125, Loss: 29.0154\n",
      "Iteration 23800/28125, Loss: 43.3086\n",
      "Iteration 23900/28125, Loss: 30.1583\n",
      "Iteration 24000/28125, Loss: 39.0577\n",
      "Iteration 24100/28125, Loss: 27.0458\n",
      "Iteration 24200/28125, Loss: 34.4177\n",
      "Iteration 24300/28125, Loss: 42.0922\n",
      "Iteration 24400/28125, Loss: 32.8608\n",
      "Iteration 24500/28125, Loss: 36.2850\n",
      "Iteration 24600/28125, Loss: 33.7298\n",
      "Iteration 24700/28125, Loss: 32.8067\n",
      "Iteration 24800/28125, Loss: 34.4423\n",
      "Iteration 24900/28125, Loss: 32.0473\n",
      "Iteration 25000/28125, Loss: 33.6534\n",
      "Iteration 25100/28125, Loss: 44.9959\n",
      "Iteration 25200/28125, Loss: 31.5526\n",
      "Iteration 25300/28125, Loss: 35.1395\n",
      "Iteration 25400/28125, Loss: 45.5020\n",
      "Iteration 25500/28125, Loss: 31.4328\n",
      "Iteration 25600/28125, Loss: 39.9109\n",
      "Iteration 25700/28125, Loss: 34.8101\n",
      "Iteration 25800/28125, Loss: 33.1604\n",
      "Iteration 25900/28125, Loss: 41.5644\n",
      "Iteration 26000/28125, Loss: 37.4918\n",
      "Iteration 26100/28125, Loss: 41.9136\n",
      "Iteration 26200/28125, Loss: 32.2807\n",
      "Iteration 26300/28125, Loss: 42.8511\n",
      "Iteration 26400/28125, Loss: 33.7096\n",
      "Iteration 26500/28125, Loss: 34.4463\n",
      "Iteration 26600/28125, Loss: 37.2678\n",
      "Iteration 26700/28125, Loss: 33.8199\n",
      "Iteration 26800/28125, Loss: 41.6026\n",
      "Iteration 26900/28125, Loss: 35.7733\n",
      "Iteration 27000/28125, Loss: 40.9011\n",
      "Iteration 27100/28125, Loss: 34.2255\n",
      "Iteration 27200/28125, Loss: 45.9903\n",
      "Iteration 27300/28125, Loss: 33.1751\n",
      "Iteration 27400/28125, Loss: 33.3521\n",
      "Iteration 27500/28125, Loss: 35.9870\n",
      "Iteration 27600/28125, Loss: 35.0151\n",
      "Iteration 27700/28125, Loss: 34.7217\n",
      "Iteration 27800/28125, Loss: 34.7324\n",
      "Iteration 27900/28125, Loss: 32.1795\n",
      "Iteration 28000/28125, Loss: 31.2665\n",
      "Iteration 28100/28125, Loss: 33.0111\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 22.1074\n",
      "Iteration 200/14025, Loss: 34.1594\n",
      "Iteration 300/14025, Loss: 29.8133\n",
      "Iteration 400/14025, Loss: 40.1849\n",
      "Iteration 500/14025, Loss: 40.8524\n",
      "Iteration 600/14025, Loss: 39.8447\n",
      "Iteration 700/14025, Loss: 43.2294\n",
      "Iteration 800/14025, Loss: 43.6629\n",
      "Iteration 900/14025, Loss: 45.8115\n",
      "Iteration 1000/14025, Loss: 54.5383\n",
      "Iteration 1100/14025, Loss: 45.8964\n",
      "Iteration 1200/14025, Loss: 50.8618\n",
      "Iteration 1300/14025, Loss: 51.8854\n",
      "Iteration 1400/14025, Loss: 60.2101\n",
      "Iteration 1500/14025, Loss: 60.1571\n",
      "Iteration 1600/14025, Loss: 60.4479\n",
      "Iteration 1700/14025, Loss: 51.6197\n",
      "Iteration 1800/14025, Loss: 65.7675\n",
      "Iteration 1900/14025, Loss: 58.8566\n",
      "Iteration 2000/14025, Loss: 60.5470\n",
      "Iteration 2100/14025, Loss: 57.5449\n",
      "Iteration 2200/14025, Loss: 61.8291\n",
      "Iteration 2300/14025, Loss: 59.9133\n",
      "Iteration 2400/14025, Loss: 58.3032\n",
      "Iteration 2500/14025, Loss: 63.1019\n",
      "Iteration 2600/14025, Loss: 62.7340\n",
      "Iteration 2700/14025, Loss: 59.5841\n",
      "Iteration 2800/14025, Loss: 60.8698\n",
      "Iteration 2900/14025, Loss: 61.6423\n",
      "Iteration 3000/14025, Loss: 59.0921\n",
      "Iteration 3100/14025, Loss: 64.3567\n",
      "Iteration 3200/14025, Loss: 61.7405\n",
      "Iteration 3300/14025, Loss: 57.6741\n",
      "Iteration 3400/14025, Loss: 63.3746\n",
      "Iteration 3500/14025, Loss: 63.3832\n",
      "Iteration 3600/14025, Loss: 55.5853\n",
      "Iteration 3700/14025, Loss: 61.7978\n",
      "Iteration 3800/14025, Loss: 56.3954\n",
      "Iteration 3900/14025, Loss: 59.1770\n",
      "Iteration 4000/14025, Loss: 63.3397\n",
      "Iteration 4100/14025, Loss: 61.8802\n",
      "Iteration 4200/14025, Loss: 60.0758\n",
      "Iteration 4300/14025, Loss: 61.1511\n",
      "Iteration 4400/14025, Loss: 62.3068\n",
      "Iteration 4500/14025, Loss: 67.0397\n",
      "Iteration 4600/14025, Loss: 64.5902\n",
      "Iteration 4700/14025, Loss: 59.2553\n",
      "Iteration 4800/14025, Loss: 59.6097\n",
      "Iteration 4900/14025, Loss: 63.3874\n",
      "Iteration 5000/14025, Loss: 57.9559\n",
      "Iteration 5100/14025, Loss: 60.7465\n",
      "Iteration 5200/14025, Loss: 65.1046\n",
      "Iteration 5300/14025, Loss: 70.7258\n",
      "Iteration 5400/14025, Loss: 61.4001\n",
      "Iteration 5500/14025, Loss: 55.3356\n",
      "Iteration 5600/14025, Loss: 61.8087\n",
      "Iteration 5700/14025, Loss: 62.6071\n",
      "Iteration 5800/14025, Loss: 61.9941\n",
      "Iteration 5900/14025, Loss: 67.2412\n",
      "Iteration 6000/14025, Loss: 59.7958\n",
      "Iteration 6100/14025, Loss: 63.8650\n",
      "Iteration 6200/14025, Loss: 70.6958\n",
      "Iteration 6300/14025, Loss: 63.2154\n",
      "Iteration 6400/14025, Loss: 63.7331\n",
      "Iteration 6500/14025, Loss: 60.0378\n",
      "Iteration 6600/14025, Loss: 66.0227\n",
      "Iteration 6700/14025, Loss: 56.4408\n",
      "Iteration 6800/14025, Loss: 66.0293\n",
      "Iteration 6900/14025, Loss: 55.8040\n",
      "Iteration 7000/14025, Loss: 60.3638\n",
      "Iteration 7100/14025, Loss: 65.9916\n",
      "Iteration 7200/14025, Loss: 64.5025\n",
      "Iteration 7300/14025, Loss: 62.1473\n",
      "Iteration 7400/14025, Loss: 64.9319\n",
      "Iteration 7500/14025, Loss: 63.7856\n",
      "Iteration 7600/14025, Loss: 57.1239\n",
      "Iteration 7700/14025, Loss: 65.3407\n",
      "Iteration 7800/14025, Loss: 67.9437\n",
      "Iteration 7900/14025, Loss: 58.5474\n",
      "Iteration 8000/14025, Loss: 60.9724\n",
      "Iteration 8100/14025, Loss: 64.3954\n",
      "Iteration 8200/14025, Loss: 63.5992\n",
      "Iteration 8300/14025, Loss: 62.4657\n",
      "Iteration 8400/14025, Loss: 60.5292\n",
      "Iteration 8500/14025, Loss: 65.0829\n",
      "Iteration 8600/14025, Loss: 57.7115\n",
      "Iteration 8700/14025, Loss: 51.6803\n",
      "Iteration 8800/14025, Loss: 60.6115\n",
      "Iteration 8900/14025, Loss: 53.2878\n",
      "Iteration 9000/14025, Loss: 59.3847\n",
      "Iteration 9100/14025, Loss: 59.4680\n",
      "Iteration 9200/14025, Loss: 64.5726\n",
      "Iteration 9300/14025, Loss: 59.0348\n",
      "Iteration 9400/14025, Loss: 61.2942\n",
      "Iteration 9500/14025, Loss: 66.2862\n",
      "Iteration 9600/14025, Loss: 66.2924\n",
      "Iteration 9700/14025, Loss: 59.9552\n",
      "Iteration 9800/14025, Loss: 60.4199\n",
      "Iteration 9900/14025, Loss: 65.8182\n",
      "Iteration 10000/14025, Loss: 59.0663\n",
      "Iteration 10100/14025, Loss: 58.3362\n",
      "Iteration 10200/14025, Loss: 62.3410\n",
      "Iteration 10300/14025, Loss: 62.9559\n",
      "Iteration 10400/14025, Loss: 60.5806\n",
      "Iteration 10500/14025, Loss: 60.9552\n",
      "Iteration 10600/14025, Loss: 59.4354\n",
      "Iteration 10700/14025, Loss: 59.3340\n",
      "Iteration 10800/14025, Loss: 64.5328\n",
      "Iteration 10900/14025, Loss: 59.8893\n",
      "Iteration 11000/14025, Loss: 54.3997\n",
      "Iteration 11100/14025, Loss: 59.3179\n",
      "Iteration 11200/14025, Loss: 63.0871\n",
      "Iteration 11300/14025, Loss: 65.9222\n",
      "Iteration 11400/14025, Loss: 59.8563\n",
      "Iteration 11500/14025, Loss: 61.0319\n",
      "Iteration 11600/14025, Loss: 56.5417\n",
      "Iteration 11700/14025, Loss: 54.7677\n",
      "Iteration 11800/14025, Loss: 62.5272\n",
      "Iteration 11900/14025, Loss: 67.1862\n",
      "Iteration 12000/14025, Loss: 62.2319\n",
      "Iteration 12100/14025, Loss: 60.3810\n",
      "Iteration 12200/14025, Loss: 70.7395\n",
      "Iteration 12300/14025, Loss: 63.3294\n",
      "Iteration 12400/14025, Loss: 63.4891\n",
      "Iteration 12500/14025, Loss: 60.2747\n",
      "Iteration 12600/14025, Loss: 60.0752\n",
      "Iteration 12700/14025, Loss: 62.7992\n",
      "Iteration 12800/14025, Loss: 62.8866\n",
      "Iteration 12900/14025, Loss: 61.0130\n",
      "Iteration 13000/14025, Loss: 65.0607\n",
      "Iteration 13100/14025, Loss: 57.3346\n",
      "Iteration 13200/14025, Loss: 61.1262\n",
      "Iteration 13300/14025, Loss: 58.4075\n",
      "Iteration 13400/14025, Loss: 66.2198\n",
      "Iteration 13500/14025, Loss: 65.3932\n",
      "Iteration 13600/14025, Loss: 65.9572\n",
      "Iteration 13700/14025, Loss: 57.6242\n",
      "Iteration 13800/14025, Loss: 59.4163\n",
      "Iteration 13900/14025, Loss: 58.2442\n",
      "Iteration 14000/14025, Loss: 60.4192\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 33.0840\n",
      "Iteration 200/6975, Loss: 47.9738\n",
      "Iteration 300/6975, Loss: 51.0786\n",
      "Iteration 400/6975, Loss: 67.8838\n",
      "Iteration 500/6975, Loss: 72.7756\n",
      "Iteration 600/6975, Loss: 78.0608\n",
      "Iteration 700/6975, Loss: 90.1684\n",
      "Iteration 800/6975, Loss: 91.8403\n",
      "Iteration 900/6975, Loss: 97.2316\n",
      "Iteration 1000/6975, Loss: 102.5665\n",
      "Iteration 1100/6975, Loss: 103.4625\n",
      "Iteration 1200/6975, Loss: 103.9957\n",
      "Iteration 1300/6975, Loss: 110.0136\n",
      "Iteration 1400/6975, Loss: 109.2229\n",
      "Iteration 1500/6975, Loss: 114.6451\n",
      "Iteration 1600/6975, Loss: 115.4916\n",
      "Iteration 1700/6975, Loss: 120.6855\n",
      "Iteration 1800/6975, Loss: 117.0694\n",
      "Iteration 1900/6975, Loss: 120.7370\n",
      "Iteration 2000/6975, Loss: 121.8480\n",
      "Iteration 2100/6975, Loss: 122.5700\n",
      "Iteration 2200/6975, Loss: 123.6241\n",
      "Iteration 2300/6975, Loss: 126.4177\n",
      "Iteration 2400/6975, Loss: 125.7539\n",
      "Iteration 2500/6975, Loss: 128.9219\n",
      "Iteration 2600/6975, Loss: 128.2254\n",
      "Iteration 2700/6975, Loss: 127.5028\n",
      "Iteration 2800/6975, Loss: 129.0142\n",
      "Iteration 2900/6975, Loss: 127.7665\n",
      "Iteration 3000/6975, Loss: 134.6575\n",
      "Iteration 3100/6975, Loss: 128.4381\n",
      "Iteration 3200/6975, Loss: 126.2077\n",
      "Iteration 3300/6975, Loss: 125.7105\n",
      "Iteration 3400/6975, Loss: 133.1750\n",
      "Iteration 3500/6975, Loss: 125.8411\n",
      "Iteration 3600/6975, Loss: 130.7374\n",
      "Iteration 3700/6975, Loss: 128.5877\n",
      "Iteration 3800/6975, Loss: 127.5176\n",
      "Iteration 3900/6975, Loss: 133.5339\n",
      "Iteration 4000/6975, Loss: 127.3636\n",
      "Iteration 4100/6975, Loss: 125.7385\n",
      "Iteration 4200/6975, Loss: 124.8312\n",
      "Iteration 4300/6975, Loss: 123.8861\n",
      "Iteration 4400/6975, Loss: 127.7742\n",
      "Iteration 4500/6975, Loss: 128.5563\n",
      "Iteration 4600/6975, Loss: 128.8833\n",
      "Iteration 4700/6975, Loss: 127.7372\n",
      "Iteration 4800/6975, Loss: 127.1441\n",
      "Iteration 4900/6975, Loss: 127.7433\n",
      "Iteration 5000/6975, Loss: 130.3460\n",
      "Iteration 5100/6975, Loss: 129.9568\n",
      "Iteration 5200/6975, Loss: 125.3346\n",
      "Iteration 5300/6975, Loss: 126.6352\n",
      "Iteration 5400/6975, Loss: 124.7681\n",
      "Iteration 5500/6975, Loss: 124.8468\n",
      "Iteration 5600/6975, Loss: 126.3792\n",
      "Iteration 5700/6975, Loss: 123.5226\n",
      "Iteration 5800/6975, Loss: 122.2991\n",
      "Iteration 5900/6975, Loss: 127.3607\n",
      "Iteration 6000/6975, Loss: 132.0463\n",
      "Iteration 6100/6975, Loss: 126.0154\n",
      "Iteration 6200/6975, Loss: 126.1539\n",
      "Iteration 6300/6975, Loss: 129.6151\n",
      "Iteration 6400/6975, Loss: 130.0850\n",
      "Iteration 6500/6975, Loss: 123.9151\n",
      "Iteration 6600/6975, Loss: 120.6448\n",
      "Iteration 6700/6975, Loss: 126.6519\n",
      "Iteration 6800/6975, Loss: 126.0036\n",
      "Iteration 6900/6975, Loss: 121.1732\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 64.8111\n",
      "Iteration 200/3450, Loss: 93.7983\n",
      "Iteration 300/3450, Loss: 119.4869\n",
      "Iteration 400/3450, Loss: 148.5176\n",
      "Iteration 500/3450, Loss: 168.9312\n",
      "Iteration 600/3450, Loss: 182.3896\n",
      "Iteration 700/3450, Loss: 210.2857\n",
      "Iteration 800/3450, Loss: 220.5990\n",
      "Iteration 900/3450, Loss: 236.2287\n",
      "Iteration 1000/3450, Loss: 248.2182\n",
      "Iteration 1100/3450, Loss: 253.4053\n",
      "Iteration 1200/3450, Loss: 261.6677\n",
      "Iteration 1300/3450, Loss: 269.8257\n",
      "Iteration 1400/3450, Loss: 278.1359\n",
      "Iteration 1500/3450, Loss: 286.1177\n",
      "Iteration 1600/3450, Loss: 288.2328\n",
      "Iteration 1700/3450, Loss: 293.3394\n",
      "Iteration 1800/3450, Loss: 294.9617\n",
      "Iteration 1900/3450, Loss: 300.5968\n",
      "Iteration 2000/3450, Loss: 301.9868\n",
      "Iteration 2100/3450, Loss: 306.6429\n",
      "Iteration 2200/3450, Loss: 316.1109\n",
      "Iteration 2300/3450, Loss: 313.2255\n",
      "Iteration 2400/3450, Loss: 305.9999\n",
      "Iteration 2500/3450, Loss: 308.8190\n",
      "Iteration 2600/3450, Loss: 310.5104\n",
      "Iteration 2700/3450, Loss: 316.4938\n",
      "Iteration 2800/3450, Loss: 311.6165\n",
      "Iteration 2900/3450, Loss: 308.2641\n",
      "Iteration 3000/3450, Loss: 310.8994\n",
      "Iteration 3100/3450, Loss: 309.3557\n",
      "Iteration 3200/3450, Loss: 307.1029\n",
      "Iteration 3300/3450, Loss: 310.4135\n",
      "Iteration 3400/3450, Loss: 308.9327\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 150.9564\n",
      "Iteration 200/1725, Loss: 238.3353\n",
      "Iteration 300/1725, Loss: 341.0465\n",
      "Iteration 400/1725, Loss: 440.1106\n",
      "Iteration 500/1725, Loss: 514.9119\n",
      "Iteration 600/1725, Loss: 570.8086\n",
      "Iteration 700/1725, Loss: 633.3661\n",
      "Iteration 800/1725, Loss: 672.2754\n",
      "Iteration 900/1725, Loss: 732.8741\n",
      "Iteration 1000/1725, Loss: 753.5953\n",
      "Iteration 1100/1725, Loss: 779.6666\n",
      "Iteration 1200/1725, Loss: 787.0888\n",
      "Iteration 1300/1725, Loss: 820.4934\n",
      "Iteration 1400/1725, Loss: 833.6390\n",
      "Iteration 1500/1725, Loss: 858.1258\n",
      "Iteration 1600/1725, Loss: 871.6090\n",
      "Iteration 1700/1725, Loss: 873.7469\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 442.0557\n",
      "Iteration 200/825, Loss: 858.1606\n",
      "Iteration 300/825, Loss: 1216.0523\n",
      "Iteration 400/825, Loss: 1516.2934\n",
      "Iteration 500/825, Loss: 1803.9885\n",
      "Iteration 600/825, Loss: 2010.6055\n",
      "Iteration 700/825, Loss: 2237.3967\n",
      "Iteration 800/825, Loss: 2399.6146\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 1602.5935\n",
      "Iteration 200/375, Loss: 3106.8589\n",
      "Iteration 300/375, Loss: 4533.9126\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 5661.2368\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 26.5458\n",
      "Iteration 200/28125, Loss: 26.3598\n",
      "Iteration 300/28125, Loss: 27.3336\n",
      "Iteration 400/28125, Loss: 32.1644\n",
      "Iteration 500/28125, Loss: 22.5447\n",
      "Iteration 600/28125, Loss: 24.0977\n",
      "Iteration 700/28125, Loss: 13.9837\n",
      "Iteration 800/28125, Loss: 33.5145\n",
      "Iteration 900/28125, Loss: 12.3212\n",
      "Iteration 1000/28125, Loss: 22.7999\n",
      "Iteration 1100/28125, Loss: 16.7696\n",
      "Iteration 1200/28125, Loss: 8.9950\n",
      "Iteration 1300/28125, Loss: 25.7270\n",
      "Iteration 1400/28125, Loss: 28.8242\n",
      "Iteration 1500/28125, Loss: 18.7618\n",
      "Iteration 1600/28125, Loss: 23.4355\n",
      "Iteration 1700/28125, Loss: 15.1631\n",
      "Iteration 1800/28125, Loss: 26.4774\n",
      "Iteration 1900/28125, Loss: 20.2452\n",
      "Iteration 2000/28125, Loss: 24.8899\n",
      "Iteration 2100/28125, Loss: 20.9069\n",
      "Iteration 2200/28125, Loss: 19.0315\n",
      "Iteration 2300/28125, Loss: 21.5553\n",
      "Iteration 2400/28125, Loss: 26.3764\n",
      "Iteration 2500/28125, Loss: 31.0686\n",
      "Iteration 2600/28125, Loss: 27.1771\n",
      "Iteration 2700/28125, Loss: 18.9434\n",
      "Iteration 2800/28125, Loss: 32.4456\n",
      "Iteration 2900/28125, Loss: 28.0618\n",
      "Iteration 3000/28125, Loss: 24.3537\n",
      "Iteration 3100/28125, Loss: 37.7503\n",
      "Iteration 3200/28125, Loss: 24.3802\n",
      "Iteration 3300/28125, Loss: 14.7945\n",
      "Iteration 3400/28125, Loss: 25.9119\n",
      "Iteration 3500/28125, Loss: 26.2426\n",
      "Iteration 3600/28125, Loss: 35.2714\n",
      "Iteration 3700/28125, Loss: 22.7279\n",
      "Iteration 3800/28125, Loss: 27.4695\n",
      "Iteration 3900/28125, Loss: 27.8630\n",
      "Iteration 4000/28125, Loss: 32.6482\n",
      "Iteration 4100/28125, Loss: 24.3993\n",
      "Iteration 4200/28125, Loss: 37.6899\n",
      "Iteration 4300/28125, Loss: 20.7740\n",
      "Iteration 4400/28125, Loss: 29.7820\n",
      "Iteration 4500/28125, Loss: 34.4830\n",
      "Iteration 4600/28125, Loss: 27.1626\n",
      "Iteration 4700/28125, Loss: 27.5289\n",
      "Iteration 4800/28125, Loss: 31.1671\n",
      "Iteration 4900/28125, Loss: 27.3327\n",
      "Iteration 5000/28125, Loss: 45.0064\n",
      "Iteration 5100/28125, Loss: 36.7842\n",
      "Iteration 5200/28125, Loss: 32.8765\n",
      "Iteration 5300/28125, Loss: 39.2465\n",
      "Iteration 5400/28125, Loss: 27.3770\n",
      "Iteration 5500/28125, Loss: 38.9142\n",
      "Iteration 5600/28125, Loss: 25.7258\n",
      "Iteration 5700/28125, Loss: 43.2756\n",
      "Iteration 5800/28125, Loss: 34.9428\n",
      "Iteration 5900/28125, Loss: 22.3884\n",
      "Iteration 6000/28125, Loss: 40.0327\n",
      "Iteration 6100/28125, Loss: 31.7878\n",
      "Iteration 6200/28125, Loss: 36.5952\n",
      "Iteration 6300/28125, Loss: 36.8844\n",
      "Iteration 6400/28125, Loss: 25.1181\n",
      "Iteration 6500/28125, Loss: 33.2622\n",
      "Iteration 6600/28125, Loss: 33.5979\n",
      "Iteration 6700/28125, Loss: 33.8799\n",
      "Iteration 6800/28125, Loss: 36.5887\n",
      "Iteration 6900/28125, Loss: 38.7662\n",
      "Iteration 7000/28125, Loss: 34.9274\n",
      "Iteration 7100/28125, Loss: 30.9887\n",
      "Iteration 7200/28125, Loss: 27.0388\n",
      "Iteration 7300/28125, Loss: 40.3356\n",
      "Iteration 7400/28125, Loss: 33.0702\n",
      "Iteration 7500/28125, Loss: 36.2550\n",
      "Iteration 7600/28125, Loss: 41.3772\n",
      "Iteration 7700/28125, Loss: 50.3233\n",
      "Iteration 7800/28125, Loss: 38.3412\n",
      "Iteration 7900/28125, Loss: 42.5407\n",
      "Iteration 8000/28125, Loss: 38.2866\n",
      "Iteration 8100/28125, Loss: 38.6360\n",
      "Iteration 8200/28125, Loss: 47.1113\n",
      "Iteration 8300/28125, Loss: 43.5723\n",
      "Iteration 8400/28125, Loss: 39.5574\n",
      "Iteration 8500/28125, Loss: 35.5903\n",
      "Iteration 8600/28125, Loss: 44.5695\n",
      "Iteration 8700/28125, Loss: 31.9765\n",
      "Iteration 8800/28125, Loss: 49.5376\n",
      "Iteration 8900/28125, Loss: 49.9236\n",
      "Iteration 9000/28125, Loss: 41.6142\n",
      "Iteration 9100/28125, Loss: 33.3256\n",
      "Iteration 9200/28125, Loss: 46.6172\n",
      "Iteration 9300/28125, Loss: 46.9098\n",
      "Iteration 9400/28125, Loss: 38.5536\n",
      "Iteration 9500/28125, Loss: 38.9073\n",
      "Iteration 9600/28125, Loss: 41.5176\n",
      "Iteration 9700/28125, Loss: 48.2560\n",
      "Iteration 9800/28125, Loss: 48.6853\n",
      "Iteration 9900/28125, Loss: 44.6989\n",
      "Iteration 10000/28125, Loss: 40.0765\n",
      "Iteration 10100/28125, Loss: 53.9665\n",
      "Iteration 10200/28125, Loss: 45.6247\n",
      "Iteration 10300/28125, Loss: 45.9335\n",
      "Iteration 10400/28125, Loss: 46.2745\n",
      "Iteration 10500/28125, Loss: 48.6046\n",
      "Iteration 10600/28125, Loss: 51.2122\n",
      "Iteration 10700/28125, Loss: 47.2798\n",
      "Iteration 10800/28125, Loss: 51.9514\n",
      "Iteration 10900/28125, Loss: 47.9458\n",
      "Iteration 11000/28125, Loss: 48.2025\n",
      "Iteration 11100/28125, Loss: 52.8048\n",
      "Iteration 11200/28125, Loss: 44.4563\n",
      "Iteration 11300/28125, Loss: 43.2437\n",
      "Iteration 11400/28125, Loss: 45.0053\n",
      "Iteration 11500/28125, Loss: 53.9830\n",
      "Iteration 11600/28125, Loss: 45.6649\n",
      "Iteration 11700/28125, Loss: 63.3280\n",
      "Iteration 11800/28125, Loss: 42.0134\n",
      "Iteration 11900/28125, Loss: 42.3329\n",
      "Iteration 12000/28125, Loss: 56.4301\n",
      "Iteration 12100/28125, Loss: 55.7767\n",
      "Iteration 12200/28125, Loss: 55.9332\n",
      "Iteration 12300/28125, Loss: 47.7017\n",
      "Iteration 12400/28125, Loss: 69.5290\n",
      "Iteration 12500/28125, Loss: 56.8924\n",
      "Iteration 12600/28125, Loss: 52.8849\n",
      "Iteration 12700/28125, Loss: 57.3931\n",
      "Iteration 12800/28125, Loss: 53.4434\n",
      "Iteration 12900/28125, Loss: 51.6180\n",
      "Iteration 13000/28125, Loss: 57.8717\n",
      "Iteration 13100/28125, Loss: 49.9766\n",
      "Iteration 13200/28125, Loss: 67.4904\n",
      "Iteration 13300/28125, Loss: 46.1685\n",
      "Iteration 13400/28125, Loss: 54.1614\n",
      "Iteration 13500/28125, Loss: 46.7553\n",
      "Iteration 13600/28125, Loss: 51.3400\n",
      "Iteration 13700/28125, Loss: 49.6215\n",
      "Iteration 13800/28125, Loss: 51.8089\n",
      "Iteration 13900/28125, Loss: 52.0088\n",
      "Iteration 14000/28125, Loss: 52.2526\n",
      "Iteration 14100/28125, Loss: 48.1066\n",
      "Iteration 14200/28125, Loss: 48.3166\n",
      "Iteration 14300/28125, Loss: 61.5472\n",
      "Iteration 14400/28125, Loss: 57.4488\n",
      "Iteration 14500/28125, Loss: 66.3300\n",
      "Iteration 14600/28125, Loss: 62.2600\n",
      "Iteration 14700/28125, Loss: 53.8913\n",
      "Iteration 14800/28125, Loss: 60.9437\n",
      "Iteration 14900/28125, Loss: 71.5722\n",
      "Iteration 15000/28125, Loss: 50.2493\n",
      "Iteration 15100/28125, Loss: 54.7606\n",
      "Iteration 15200/28125, Loss: 54.9199\n",
      "Iteration 15300/28125, Loss: 68.0876\n",
      "Iteration 15400/28125, Loss: 60.8612\n",
      "Iteration 15500/28125, Loss: 59.9968\n",
      "Iteration 15600/28125, Loss: 60.2442\n",
      "Iteration 15700/28125, Loss: 54.6172\n",
      "Iteration 15800/28125, Loss: 56.4083\n",
      "Iteration 15900/28125, Loss: 56.6677\n",
      "Iteration 16000/28125, Loss: 61.1847\n",
      "Iteration 16100/28125, Loss: 57.1484\n",
      "Iteration 16200/28125, Loss: 61.6787\n",
      "Iteration 16300/28125, Loss: 53.2467\n",
      "Iteration 16400/28125, Loss: 62.1162\n",
      "Iteration 16500/28125, Loss: 62.7091\n",
      "Iteration 16600/28125, Loss: 62.9901\n",
      "Iteration 16700/28125, Loss: 62.7904\n",
      "Iteration 16800/28125, Loss: 69.0612\n",
      "Iteration 16900/28125, Loss: 58.8910\n",
      "Iteration 17000/28125, Loss: 59.5111\n",
      "Iteration 17100/28125, Loss: 54.9518\n",
      "Iteration 17200/28125, Loss: 61.4869\n",
      "Iteration 17300/28125, Loss: 56.3656\n",
      "Iteration 17400/28125, Loss: 59.7921\n",
      "Iteration 17500/28125, Loss: 59.9741\n",
      "Iteration 17600/28125, Loss: 61.1802\n",
      "Iteration 17700/28125, Loss: 73.2779\n",
      "Iteration 17800/28125, Loss: 57.1541\n",
      "Iteration 17900/28125, Loss: 65.6332\n",
      "Iteration 18000/28125, Loss: 72.6438\n",
      "Iteration 18100/28125, Loss: 65.3644\n",
      "Iteration 18200/28125, Loss: 66.3688\n",
      "Iteration 18300/28125, Loss: 66.3092\n",
      "Iteration 18400/28125, Loss: 68.7023\n",
      "Iteration 18500/28125, Loss: 66.2568\n",
      "Iteration 18600/28125, Loss: 62.1092\n",
      "Iteration 18700/28125, Loss: 58.0272\n",
      "Iteration 18800/28125, Loss: 63.8662\n",
      "Iteration 18900/28125, Loss: 62.6881\n",
      "Iteration 19000/28125, Loss: 73.5219\n",
      "Iteration 19100/28125, Loss: 70.2099\n",
      "Iteration 19200/28125, Loss: 67.1585\n",
      "Iteration 19300/28125, Loss: 62.3629\n",
      "Iteration 19400/28125, Loss: 63.7451\n",
      "Iteration 19500/28125, Loss: 66.5112\n",
      "Iteration 19600/28125, Loss: 61.9290\n",
      "Iteration 19700/28125, Loss: 68.6447\n",
      "Iteration 19800/28125, Loss: 60.2838\n",
      "Iteration 19900/28125, Loss: 63.9428\n",
      "Iteration 20000/28125, Loss: 73.4396\n",
      "Iteration 20100/28125, Loss: 69.7409\n",
      "Iteration 20200/28125, Loss: 76.5369\n",
      "Iteration 20300/28125, Loss: 62.0673\n",
      "Iteration 20400/28125, Loss: 66.3638\n",
      "Iteration 20500/28125, Loss: 65.9086\n",
      "Iteration 20600/28125, Loss: 66.1152\n",
      "Iteration 20700/28125, Loss: 64.8437\n",
      "Iteration 20800/28125, Loss: 66.4965\n",
      "Iteration 20900/28125, Loss: 66.7609\n",
      "Iteration 21000/28125, Loss: 71.2550\n",
      "Iteration 21100/28125, Loss: 67.1460\n",
      "Iteration 21200/28125, Loss: 68.7535\n",
      "Iteration 21300/28125, Loss: 63.0883\n",
      "Iteration 21400/28125, Loss: 71.9012\n",
      "Iteration 21500/28125, Loss: 74.3370\n",
      "Iteration 21600/28125, Loss: 67.9272\n",
      "Iteration 21700/28125, Loss: 68.0962\n",
      "Iteration 21800/28125, Loss: 64.8123\n",
      "Iteration 21900/28125, Loss: 76.2445\n",
      "Iteration 22000/28125, Loss: 71.2052\n",
      "Iteration 22100/28125, Loss: 70.6131\n",
      "Iteration 22200/28125, Loss: 64.7054\n",
      "Iteration 22300/28125, Loss: 73.4755\n",
      "Iteration 22400/28125, Loss: 66.9366\n",
      "Iteration 22500/28125, Loss: 82.3908\n",
      "Iteration 22600/28125, Loss: 69.6160\n",
      "Iteration 22700/28125, Loss: 69.7673\n",
      "Iteration 22800/28125, Loss: 65.9204\n",
      "Iteration 22900/28125, Loss: 70.4020\n",
      "Iteration 23000/28125, Loss: 78.8390\n",
      "Iteration 23100/28125, Loss: 70.3260\n",
      "Iteration 23200/28125, Loss: 70.4994\n",
      "Iteration 23300/28125, Loss: 79.3107\n",
      "Iteration 23400/28125, Loss: 66.4703\n",
      "Iteration 23500/28125, Loss: 70.9279\n",
      "Iteration 23600/28125, Loss: 75.3353\n",
      "Iteration 23700/28125, Loss: 66.8457\n",
      "Iteration 23800/28125, Loss: 71.3469\n",
      "Iteration 23900/28125, Loss: 71.3348\n",
      "Iteration 24000/28125, Loss: 71.7149\n",
      "Iteration 24100/28125, Loss: 70.7480\n",
      "Iteration 24200/28125, Loss: 72.0345\n",
      "Iteration 24300/28125, Loss: 72.1544\n",
      "Iteration 24400/28125, Loss: 72.3070\n",
      "Iteration 24500/28125, Loss: 76.7160\n",
      "Iteration 24600/28125, Loss: 68.2239\n",
      "Iteration 24700/28125, Loss: 77.1375\n",
      "Iteration 24800/28125, Loss: 68.4004\n",
      "Iteration 24900/28125, Loss: 70.8270\n",
      "Iteration 25000/28125, Loss: 74.9452\n",
      "Iteration 25100/28125, Loss: 68.8048\n",
      "Iteration 25200/28125, Loss: 73.4789\n",
      "Iteration 25300/28125, Loss: 73.3301\n",
      "Iteration 25400/28125, Loss: 73.4444\n",
      "Iteration 25500/28125, Loss: 69.2076\n",
      "Iteration 25600/28125, Loss: 73.6508\n",
      "Iteration 25700/28125, Loss: 74.4716\n",
      "Iteration 25800/28125, Loss: 77.8502\n",
      "Iteration 25900/28125, Loss: 78.4217\n",
      "Iteration 26000/28125, Loss: 82.8875\n",
      "Iteration 26100/28125, Loss: 87.2601\n",
      "Iteration 26200/28125, Loss: 74.6896\n",
      "Iteration 26300/28125, Loss: 70.3519\n",
      "Iteration 26400/28125, Loss: 80.6397\n",
      "Iteration 26500/28125, Loss: 70.6109\n",
      "Iteration 26600/28125, Loss: 75.0444\n",
      "Iteration 26700/28125, Loss: 70.8168\n",
      "Iteration 26800/28125, Loss: 73.2918\n",
      "Iteration 26900/28125, Loss: 71.0101\n",
      "Iteration 27000/28125, Loss: 75.4005\n",
      "Iteration 27100/28125, Loss: 75.4941\n",
      "Iteration 27200/28125, Loss: 78.2798\n",
      "Iteration 27300/28125, Loss: 75.6708\n",
      "Iteration 27400/28125, Loss: 74.0994\n",
      "Iteration 27500/28125, Loss: 79.0704\n",
      "Iteration 27600/28125, Loss: 71.6824\n",
      "Iteration 27700/28125, Loss: 71.7609\n",
      "Iteration 27800/28125, Loss: 71.9121\n",
      "Iteration 27900/28125, Loss: 76.3231\n",
      "Iteration 28000/28125, Loss: 72.2346\n",
      "Iteration 28100/28125, Loss: 80.9654\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 18.3553\n",
      "Iteration 200/14025, Loss: 27.9706\n",
      "Iteration 300/14025, Loss: 22.7095\n",
      "Iteration 400/14025, Loss: 25.0969\n",
      "Iteration 500/14025, Loss: 21.4919\n",
      "Iteration 600/14025, Loss: 19.9211\n",
      "Iteration 700/14025, Loss: 27.2161\n",
      "Iteration 800/14025, Loss: 22.0934\n",
      "Iteration 900/14025, Loss: 17.5335\n",
      "Iteration 1000/14025, Loss: 30.7319\n",
      "Iteration 1100/14025, Loss: 25.4550\n",
      "Iteration 1200/14025, Loss: 29.0195\n",
      "Iteration 1300/14025, Loss: 17.5897\n",
      "Iteration 1400/14025, Loss: 28.5106\n",
      "Iteration 1500/14025, Loss: 33.5910\n",
      "Iteration 1600/14025, Loss: 31.1415\n",
      "Iteration 1700/14025, Loss: 24.5077\n",
      "Iteration 1800/14025, Loss: 25.4060\n",
      "Iteration 1900/14025, Loss: 31.5193\n",
      "Iteration 2000/14025, Loss: 25.0600\n",
      "Iteration 2100/14025, Loss: 29.3260\n",
      "Iteration 2200/14025, Loss: 34.9987\n",
      "Iteration 2300/14025, Loss: 36.6887\n",
      "Iteration 2400/14025, Loss: 35.1881\n",
      "Iteration 2500/14025, Loss: 34.7546\n",
      "Iteration 2600/14025, Loss: 35.3805\n",
      "Iteration 2700/14025, Loss: 31.1230\n",
      "Iteration 2800/14025, Loss: 33.6319\n",
      "Iteration 2900/14025, Loss: 33.8119\n",
      "Iteration 3000/14025, Loss: 32.6526\n",
      "Iteration 3100/14025, Loss: 37.1665\n",
      "Iteration 3200/14025, Loss: 30.5207\n",
      "Iteration 3300/14025, Loss: 35.1809\n",
      "Iteration 3400/14025, Loss: 40.8298\n",
      "Iteration 3500/14025, Loss: 43.8720\n",
      "Iteration 3600/14025, Loss: 39.1906\n",
      "Iteration 3700/14025, Loss: 39.4896\n",
      "Iteration 3800/14025, Loss: 37.8390\n",
      "Iteration 3900/14025, Loss: 42.4217\n",
      "Iteration 4000/14025, Loss: 45.3928\n",
      "Iteration 4100/14025, Loss: 39.8139\n",
      "Iteration 4200/14025, Loss: 45.5889\n",
      "Iteration 4300/14025, Loss: 43.8949\n",
      "Iteration 4400/14025, Loss: 50.2244\n",
      "Iteration 4500/14025, Loss: 51.4955\n",
      "Iteration 4600/14025, Loss: 51.0055\n",
      "Iteration 4700/14025, Loss: 49.2352\n",
      "Iteration 4800/14025, Loss: 43.0390\n",
      "Iteration 4900/14025, Loss: 48.0494\n",
      "Iteration 5000/14025, Loss: 45.0727\n",
      "Iteration 5100/14025, Loss: 55.9306\n",
      "Iteration 5200/14025, Loss: 53.3011\n",
      "Iteration 5300/14025, Loss: 52.4531\n",
      "Iteration 5400/14025, Loss: 53.6861\n",
      "Iteration 5500/14025, Loss: 48.4229\n",
      "Iteration 5600/14025, Loss: 55.0040\n",
      "Iteration 5700/14025, Loss: 56.6858\n",
      "Iteration 5800/14025, Loss: 56.4470\n",
      "Iteration 5900/14025, Loss: 51.0446\n",
      "Iteration 6000/14025, Loss: 62.1044\n",
      "Iteration 6100/14025, Loss: 56.3404\n",
      "Iteration 6200/14025, Loss: 63.4063\n",
      "Iteration 6300/14025, Loss: 59.4217\n",
      "Iteration 6400/14025, Loss: 60.6017\n",
      "Iteration 6500/14025, Loss: 63.5866\n",
      "Iteration 6600/14025, Loss: 62.9220\n",
      "Iteration 6700/14025, Loss: 58.0289\n",
      "Iteration 6800/14025, Loss: 61.3587\n",
      "Iteration 6900/14025, Loss: 62.6985\n",
      "Iteration 7000/14025, Loss: 61.7867\n",
      "Iteration 7100/14025, Loss: 61.1584\n",
      "Iteration 7200/14025, Loss: 67.6086\n",
      "Iteration 7300/14025, Loss: 62.6068\n",
      "Iteration 7400/14025, Loss: 66.0065\n",
      "Iteration 7500/14025, Loss: 66.5188\n",
      "Iteration 7600/14025, Loss: 61.2855\n",
      "Iteration 7700/14025, Loss: 74.3876\n",
      "Iteration 7800/14025, Loss: 69.9306\n",
      "Iteration 7900/14025, Loss: 66.3096\n",
      "Iteration 8000/14025, Loss: 67.9479\n",
      "Iteration 8100/14025, Loss: 65.5058\n",
      "Iteration 8200/14025, Loss: 72.6675\n",
      "Iteration 8300/14025, Loss: 69.2432\n",
      "Iteration 8400/14025, Loss: 64.7462\n",
      "Iteration 8500/14025, Loss: 67.3724\n",
      "Iteration 8600/14025, Loss: 66.8525\n",
      "Iteration 8700/14025, Loss: 64.2647\n",
      "Iteration 8800/14025, Loss: 68.6550\n",
      "Iteration 8900/14025, Loss: 66.8758\n",
      "Iteration 9000/14025, Loss: 80.4482\n",
      "Iteration 9100/14025, Loss: 72.3608\n",
      "Iteration 9200/14025, Loss: 68.5240\n",
      "Iteration 9300/14025, Loss: 68.9611\n",
      "Iteration 9400/14025, Loss: 74.1826\n",
      "Iteration 9500/14025, Loss: 73.4735\n",
      "Iteration 9600/14025, Loss: 80.8046\n",
      "Iteration 9700/14025, Loss: 77.2298\n",
      "Iteration 9800/14025, Loss: 69.1030\n",
      "Iteration 9900/14025, Loss: 76.7074\n",
      "Iteration 10000/14025, Loss: 78.7417\n",
      "Iteration 10100/14025, Loss: 75.9719\n",
      "Iteration 10200/14025, Loss: 78.4974\n",
      "Iteration 10300/14025, Loss: 77.9237\n",
      "Iteration 10400/14025, Loss: 75.7151\n",
      "Iteration 10500/14025, Loss: 74.6687\n",
      "Iteration 10600/14025, Loss: 78.8486\n",
      "Iteration 10700/14025, Loss: 73.3203\n",
      "Iteration 10800/14025, Loss: 82.2851\n",
      "Iteration 10900/14025, Loss: 81.2488\n",
      "Iteration 11000/14025, Loss: 78.8564\n",
      "Iteration 11100/14025, Loss: 77.7981\n",
      "Iteration 11200/14025, Loss: 82.6136\n",
      "Iteration 11300/14025, Loss: 84.6394\n",
      "Iteration 11400/14025, Loss: 78.5701\n",
      "Iteration 11500/14025, Loss: 87.5349\n",
      "Iteration 11600/14025, Loss: 79.3463\n",
      "Iteration 11700/14025, Loss: 86.2529\n",
      "Iteration 11800/14025, Loss: 80.2062\n",
      "Iteration 11900/14025, Loss: 80.5493\n",
      "Iteration 12000/14025, Loss: 83.1522\n",
      "Iteration 12100/14025, Loss: 85.7633\n",
      "Iteration 12200/14025, Loss: 84.4314\n",
      "Iteration 12300/14025, Loss: 86.5910\n",
      "Iteration 12400/14025, Loss: 87.8426\n",
      "Iteration 12500/14025, Loss: 89.5222\n",
      "Iteration 12600/14025, Loss: 85.6099\n",
      "Iteration 12700/14025, Loss: 83.2948\n",
      "Iteration 12800/14025, Loss: 88.4824\n",
      "Iteration 12900/14025, Loss: 90.3344\n",
      "Iteration 13000/14025, Loss: 90.1848\n",
      "Iteration 13100/14025, Loss: 85.2476\n",
      "Iteration 13200/14025, Loss: 92.1394\n",
      "Iteration 13300/14025, Loss: 83.8076\n",
      "Iteration 13400/14025, Loss: 86.3581\n",
      "Iteration 13500/14025, Loss: 86.6877\n",
      "Iteration 13600/14025, Loss: 92.5933\n",
      "Iteration 13700/14025, Loss: 89.4429\n",
      "Iteration 13800/14025, Loss: 87.7912\n",
      "Iteration 13900/14025, Loss: 85.7796\n",
      "Iteration 14000/14025, Loss: 86.3856\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 21.9752\n",
      "Iteration 200/6975, Loss: 24.9360\n",
      "Iteration 300/6975, Loss: 27.2104\n",
      "Iteration 400/6975, Loss: 28.7770\n",
      "Iteration 500/6975, Loss: 26.8135\n",
      "Iteration 600/6975, Loss: 26.9121\n",
      "Iteration 700/6975, Loss: 30.2625\n",
      "Iteration 800/6975, Loss: 33.8592\n",
      "Iteration 900/6975, Loss: 32.6055\n",
      "Iteration 1000/6975, Loss: 34.1559\n",
      "Iteration 1100/6975, Loss: 37.5536\n",
      "Iteration 1200/6975, Loss: 34.5495\n",
      "Iteration 1300/6975, Loss: 39.3907\n",
      "Iteration 1400/6975, Loss: 40.5982\n",
      "Iteration 1500/6975, Loss: 42.7353\n",
      "Iteration 1600/6975, Loss: 37.2754\n",
      "Iteration 1700/6975, Loss: 41.2419\n",
      "Iteration 1800/6975, Loss: 44.0689\n",
      "Iteration 1900/6975, Loss: 43.6412\n",
      "Iteration 2000/6975, Loss: 51.2764\n",
      "Iteration 2100/6975, Loss: 47.9653\n",
      "Iteration 2200/6975, Loss: 48.4660\n",
      "Iteration 2300/6975, Loss: 57.1276\n",
      "Iteration 2400/6975, Loss: 56.2278\n",
      "Iteration 2500/6975, Loss: 52.7684\n",
      "Iteration 2600/6975, Loss: 58.4100\n",
      "Iteration 2700/6975, Loss: 60.8552\n",
      "Iteration 2800/6975, Loss: 60.5705\n",
      "Iteration 2900/6975, Loss: 64.3452\n",
      "Iteration 3000/6975, Loss: 64.8754\n",
      "Iteration 3100/6975, Loss: 65.7330\n",
      "Iteration 3200/6975, Loss: 69.8302\n",
      "Iteration 3300/6975, Loss: 68.5027\n",
      "Iteration 3400/6975, Loss: 68.9421\n",
      "Iteration 3500/6975, Loss: 68.0348\n",
      "Iteration 3600/6975, Loss: 71.1381\n",
      "Iteration 3700/6975, Loss: 77.4435\n",
      "Iteration 3800/6975, Loss: 77.4228\n",
      "Iteration 3900/6975, Loss: 84.1664\n",
      "Iteration 4000/6975, Loss: 74.9082\n",
      "Iteration 4100/6975, Loss: 79.8229\n",
      "Iteration 4200/6975, Loss: 78.8349\n",
      "Iteration 4300/6975, Loss: 75.3319\n",
      "Iteration 4400/6975, Loss: 85.7801\n",
      "Iteration 4500/6975, Loss: 88.1857\n",
      "Iteration 4600/6975, Loss: 86.0586\n",
      "Iteration 4700/6975, Loss: 85.3321\n",
      "Iteration 4800/6975, Loss: 86.5840\n",
      "Iteration 4900/6975, Loss: 88.0600\n",
      "Iteration 5000/6975, Loss: 91.9701\n",
      "Iteration 5100/6975, Loss: 87.7852\n",
      "Iteration 5200/6975, Loss: 89.3776\n",
      "Iteration 5300/6975, Loss: 89.2609\n",
      "Iteration 5400/6975, Loss: 93.8225\n",
      "Iteration 5500/6975, Loss: 99.2652\n",
      "Iteration 5600/6975, Loss: 98.9199\n",
      "Iteration 5700/6975, Loss: 102.2808\n",
      "Iteration 5800/6975, Loss: 99.5340\n",
      "Iteration 5900/6975, Loss: 98.6531\n",
      "Iteration 6000/6975, Loss: 97.2297\n",
      "Iteration 6100/6975, Loss: 101.8944\n",
      "Iteration 6200/6975, Loss: 101.4887\n",
      "Iteration 6300/6975, Loss: 105.8430\n",
      "Iteration 6400/6975, Loss: 104.7119\n",
      "Iteration 6500/6975, Loss: 105.0347\n",
      "Iteration 6600/6975, Loss: 102.0149\n",
      "Iteration 6700/6975, Loss: 102.9514\n",
      "Iteration 6800/6975, Loss: 108.6941\n",
      "Iteration 6900/6975, Loss: 105.4046\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 29.1167\n",
      "Iteration 200/3450, Loss: 32.1393\n",
      "Iteration 300/3450, Loss: 27.4811\n",
      "Iteration 400/3450, Loss: 32.1827\n",
      "Iteration 500/3450, Loss: 41.9908\n",
      "Iteration 600/3450, Loss: 40.6454\n",
      "Iteration 700/3450, Loss: 52.3020\n",
      "Iteration 800/3450, Loss: 50.0527\n",
      "Iteration 900/3450, Loss: 49.5697\n",
      "Iteration 1000/3450, Loss: 60.5979\n",
      "Iteration 1100/3450, Loss: 62.7844\n",
      "Iteration 1200/3450, Loss: 69.5215\n",
      "Iteration 1300/3450, Loss: 72.0663\n",
      "Iteration 1400/3450, Loss: 77.3293\n",
      "Iteration 1500/3450, Loss: 78.8484\n",
      "Iteration 1600/3450, Loss: 85.5111\n",
      "Iteration 1700/3450, Loss: 84.8909\n",
      "Iteration 1800/3450, Loss: 92.3252\n",
      "Iteration 1900/3450, Loss: 96.3940\n",
      "Iteration 2000/3450, Loss: 96.5400\n",
      "Iteration 2100/3450, Loss: 102.3902\n",
      "Iteration 2200/3450, Loss: 113.5546\n",
      "Iteration 2300/3450, Loss: 110.2503\n",
      "Iteration 2400/3450, Loss: 112.6970\n",
      "Iteration 2500/3450, Loss: 118.1143\n",
      "Iteration 2600/3450, Loss: 118.4572\n",
      "Iteration 2700/3450, Loss: 121.2507\n",
      "Iteration 2800/3450, Loss: 124.3106\n",
      "Iteration 2900/3450, Loss: 125.3502\n",
      "Iteration 3000/3450, Loss: 132.3476\n",
      "Iteration 3100/3450, Loss: 130.9006\n",
      "Iteration 3200/3450, Loss: 133.3705\n",
      "Iteration 3300/3450, Loss: 134.9957\n",
      "Iteration 3400/3450, Loss: 139.9448\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 38.5103\n",
      "Iteration 200/1725, Loss: 42.6871\n",
      "Iteration 300/1725, Loss: 59.8346\n",
      "Iteration 400/1725, Loss: 72.5085\n",
      "Iteration 500/1725, Loss: 84.3395\n",
      "Iteration 600/1725, Loss: 105.6013\n",
      "Iteration 700/1725, Loss: 115.2721\n",
      "Iteration 800/1725, Loss: 127.6158\n",
      "Iteration 900/1725, Loss: 144.9362\n",
      "Iteration 1000/1725, Loss: 151.4638\n",
      "Iteration 1100/1725, Loss: 166.8503\n",
      "Iteration 1200/1725, Loss: 177.5457\n",
      "Iteration 1300/1725, Loss: 188.1308\n",
      "Iteration 1400/1725, Loss: 200.9235\n",
      "Iteration 1500/1725, Loss: 207.5561\n",
      "Iteration 1600/1725, Loss: 218.1799\n",
      "Iteration 1700/1725, Loss: 225.3246\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 67.0479\n",
      "Iteration 200/825, Loss: 119.0653\n",
      "Iteration 300/825, Loss: 165.9279\n",
      "Iteration 400/825, Loss: 217.0115\n",
      "Iteration 500/825, Loss: 262.9525\n",
      "Iteration 600/825, Loss: 317.3585\n",
      "Iteration 700/825, Loss: 368.3984\n",
      "Iteration 800/825, Loss: 420.8069\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 184.5518\n",
      "Iteration 200/375, Loss: 371.4978\n",
      "Iteration 300/375, Loss: 566.8778\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 632.7961\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 35.1969\n",
      "Iteration 200/28125, Loss: 35.9690\n",
      "Iteration 300/28125, Loss: 39.5815\n",
      "Iteration 400/28125, Loss: 43.2657\n",
      "Iteration 500/28125, Loss: 38.9649\n",
      "Iteration 600/28125, Loss: 38.3739\n",
      "Iteration 700/28125, Loss: 43.4197\n",
      "Iteration 800/28125, Loss: 55.1124\n",
      "Iteration 900/28125, Loss: 55.9895\n",
      "Iteration 1000/28125, Loss: 49.4894\n",
      "Iteration 1100/28125, Loss: 47.7405\n",
      "Iteration 1200/28125, Loss: 50.2999\n",
      "Iteration 1300/28125, Loss: 75.2246\n",
      "Iteration 1400/28125, Loss: 60.4723\n",
      "Iteration 1500/28125, Loss: 72.0139\n",
      "Iteration 1600/28125, Loss: 69.0549\n",
      "Iteration 1700/28125, Loss: 64.8809\n",
      "Iteration 1800/28125, Loss: 75.2942\n",
      "Iteration 1900/28125, Loss: 83.6744\n",
      "Iteration 2000/28125, Loss: 81.5562\n",
      "Iteration 2100/28125, Loss: 79.6832\n",
      "Iteration 2200/28125, Loss: 78.5944\n",
      "Iteration 2300/28125, Loss: 89.9608\n",
      "Iteration 2400/28125, Loss: 86.2463\n",
      "Iteration 2500/28125, Loss: 86.6989\n",
      "Iteration 2600/28125, Loss: 98.1107\n",
      "Iteration 2700/28125, Loss: 99.3032\n",
      "Iteration 2800/28125, Loss: 106.5285\n",
      "Iteration 2900/28125, Loss: 100.4242\n",
      "Iteration 3000/28125, Loss: 99.6821\n",
      "Iteration 3100/28125, Loss: 109.0333\n",
      "Iteration 3200/28125, Loss: 107.9375\n",
      "Iteration 3300/28125, Loss: 97.5921\n",
      "Iteration 3400/28125, Loss: 117.9607\n",
      "Iteration 3500/28125, Loss: 107.2952\n",
      "Iteration 3600/28125, Loss: 125.8856\n",
      "Iteration 3700/28125, Loss: 118.7974\n",
      "Iteration 3800/28125, Loss: 116.2979\n",
      "Iteration 3900/28125, Loss: 118.7566\n",
      "Iteration 4000/28125, Loss: 131.5170\n",
      "Iteration 4100/28125, Loss: 118.1593\n",
      "Iteration 4200/28125, Loss: 135.1988\n",
      "Iteration 4300/28125, Loss: 131.6663\n",
      "Iteration 4400/28125, Loss: 147.1619\n",
      "Iteration 4500/28125, Loss: 136.1292\n",
      "Iteration 4600/28125, Loss: 145.9016\n",
      "Iteration 4700/28125, Loss: 153.2021\n",
      "Iteration 4800/28125, Loss: 139.7436\n",
      "Iteration 4900/28125, Loss: 144.0584\n",
      "Iteration 5000/28125, Loss: 150.7108\n",
      "Iteration 5100/28125, Loss: 145.6417\n",
      "Iteration 5200/28125, Loss: 154.3901\n",
      "Iteration 5300/28125, Loss: 147.4404\n",
      "Iteration 5400/28125, Loss: 149.4204\n",
      "Iteration 5500/28125, Loss: 163.4054\n",
      "Iteration 5600/28125, Loss: 164.1763\n",
      "Iteration 5700/28125, Loss: 158.4107\n",
      "Iteration 5800/28125, Loss: 160.8345\n",
      "Iteration 5900/28125, Loss: 148.0203\n",
      "Iteration 6000/28125, Loss: 161.4427\n",
      "Iteration 6100/28125, Loss: 161.9198\n",
      "Iteration 6200/28125, Loss: 161.4430\n",
      "Iteration 6300/28125, Loss: 167.4430\n",
      "Iteration 6400/28125, Loss: 159.9168\n",
      "Iteration 6500/28125, Loss: 167.4489\n",
      "Iteration 6600/28125, Loss: 165.9595\n",
      "Iteration 6700/28125, Loss: 175.6931\n",
      "Iteration 6800/28125, Loss: 188.7878\n",
      "Iteration 6900/28125, Loss: 178.9094\n",
      "Iteration 7000/28125, Loss: 176.2831\n",
      "Iteration 7100/28125, Loss: 172.8723\n",
      "Iteration 7200/28125, Loss: 176.1684\n",
      "Iteration 7300/28125, Loss: 193.0196\n",
      "Iteration 7400/28125, Loss: 176.8530\n",
      "Iteration 7500/28125, Loss: 183.0234\n",
      "Iteration 7600/28125, Loss: 181.6130\n",
      "Iteration 7700/28125, Loss: 198.2615\n",
      "Iteration 7800/28125, Loss: 192.1231\n",
      "Iteration 7900/28125, Loss: 185.9709\n",
      "Iteration 8000/28125, Loss: 187.2422\n",
      "Iteration 8100/28125, Loss: 180.2611\n",
      "Iteration 8200/28125, Loss: 201.9110\n",
      "Iteration 8300/28125, Loss: 193.0564\n",
      "Iteration 8400/28125, Loss: 192.0622\n",
      "Iteration 8500/28125, Loss: 189.2152\n",
      "Iteration 8600/28125, Loss: 204.4325\n",
      "Iteration 8700/28125, Loss: 188.1013\n",
      "Iteration 8800/28125, Loss: 197.4897\n",
      "Iteration 8900/28125, Loss: 206.9543\n",
      "Iteration 9000/28125, Loss: 204.7878\n",
      "Iteration 9100/28125, Loss: 195.9337\n",
      "Iteration 9200/28125, Loss: 201.7171\n",
      "Iteration 9300/28125, Loss: 206.5855\n",
      "Iteration 9400/28125, Loss: 200.4062\n",
      "Iteration 9500/28125, Loss: 195.0857\n",
      "Iteration 9600/28125, Loss: 196.0270\n",
      "Iteration 9700/28125, Loss: 207.9903\n",
      "Iteration 9800/28125, Loss: 207.1142\n",
      "Iteration 9900/28125, Loss: 203.6871\n",
      "Iteration 10000/28125, Loss: 199.9148\n",
      "Iteration 10100/28125, Loss: 206.6911\n",
      "Iteration 10200/28125, Loss: 210.7440\n",
      "Iteration 10300/28125, Loss: 207.3350\n",
      "Iteration 10400/28125, Loss: 214.7719\n",
      "Iteration 10500/28125, Loss: 216.7766\n",
      "Iteration 10600/28125, Loss: 212.1418\n",
      "Iteration 10700/28125, Loss: 226.0532\n",
      "Iteration 10800/28125, Loss: 222.3572\n",
      "Iteration 10900/28125, Loss: 215.0443\n",
      "Iteration 11000/28125, Loss: 214.6818\n",
      "Iteration 11100/28125, Loss: 211.4880\n",
      "Iteration 11200/28125, Loss: 208.7755\n",
      "Iteration 11300/28125, Loss: 212.6425\n",
      "Iteration 11400/28125, Loss: 223.1490\n",
      "Iteration 11500/28125, Loss: 217.8540\n",
      "Iteration 11600/28125, Loss: 218.4807\n",
      "Iteration 11700/28125, Loss: 219.8458\n",
      "Iteration 11800/28125, Loss: 224.8225\n",
      "Iteration 11900/28125, Loss: 216.9266\n",
      "Iteration 12000/28125, Loss: 218.9792\n",
      "Iteration 12100/28125, Loss: 230.4302\n",
      "Iteration 12200/28125, Loss: 222.3033\n",
      "Iteration 12300/28125, Loss: 222.4518\n",
      "Iteration 12400/28125, Loss: 227.1944\n",
      "Iteration 12500/28125, Loss: 224.9017\n",
      "Iteration 12600/28125, Loss: 220.9546\n",
      "Iteration 12700/28125, Loss: 228.9905\n",
      "Iteration 12800/28125, Loss: 227.2130\n",
      "Iteration 12900/28125, Loss: 219.7996\n",
      "Iteration 13000/28125, Loss: 225.0572\n",
      "Iteration 13100/28125, Loss: 220.8374\n",
      "Iteration 13200/28125, Loss: 228.6384\n",
      "Iteration 13300/28125, Loss: 218.0666\n",
      "Iteration 13400/28125, Loss: 235.6710\n",
      "Iteration 13500/28125, Loss: 223.7427\n",
      "Iteration 13600/28125, Loss: 224.9233\n",
      "Iteration 13700/28125, Loss: 221.3160\n",
      "Iteration 13800/28125, Loss: 220.1620\n",
      "Iteration 13900/28125, Loss: 220.5228\n",
      "Iteration 14000/28125, Loss: 225.3387\n",
      "Iteration 14100/28125, Loss: 221.8557\n",
      "Iteration 14200/28125, Loss: 230.9551\n",
      "Iteration 14300/28125, Loss: 224.6471\n",
      "Iteration 14400/28125, Loss: 240.1229\n",
      "Iteration 14500/28125, Loss: 236.6688\n",
      "Iteration 14600/28125, Loss: 231.8492\n",
      "Iteration 14700/28125, Loss: 233.8964\n",
      "Iteration 14800/28125, Loss: 242.6293\n",
      "Iteration 14900/28125, Loss: 239.2899\n",
      "Iteration 15000/28125, Loss: 241.9627\n",
      "Iteration 15100/28125, Loss: 238.3996\n",
      "Iteration 15200/28125, Loss: 233.7722\n",
      "Iteration 15300/28125, Loss: 233.9033\n",
      "Iteration 15400/28125, Loss: 231.0512\n",
      "Iteration 15500/28125, Loss: 225.8095\n",
      "Iteration 15600/28125, Loss: 234.4731\n",
      "Iteration 15700/28125, Loss: 226.1987\n",
      "Iteration 15800/28125, Loss: 239.3449\n",
      "Iteration 15900/28125, Loss: 231.6312\n",
      "Iteration 16000/28125, Loss: 233.2764\n",
      "Iteration 16100/28125, Loss: 231.7939\n",
      "Iteration 16200/28125, Loss: 240.6717\n",
      "Iteration 16300/28125, Loss: 223.0882\n",
      "Iteration 16400/28125, Loss: 228.9928\n",
      "Iteration 16500/28125, Loss: 241.9115\n",
      "Iteration 16600/28125, Loss: 236.9879\n",
      "Iteration 16700/28125, Loss: 229.7498\n",
      "Iteration 16800/28125, Loss: 242.8168\n",
      "Iteration 16900/28125, Loss: 239.5594\n",
      "Iteration 17000/28125, Loss: 232.1435\n",
      "Iteration 17100/28125, Loss: 227.6099\n",
      "Iteration 17200/28125, Loss: 238.3151\n",
      "Iteration 17300/28125, Loss: 236.1410\n",
      "Iteration 17400/28125, Loss: 236.1215\n",
      "Iteration 17500/28125, Loss: 236.7098\n",
      "Iteration 17600/28125, Loss: 232.1865\n",
      "Iteration 17700/28125, Loss: 240.8061\n",
      "Iteration 17800/28125, Loss: 236.6578\n",
      "Iteration 17900/28125, Loss: 241.1442\n",
      "Iteration 18000/28125, Loss: 241.2234\n",
      "Iteration 18100/28125, Loss: 237.0596\n",
      "Iteration 18200/28125, Loss: 241.7169\n",
      "Iteration 18300/28125, Loss: 238.7356\n",
      "Iteration 18400/28125, Loss: 242.5672\n",
      "Iteration 18500/28125, Loss: 237.9172\n",
      "Iteration 18600/28125, Loss: 236.6016\n",
      "Iteration 18700/28125, Loss: 229.5257\n",
      "Iteration 18800/28125, Loss: 238.3198\n",
      "Iteration 18900/28125, Loss: 236.6074\n",
      "Iteration 19000/28125, Loss: 250.8577\n",
      "Iteration 19100/28125, Loss: 238.2842\n",
      "Iteration 19200/28125, Loss: 244.3152\n",
      "Iteration 19300/28125, Loss: 229.1476\n",
      "Iteration 19400/28125, Loss: 249.1493\n",
      "Iteration 19500/28125, Loss: 239.1973\n",
      "Iteration 19600/28125, Loss: 235.4164\n",
      "Iteration 19700/28125, Loss: 238.7391\n",
      "Iteration 19800/28125, Loss: 235.5408\n",
      "Iteration 19900/28125, Loss: 240.5038\n",
      "Iteration 20000/28125, Loss: 245.2323\n",
      "Iteration 20100/28125, Loss: 239.4476\n",
      "Iteration 20200/28125, Loss: 236.7732\n",
      "Iteration 20300/28125, Loss: 229.4497\n",
      "Iteration 20400/28125, Loss: 240.9854\n",
      "Iteration 20500/28125, Loss: 231.9084\n",
      "Iteration 20600/28125, Loss: 233.2091\n",
      "Iteration 20700/28125, Loss: 244.2376\n",
      "Iteration 20800/28125, Loss: 232.5625\n",
      "Iteration 20900/28125, Loss: 241.6921\n",
      "Iteration 21000/28125, Loss: 241.9682\n",
      "Iteration 21100/28125, Loss: 232.9513\n",
      "Iteration 21200/28125, Loss: 237.2376\n",
      "Iteration 21300/28125, Loss: 245.6334\n",
      "Iteration 21400/28125, Loss: 229.6977\n",
      "Iteration 21500/28125, Loss: 240.6847\n",
      "Iteration 21600/28125, Loss: 258.0458\n",
      "Iteration 21700/28125, Loss: 237.0160\n",
      "Iteration 21800/28125, Loss: 241.4995\n",
      "Iteration 21900/28125, Loss: 233.6192\n",
      "Iteration 22000/28125, Loss: 234.0647\n",
      "Iteration 22100/28125, Loss: 243.8879\n",
      "Iteration 22200/28125, Loss: 238.3494\n",
      "Iteration 22300/28125, Loss: 238.0381\n",
      "Iteration 22400/28125, Loss: 242.2368\n",
      "Iteration 22500/28125, Loss: 254.7288\n",
      "Iteration 22600/28125, Loss: 242.3654\n",
      "Iteration 22700/28125, Loss: 237.8194\n",
      "Iteration 22800/28125, Loss: 243.5592\n",
      "Iteration 22900/28125, Loss: 255.2628\n",
      "Iteration 23000/28125, Loss: 237.8625\n",
      "Iteration 23100/28125, Loss: 241.7674\n",
      "Iteration 23200/28125, Loss: 232.6030\n",
      "Iteration 23300/28125, Loss: 241.3788\n",
      "Iteration 23400/28125, Loss: 232.3997\n",
      "Iteration 23500/28125, Loss: 233.7429\n",
      "Iteration 23600/28125, Loss: 251.0253\n",
      "Iteration 23700/28125, Loss: 246.3579\n",
      "Iteration 23800/28125, Loss: 238.0693\n",
      "Iteration 23900/28125, Loss: 242.1354\n",
      "Iteration 24000/28125, Loss: 244.5199\n",
      "Iteration 24100/28125, Loss: 233.7924\n",
      "Iteration 24200/28125, Loss: 237.8732\n",
      "Iteration 24300/28125, Loss: 242.0039\n",
      "Iteration 24400/28125, Loss: 229.4091\n",
      "Iteration 24500/28125, Loss: 251.4948\n",
      "Iteration 24600/28125, Loss: 251.4191\n",
      "Iteration 24700/28125, Loss: 238.5740\n",
      "Iteration 24800/28125, Loss: 243.3979\n",
      "Iteration 24900/28125, Loss: 237.2293\n",
      "Iteration 25000/28125, Loss: 243.3365\n",
      "Iteration 25100/28125, Loss: 243.0770\n",
      "Iteration 25200/28125, Loss: 243.1005\n",
      "Iteration 25300/28125, Loss: 235.0535\n",
      "Iteration 25400/28125, Loss: 251.6915\n",
      "Iteration 25500/28125, Loss: 235.1094\n",
      "Iteration 25600/28125, Loss: 245.3163\n",
      "Iteration 25700/28125, Loss: 244.4564\n",
      "Iteration 25800/28125, Loss: 238.2081\n",
      "Iteration 25900/28125, Loss: 243.3917\n",
      "Iteration 26000/28125, Loss: 243.6906\n",
      "Iteration 26100/28125, Loss: 242.2970\n",
      "Iteration 26200/28125, Loss: 241.5423\n",
      "Iteration 26300/28125, Loss: 249.5115\n",
      "Iteration 26400/28125, Loss: 235.3289\n",
      "Iteration 26500/28125, Loss: 239.5882\n",
      "Iteration 26600/28125, Loss: 246.2094\n",
      "Iteration 26700/28125, Loss: 243.9115\n",
      "Iteration 26800/28125, Loss: 243.8887\n",
      "Iteration 26900/28125, Loss: 258.0550\n",
      "Iteration 27000/28125, Loss: 256.8175\n",
      "Iteration 27100/28125, Loss: 248.8080\n",
      "Iteration 27200/28125, Loss: 244.4842\n",
      "Iteration 27300/28125, Loss: 239.1855\n",
      "Iteration 27400/28125, Loss: 239.3509\n",
      "Iteration 27500/28125, Loss: 248.1860\n",
      "Iteration 27600/28125, Loss: 235.2836\n",
      "Iteration 27700/28125, Loss: 248.1279\n",
      "Iteration 27800/28125, Loss: 235.3238\n",
      "Iteration 27900/28125, Loss: 239.4431\n",
      "Iteration 28000/28125, Loss: 239.7141\n",
      "Iteration 28100/28125, Loss: 243.8516\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 28.1300\n",
      "Iteration 200/14025, Loss: 40.1190\n",
      "Iteration 300/14025, Loss: 43.4041\n",
      "Iteration 400/14025, Loss: 55.3293\n",
      "Iteration 500/14025, Loss: 56.5385\n",
      "Iteration 600/14025, Loss: 67.2441\n",
      "Iteration 700/14025, Loss: 74.3462\n",
      "Iteration 800/14025, Loss: 80.9600\n",
      "Iteration 900/14025, Loss: 83.6200\n",
      "Iteration 1000/14025, Loss: 98.7992\n",
      "Iteration 1100/14025, Loss: 92.6141\n",
      "Iteration 1200/14025, Loss: 106.6263\n",
      "Iteration 1300/14025, Loss: 106.4465\n",
      "Iteration 1400/14025, Loss: 120.4986\n",
      "Iteration 1500/14025, Loss: 127.4760\n",
      "Iteration 1600/14025, Loss: 128.1745\n",
      "Iteration 1700/14025, Loss: 138.7936\n",
      "Iteration 1800/14025, Loss: 145.1134\n",
      "Iteration 1900/14025, Loss: 147.5095\n",
      "Iteration 2000/14025, Loss: 155.1708\n",
      "Iteration 2100/14025, Loss: 159.5973\n",
      "Iteration 2200/14025, Loss: 166.9891\n",
      "Iteration 2300/14025, Loss: 174.1956\n",
      "Iteration 2400/14025, Loss: 179.6604\n",
      "Iteration 2500/14025, Loss: 186.2836\n",
      "Iteration 2600/14025, Loss: 193.0419\n",
      "Iteration 2700/14025, Loss: 195.1428\n",
      "Iteration 2800/14025, Loss: 199.8964\n",
      "Iteration 2900/14025, Loss: 202.2632\n",
      "Iteration 3000/14025, Loss: 201.3265\n",
      "Iteration 3100/14025, Loss: 216.0520\n",
      "Iteration 3200/14025, Loss: 216.8131\n",
      "Iteration 3300/14025, Loss: 218.1429\n",
      "Iteration 3400/14025, Loss: 232.6022\n",
      "Iteration 3500/14025, Loss: 230.2591\n",
      "Iteration 3600/14025, Loss: 228.8253\n",
      "Iteration 3700/14025, Loss: 244.3116\n",
      "Iteration 3800/14025, Loss: 245.0554\n",
      "Iteration 3900/14025, Loss: 248.1712\n",
      "Iteration 4000/14025, Loss: 259.1457\n",
      "Iteration 4100/14025, Loss: 251.9817\n",
      "Iteration 4200/14025, Loss: 263.4779\n",
      "Iteration 4300/14025, Loss: 268.4497\n",
      "Iteration 4400/14025, Loss: 266.3644\n",
      "Iteration 4500/14025, Loss: 272.3042\n",
      "Iteration 4600/14025, Loss: 277.4547\n",
      "Iteration 4700/14025, Loss: 282.4594\n",
      "Iteration 4800/14025, Loss: 280.3586\n",
      "Iteration 4900/14025, Loss: 282.3129\n",
      "Iteration 5000/14025, Loss: 292.9121\n",
      "Iteration 5100/14025, Loss: 293.5430\n",
      "Iteration 5200/14025, Loss: 298.8348\n",
      "Iteration 5300/14025, Loss: 309.1912\n",
      "Iteration 5400/14025, Loss: 308.0953\n",
      "Iteration 5500/14025, Loss: 310.6192\n",
      "Iteration 5600/14025, Loss: 315.6056\n",
      "Iteration 5700/14025, Loss: 314.7602\n",
      "Iteration 5800/14025, Loss: 318.0261\n",
      "Iteration 5900/14025, Loss: 315.9227\n",
      "Iteration 6000/14025, Loss: 324.8965\n",
      "Iteration 6100/14025, Loss: 329.3124\n",
      "Iteration 6200/14025, Loss: 336.3633\n",
      "Iteration 6300/14025, Loss: 323.3202\n",
      "Iteration 6400/14025, Loss: 333.8685\n",
      "Iteration 6500/14025, Loss: 335.4092\n",
      "Iteration 6600/14025, Loss: 341.3975\n",
      "Iteration 6700/14025, Loss: 337.3969\n",
      "Iteration 6800/14025, Loss: 349.5865\n",
      "Iteration 6900/14025, Loss: 345.1953\n",
      "Iteration 7000/14025, Loss: 350.2854\n",
      "Iteration 7100/14025, Loss: 342.6851\n",
      "Iteration 7200/14025, Loss: 361.6789\n",
      "Iteration 7300/14025, Loss: 350.7468\n",
      "Iteration 7400/14025, Loss: 363.6246\n",
      "Iteration 7500/14025, Loss: 367.6106\n",
      "Iteration 7600/14025, Loss: 361.8867\n",
      "Iteration 7700/14025, Loss: 368.3665\n",
      "Iteration 7800/14025, Loss: 369.0676\n",
      "Iteration 7900/14025, Loss: 372.6794\n",
      "Iteration 8000/14025, Loss: 368.8211\n",
      "Iteration 8100/14025, Loss: 373.3417\n",
      "Iteration 8200/14025, Loss: 376.6335\n",
      "Iteration 8300/14025, Loss: 378.2854\n",
      "Iteration 8400/14025, Loss: 380.4197\n",
      "Iteration 8500/14025, Loss: 373.8576\n",
      "Iteration 8600/14025, Loss: 379.1715\n",
      "Iteration 8700/14025, Loss: 372.6259\n",
      "Iteration 8800/14025, Loss: 380.7408\n",
      "Iteration 8900/14025, Loss: 371.3980\n",
      "Iteration 9000/14025, Loss: 379.8471\n",
      "Iteration 9100/14025, Loss: 387.4726\n",
      "Iteration 9200/14025, Loss: 389.4011\n",
      "Iteration 9300/14025, Loss: 388.7327\n",
      "Iteration 9400/14025, Loss: 382.1766\n",
      "Iteration 9500/14025, Loss: 391.5982\n",
      "Iteration 9600/14025, Loss: 395.3410\n",
      "Iteration 9700/14025, Loss: 388.5334\n",
      "Iteration 9800/14025, Loss: 388.7741\n",
      "Iteration 9900/14025, Loss: 396.2943\n",
      "Iteration 10000/14025, Loss: 396.6056\n",
      "Iteration 10100/14025, Loss: 397.6594\n",
      "Iteration 10200/14025, Loss: 396.3823\n",
      "Iteration 10300/14025, Loss: 398.8319\n",
      "Iteration 10400/14025, Loss: 394.2047\n",
      "Iteration 10500/14025, Loss: 394.6647\n",
      "Iteration 10600/14025, Loss: 398.0812\n",
      "Iteration 10700/14025, Loss: 394.3892\n",
      "Iteration 10800/14025, Loss: 406.3657\n",
      "Iteration 10900/14025, Loss: 396.4147\n",
      "Iteration 11000/14025, Loss: 405.8603\n",
      "Iteration 11100/14025, Loss: 403.2201\n",
      "Iteration 11200/14025, Loss: 412.2781\n",
      "Iteration 11300/14025, Loss: 407.8933\n",
      "Iteration 11400/14025, Loss: 409.3108\n",
      "Iteration 11500/14025, Loss: 409.7687\n",
      "Iteration 11600/14025, Loss: 405.8328\n",
      "Iteration 11700/14025, Loss: 399.8381\n",
      "Iteration 11800/14025, Loss: 404.7706\n",
      "Iteration 11900/14025, Loss: 408.6701\n",
      "Iteration 12000/14025, Loss: 418.3156\n",
      "Iteration 12100/14025, Loss: 410.6379\n",
      "Iteration 12200/14025, Loss: 417.6950\n",
      "Iteration 12300/14025, Loss: 418.3760\n",
      "Iteration 12400/14025, Loss: 413.0578\n",
      "Iteration 12500/14025, Loss: 415.0678\n",
      "Iteration 12600/14025, Loss: 418.3926\n",
      "Iteration 12700/14025, Loss: 416.6361\n",
      "Iteration 12800/14025, Loss: 420.3943\n",
      "Iteration 12900/14025, Loss: 423.4514\n",
      "Iteration 13000/14025, Loss: 422.8727\n",
      "Iteration 13100/14025, Loss: 418.1024\n",
      "Iteration 13200/14025, Loss: 414.0371\n",
      "Iteration 13300/14025, Loss: 417.9099\n",
      "Iteration 13400/14025, Loss: 421.3899\n",
      "Iteration 13500/14025, Loss: 417.8367\n",
      "Iteration 13600/14025, Loss: 421.0030\n",
      "Iteration 13700/14025, Loss: 427.2092\n",
      "Iteration 13800/14025, Loss: 417.7960\n",
      "Iteration 13900/14025, Loss: 418.2693\n",
      "Iteration 14000/14025, Loss: 423.0733\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 38.8896\n",
      "Iteration 200/6975, Loss: 57.3686\n",
      "Iteration 300/6975, Loss: 69.3276\n",
      "Iteration 400/6975, Loss: 89.5725\n",
      "Iteration 500/6975, Loss: 101.8099\n",
      "Iteration 600/6975, Loss: 117.2719\n",
      "Iteration 700/6975, Loss: 140.1451\n",
      "Iteration 800/6975, Loss: 152.2385\n",
      "Iteration 900/6975, Loss: 168.9657\n",
      "Iteration 1000/6975, Loss: 187.9012\n",
      "Iteration 1100/6975, Loss: 207.0375\n",
      "Iteration 1200/6975, Loss: 216.0133\n",
      "Iteration 1300/6975, Loss: 230.3758\n",
      "Iteration 1400/6975, Loss: 245.3000\n",
      "Iteration 1500/6975, Loss: 263.5576\n",
      "Iteration 1600/6975, Loss: 272.8209\n",
      "Iteration 1700/6975, Loss: 291.3684\n",
      "Iteration 1800/6975, Loss: 303.5464\n",
      "Iteration 1900/6975, Loss: 315.6923\n",
      "Iteration 2000/6975, Loss: 326.4745\n",
      "Iteration 2100/6975, Loss: 341.0631\n",
      "Iteration 2200/6975, Loss: 351.6761\n",
      "Iteration 2300/6975, Loss: 370.7095\n",
      "Iteration 2400/6975, Loss: 382.9230\n",
      "Iteration 2500/6975, Loss: 391.1198\n",
      "Iteration 2600/6975, Loss: 404.4337\n",
      "Iteration 2700/6975, Loss: 417.6833\n",
      "Iteration 2800/6975, Loss: 430.2271\n",
      "Iteration 2900/6975, Loss: 441.2245\n",
      "Iteration 3000/6975, Loss: 449.0274\n",
      "Iteration 3100/6975, Loss: 462.9460\n",
      "Iteration 3200/6975, Loss: 466.4960\n",
      "Iteration 3300/6975, Loss: 484.1028\n",
      "Iteration 3400/6975, Loss: 489.3513\n",
      "Iteration 3500/6975, Loss: 493.1894\n",
      "Iteration 3600/6975, Loss: 505.9436\n",
      "Iteration 3700/6975, Loss: 521.3711\n",
      "Iteration 3800/6975, Loss: 526.6659\n",
      "Iteration 3900/6975, Loss: 538.9523\n",
      "Iteration 4000/6975, Loss: 538.5001\n",
      "Iteration 4100/6975, Loss: 551.6843\n",
      "Iteration 4200/6975, Loss: 550.6545\n",
      "Iteration 4300/6975, Loss: 552.8652\n",
      "Iteration 4400/6975, Loss: 565.5600\n",
      "Iteration 4500/6975, Loss: 576.3179\n",
      "Iteration 4600/6975, Loss: 576.7507\n",
      "Iteration 4700/6975, Loss: 586.5628\n",
      "Iteration 4800/6975, Loss: 594.2363\n",
      "Iteration 4900/6975, Loss: 593.6471\n",
      "Iteration 5000/6975, Loss: 607.2683\n",
      "Iteration 5100/6975, Loss: 608.2057\n",
      "Iteration 5200/6975, Loss: 611.8974\n",
      "Iteration 5300/6975, Loss: 622.7149\n",
      "Iteration 5400/6975, Loss: 625.4075\n",
      "Iteration 5500/6975, Loss: 632.6625\n",
      "Iteration 5600/6975, Loss: 638.3939\n",
      "Iteration 5700/6975, Loss: 644.0405\n",
      "Iteration 5800/6975, Loss: 643.4582\n",
      "Iteration 5900/6975, Loss: 651.2019\n",
      "Iteration 6000/6975, Loss: 652.5777\n",
      "Iteration 6100/6975, Loss: 657.6183\n",
      "Iteration 6200/6975, Loss: 663.1142\n",
      "Iteration 6300/6975, Loss: 669.1790\n",
      "Iteration 6400/6975, Loss: 671.4991\n",
      "Iteration 6500/6975, Loss: 670.1447\n",
      "Iteration 6600/6975, Loss: 672.4335\n",
      "Iteration 6700/6975, Loss: 674.2650\n",
      "Iteration 6800/6975, Loss: 682.5588\n",
      "Iteration 6900/6975, Loss: 674.5951\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 73.2501\n",
      "Iteration 200/3450, Loss: 109.6707\n",
      "Iteration 300/3450, Loss: 148.1763\n",
      "Iteration 400/3450, Loss: 190.3484\n",
      "Iteration 500/3450, Loss: 238.5619\n",
      "Iteration 600/3450, Loss: 278.9241\n",
      "Iteration 700/3450, Loss: 330.9728\n",
      "Iteration 800/3450, Loss: 372.3321\n",
      "Iteration 900/3450, Loss: 410.2094\n",
      "Iteration 1000/3450, Loss: 452.5703\n",
      "Iteration 1100/3450, Loss: 488.0126\n",
      "Iteration 1200/3450, Loss: 530.8774\n",
      "Iteration 1300/3450, Loss: 567.6704\n",
      "Iteration 1400/3450, Loss: 602.9576\n",
      "Iteration 1500/3450, Loss: 640.6186\n",
      "Iteration 1600/3450, Loss: 677.6022\n",
      "Iteration 1700/3450, Loss: 709.4756\n",
      "Iteration 1800/3450, Loss: 750.3171\n",
      "Iteration 1900/3450, Loss: 774.7597\n",
      "Iteration 2000/3450, Loss: 809.4895\n",
      "Iteration 2100/3450, Loss: 832.2881\n",
      "Iteration 2200/3450, Loss: 865.8607\n",
      "Iteration 2300/3450, Loss: 891.3629\n",
      "Iteration 2400/3450, Loss: 908.8031\n",
      "Iteration 2500/3450, Loss: 936.1153\n",
      "Iteration 2600/3450, Loss: 949.7953\n",
      "Iteration 2700/3450, Loss: 973.2363\n",
      "Iteration 2800/3450, Loss: 989.4867\n",
      "Iteration 2900/3450, Loss: 1012.1534\n",
      "Iteration 3000/3450, Loss: 1040.6988\n",
      "Iteration 3100/3450, Loss: 1055.5398\n",
      "Iteration 3200/3450, Loss: 1073.9814\n",
      "Iteration 3300/3450, Loss: 1093.3366\n",
      "Iteration 3400/3450, Loss: 1104.6027\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 163.3567\n",
      "Iteration 200/1725, Loss: 291.0487\n",
      "Iteration 300/1725, Loss: 436.2741\n",
      "Iteration 400/1725, Loss: 587.3408\n",
      "Iteration 500/1725, Loss: 707.2949\n",
      "Iteration 600/1725, Loss: 854.8961\n",
      "Iteration 700/1725, Loss: 991.2962\n",
      "Iteration 800/1725, Loss: 1123.7903\n",
      "Iteration 900/1725, Loss: 1261.3191\n",
      "Iteration 1000/1725, Loss: 1357.3008\n",
      "Iteration 1100/1725, Loss: 1480.3964\n",
      "Iteration 1200/1725, Loss: 1572.3932\n",
      "Iteration 1300/1725, Loss: 1662.4424\n",
      "Iteration 1400/1725, Loss: 1742.2034\n",
      "Iteration 1500/1725, Loss: 1839.0903\n",
      "Iteration 1600/1725, Loss: 1922.9939\n",
      "Iteration 1700/1725, Loss: 2026.8486\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 474.1330\n",
      "Iteration 200/825, Loss: 990.1902\n",
      "Iteration 300/825, Loss: 1457.5388\n",
      "Iteration 400/825, Loss: 1963.0207\n",
      "Iteration 500/825, Loss: 2475.2802\n",
      "Iteration 600/825, Loss: 2952.3535\n",
      "Iteration 700/825, Loss: 3450.8864\n",
      "Iteration 800/825, Loss: 3880.5967\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 1689.6030\n",
      "Iteration 200/375, Loss: 3610.0631\n",
      "Iteration 300/375, Loss: 5511.9123\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 6414.6663\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 85.0911\n",
      "Iteration 200/28125, Loss: 105.7026\n",
      "Iteration 300/28125, Loss: 144.2419\n",
      "Iteration 400/28125, Loss: 158.0309\n",
      "Iteration 500/28125, Loss: 163.2200\n",
      "Iteration 600/28125, Loss: 169.8641\n",
      "Iteration 700/28125, Loss: 182.5986\n",
      "Iteration 800/28125, Loss: 208.3557\n",
      "Iteration 900/28125, Loss: 227.9971\n",
      "Iteration 1000/28125, Loss: 218.4224\n",
      "Iteration 1100/28125, Loss: 219.6343\n",
      "Iteration 1200/28125, Loss: 215.2766\n",
      "Iteration 1300/28125, Loss: 240.5569\n",
      "Iteration 1400/28125, Loss: 241.4646\n",
      "Iteration 1500/28125, Loss: 235.2026\n",
      "Iteration 1600/28125, Loss: 237.9613\n",
      "Iteration 1700/28125, Loss: 235.8076\n",
      "Iteration 1800/28125, Loss: 243.5127\n",
      "Iteration 1900/28125, Loss: 256.7378\n",
      "Iteration 2000/28125, Loss: 258.0677\n",
      "Iteration 2100/28125, Loss: 245.7691\n",
      "Iteration 2200/28125, Loss: 254.0870\n",
      "Iteration 2300/28125, Loss: 252.1907\n",
      "Iteration 2400/28125, Loss: 258.9027\n",
      "Iteration 2500/28125, Loss: 258.3354\n",
      "Iteration 2600/28125, Loss: 268.9226\n",
      "Iteration 2700/28125, Loss: 259.3652\n",
      "Iteration 2800/28125, Loss: 269.7489\n",
      "Iteration 2900/28125, Loss: 272.7043\n",
      "Iteration 3000/28125, Loss: 266.4842\n",
      "Iteration 3100/28125, Loss: 275.7205\n",
      "Iteration 3200/28125, Loss: 264.5778\n",
      "Iteration 3300/28125, Loss: 259.4825\n",
      "Iteration 3400/28125, Loss: 270.0325\n",
      "Iteration 3500/28125, Loss: 269.0200\n",
      "Iteration 3600/28125, Loss: 282.4367\n",
      "Iteration 3700/28125, Loss: 274.2362\n",
      "Iteration 3800/28125, Loss: 267.3649\n",
      "Iteration 3900/28125, Loss: 272.3028\n",
      "Iteration 4000/28125, Loss: 280.0854\n",
      "Iteration 4100/28125, Loss: 277.6663\n",
      "Iteration 4200/28125, Loss: 276.2551\n",
      "Iteration 4300/28125, Loss: 263.9113\n",
      "Iteration 4400/28125, Loss: 276.8542\n",
      "Iteration 4500/28125, Loss: 269.3337\n",
      "Iteration 4600/28125, Loss: 282.6562\n",
      "Iteration 4700/28125, Loss: 269.0166\n",
      "Iteration 4800/28125, Loss: 264.1771\n",
      "Iteration 4900/28125, Loss: 276.6575\n",
      "Iteration 5000/28125, Loss: 277.6281\n",
      "Iteration 5100/28125, Loss: 270.7032\n",
      "Iteration 5200/28125, Loss: 268.5651\n",
      "Iteration 5300/28125, Loss: 270.5580\n",
      "Iteration 5400/28125, Loss: 274.8457\n",
      "Iteration 5500/28125, Loss: 273.0788\n",
      "Iteration 5600/28125, Loss: 271.5051\n",
      "Iteration 5700/28125, Loss: 276.3405\n",
      "Iteration 5800/28125, Loss: 276.8307\n",
      "Iteration 5900/28125, Loss: 266.4357\n",
      "Iteration 6000/28125, Loss: 280.7018\n",
      "Iteration 6100/28125, Loss: 264.1821\n",
      "Iteration 6200/28125, Loss: 287.4968\n",
      "Iteration 6300/28125, Loss: 283.6909\n",
      "Iteration 6400/28125, Loss: 281.4645\n",
      "Iteration 6500/28125, Loss: 269.4512\n",
      "Iteration 6600/28125, Loss: 272.6572\n",
      "Iteration 6700/28125, Loss: 277.9765\n",
      "Iteration 6800/28125, Loss: 290.7557\n",
      "Iteration 6900/28125, Loss: 269.9393\n",
      "Iteration 7000/28125, Loss: 266.6791\n",
      "Iteration 7100/28125, Loss: 277.3470\n",
      "Iteration 7200/28125, Loss: 269.4654\n",
      "Iteration 7300/28125, Loss: 296.4870\n",
      "Iteration 7400/28125, Loss: 281.7729\n",
      "Iteration 7500/28125, Loss: 277.2882\n",
      "Iteration 7600/28125, Loss: 269.9807\n",
      "Iteration 7700/28125, Loss: 270.0373\n",
      "Iteration 7800/28125, Loss: 270.0460\n",
      "Iteration 7900/28125, Loss: 278.6403\n",
      "Iteration 8000/28125, Loss: 273.8993\n",
      "Iteration 8100/28125, Loss: 270.1507\n",
      "Iteration 8200/28125, Loss: 282.5188\n",
      "Iteration 8300/28125, Loss: 270.9670\n",
      "Iteration 8400/28125, Loss: 265.3142\n",
      "Iteration 8500/28125, Loss: 258.1028\n",
      "Iteration 8600/28125, Loss: 276.5020\n",
      "Iteration 8700/28125, Loss: 271.3725\n",
      "Iteration 8800/28125, Loss: 265.3085\n",
      "Iteration 8900/28125, Loss: 279.8811\n",
      "Iteration 9000/28125, Loss: 268.5616\n",
      "Iteration 9100/28125, Loss: 278.2696\n",
      "Iteration 9200/28125, Loss: 283.0990\n",
      "Iteration 9300/28125, Loss: 269.0037\n",
      "Iteration 9400/28125, Loss: 268.9213\n",
      "Iteration 9500/28125, Loss: 266.6609\n",
      "Iteration 9600/28125, Loss: 268.4754\n",
      "Iteration 9700/28125, Loss: 274.6793\n",
      "Iteration 9800/28125, Loss: 273.5559\n",
      "Iteration 9900/28125, Loss: 276.2115\n",
      "Iteration 10000/28125, Loss: 273.3926\n",
      "Iteration 10100/28125, Loss: 270.7237\n",
      "Iteration 10200/28125, Loss: 267.0680\n",
      "Iteration 10300/28125, Loss: 273.2205\n",
      "Iteration 10400/28125, Loss: 278.6004\n",
      "Iteration 10500/28125, Loss: 273.7179\n",
      "Iteration 10600/28125, Loss: 269.8424\n",
      "Iteration 10700/28125, Loss: 275.2403\n",
      "Iteration 10800/28125, Loss: 264.2630\n",
      "Iteration 10900/28125, Loss: 255.3834\n",
      "Iteration 11000/28125, Loss: 264.7207\n",
      "Iteration 11100/28125, Loss: 268.6439\n",
      "Iteration 11200/28125, Loss: 271.8620\n",
      "Iteration 11300/28125, Loss: 278.8301\n",
      "Iteration 11400/28125, Loss: 268.8308\n",
      "Iteration 11500/28125, Loss: 278.8676\n",
      "Iteration 11600/28125, Loss: 267.4430\n",
      "Iteration 11700/28125, Loss: 274.4886\n",
      "Iteration 11800/28125, Loss: 269.8672\n",
      "Iteration 11900/28125, Loss: 271.8141\n",
      "Iteration 12000/28125, Loss: 277.5733\n",
      "Iteration 12100/28125, Loss: 272.1622\n",
      "Iteration 12200/28125, Loss: 276.9207\n",
      "Iteration 12300/28125, Loss: 257.4761\n",
      "Iteration 12400/28125, Loss: 279.1377\n",
      "Iteration 12500/28125, Loss: 269.8359\n",
      "Iteration 12600/28125, Loss: 274.5388\n",
      "Iteration 12700/28125, Loss: 269.5206\n",
      "Iteration 12800/28125, Loss: 260.1297\n",
      "Iteration 12900/28125, Loss: 262.9294\n",
      "Iteration 13000/28125, Loss: 271.9915\n",
      "Iteration 13100/28125, Loss: 263.3884\n",
      "Iteration 13200/28125, Loss: 275.5342\n",
      "Iteration 13300/28125, Loss: 268.1710\n",
      "Iteration 13400/28125, Loss: 277.2964\n",
      "Iteration 13500/28125, Loss: 270.3278\n",
      "Iteration 13600/28125, Loss: 271.9557\n",
      "Iteration 13700/28125, Loss: 265.1311\n",
      "Iteration 13800/28125, Loss: 256.8168\n",
      "Iteration 13900/28125, Loss: 255.0581\n",
      "Iteration 14000/28125, Loss: 266.2654\n",
      "Iteration 14100/28125, Loss: 271.2302\n",
      "Iteration 14200/28125, Loss: 266.5532\n",
      "Iteration 14300/28125, Loss: 275.5430\n",
      "Iteration 14400/28125, Loss: 283.4346\n",
      "Iteration 14500/28125, Loss: 282.1536\n",
      "Iteration 14600/28125, Loss: 276.1812\n",
      "Iteration 14700/28125, Loss: 275.2406\n",
      "Iteration 14800/28125, Loss: 283.6067\n",
      "Iteration 14900/28125, Loss: 264.1853\n",
      "Iteration 15000/28125, Loss: 258.8459\n",
      "Iteration 15100/28125, Loss: 274.6429\n",
      "Iteration 15200/28125, Loss: 277.7177\n",
      "Iteration 15300/28125, Loss: 283.0591\n",
      "Iteration 15400/28125, Loss: 279.3201\n",
      "Iteration 15500/28125, Loss: 265.7143\n",
      "Iteration 15600/28125, Loss: 277.8134\n",
      "Iteration 15700/28125, Loss: 271.3176\n",
      "Iteration 15800/28125, Loss: 281.9188\n",
      "Iteration 15900/28125, Loss: 273.9134\n",
      "Iteration 16000/28125, Loss: 271.7785\n",
      "Iteration 16100/28125, Loss: 276.3095\n",
      "Iteration 16200/28125, Loss: 267.5703\n",
      "Iteration 16300/28125, Loss: 268.5827\n",
      "Iteration 16400/28125, Loss: 276.0381\n",
      "Iteration 16500/28125, Loss: 286.3058\n",
      "Iteration 16600/28125, Loss: 277.3995\n",
      "Iteration 16700/28125, Loss: 265.1291\n",
      "Iteration 16800/28125, Loss: 280.7341\n",
      "Iteration 16900/28125, Loss: 276.5427\n",
      "Iteration 17000/28125, Loss: 278.7517\n",
      "Iteration 17100/28125, Loss: 270.7614\n",
      "Iteration 17200/28125, Loss: 261.1414\n",
      "Iteration 17300/28125, Loss: 276.0961\n",
      "Iteration 17400/28125, Loss: 263.5824\n",
      "Iteration 17500/28125, Loss: 270.1948\n",
      "Iteration 17600/28125, Loss: 269.9076\n",
      "Iteration 17700/28125, Loss: 273.6218\n",
      "Iteration 17800/28125, Loss: 261.5827\n",
      "Iteration 17900/28125, Loss: 255.7403\n",
      "Iteration 18000/28125, Loss: 256.7359\n",
      "Iteration 18100/28125, Loss: 264.2678\n",
      "Iteration 18200/28125, Loss: 267.8217\n",
      "Iteration 18300/28125, Loss: 268.3244\n",
      "Iteration 18400/28125, Loss: 275.4445\n",
      "Iteration 18500/28125, Loss: 271.8448\n",
      "Iteration 18600/28125, Loss: 266.6144\n",
      "Iteration 18700/28125, Loss: 275.2388\n",
      "Iteration 18800/28125, Loss: 279.4951\n",
      "Iteration 18900/28125, Loss: 274.2752\n",
      "Iteration 19000/28125, Loss: 285.6245\n",
      "Iteration 19100/28125, Loss: 273.2251\n",
      "Iteration 19200/28125, Loss: 277.4663\n",
      "Iteration 19300/28125, Loss: 255.4180\n",
      "Iteration 19400/28125, Loss: 266.8300\n",
      "Iteration 19500/28125, Loss: 265.5218\n",
      "Iteration 19600/28125, Loss: 275.0763\n",
      "Iteration 19700/28125, Loss: 268.0249\n",
      "Iteration 19800/28125, Loss: 267.1570\n",
      "Iteration 19900/28125, Loss: 271.9689\n",
      "Iteration 20000/28125, Loss: 273.8870\n",
      "Iteration 20100/28125, Loss: 272.1141\n",
      "Iteration 20200/28125, Loss: 275.5141\n",
      "Iteration 20300/28125, Loss: 269.5127\n",
      "Iteration 20400/28125, Loss: 262.0663\n",
      "Iteration 20500/28125, Loss: 274.7560\n",
      "Iteration 20600/28125, Loss: 271.6362\n",
      "Iteration 20700/28125, Loss: 267.0405\n",
      "Iteration 20800/28125, Loss: 271.8480\n",
      "Iteration 20900/28125, Loss: 282.3608\n",
      "Iteration 21000/28125, Loss: 267.3013\n",
      "Iteration 21100/28125, Loss: 280.0739\n",
      "Iteration 21200/28125, Loss: 284.3713\n",
      "Iteration 21300/28125, Loss: 276.3751\n",
      "Iteration 21400/28125, Loss: 272.6555\n",
      "Iteration 21500/28125, Loss: 268.4101\n",
      "Iteration 21600/28125, Loss: 271.3478\n",
      "Iteration 21700/28125, Loss: 256.8030\n",
      "Iteration 21800/28125, Loss: 270.2560\n",
      "Iteration 21900/28125, Loss: 258.0132\n",
      "Iteration 22000/28125, Loss: 268.3071\n",
      "Iteration 22100/28125, Loss: 264.3567\n",
      "Iteration 22200/28125, Loss: 266.6870\n",
      "Iteration 22300/28125, Loss: 275.5604\n",
      "Iteration 22400/28125, Loss: 265.0596\n",
      "Iteration 22500/28125, Loss: 282.3554\n",
      "Iteration 22600/28125, Loss: 267.3843\n",
      "Iteration 22700/28125, Loss: 259.5297\n",
      "Iteration 22800/28125, Loss: 258.2166\n",
      "Iteration 22900/28125, Loss: 272.4199\n",
      "Iteration 23000/28125, Loss: 260.2880\n",
      "Iteration 23100/28125, Loss: 267.5291\n",
      "Iteration 23200/28125, Loss: 254.0936\n",
      "Iteration 23300/28125, Loss: 263.7338\n",
      "Iteration 23400/28125, Loss: 266.8241\n",
      "Iteration 23500/28125, Loss: 265.3420\n",
      "Iteration 23600/28125, Loss: 278.8991\n",
      "Iteration 23700/28125, Loss: 266.9905\n",
      "Iteration 23800/28125, Loss: 270.6569\n",
      "Iteration 23900/28125, Loss: 267.3518\n",
      "Iteration 24000/28125, Loss: 281.5089\n",
      "Iteration 24100/28125, Loss: 270.5486\n",
      "Iteration 24200/28125, Loss: 272.5691\n",
      "Iteration 24300/28125, Loss: 281.0876\n",
      "Iteration 24400/28125, Loss: 282.6842\n",
      "Iteration 24500/28125, Loss: 286.6629\n",
      "Iteration 24600/28125, Loss: 282.7385\n",
      "Iteration 24700/28125, Loss: 268.3808\n",
      "Iteration 24800/28125, Loss: 278.8084\n",
      "Iteration 24900/28125, Loss: 266.7206\n",
      "Iteration 25000/28125, Loss: 265.2538\n",
      "Iteration 25100/28125, Loss: 271.0628\n",
      "Iteration 25200/28125, Loss: 277.8111\n",
      "Iteration 25300/28125, Loss: 271.7661\n",
      "Iteration 25400/28125, Loss: 278.5372\n",
      "Iteration 25500/28125, Loss: 271.4311\n",
      "Iteration 25600/28125, Loss: 273.9173\n",
      "Iteration 25700/28125, Loss: 267.0481\n",
      "Iteration 25800/28125, Loss: 267.2187\n",
      "Iteration 25900/28125, Loss: 277.9428\n",
      "Iteration 26000/28125, Loss: 265.2224\n",
      "Iteration 26100/28125, Loss: 271.7765\n",
      "Iteration 26200/28125, Loss: 266.8931\n",
      "Iteration 26300/28125, Loss: 273.7893\n",
      "Iteration 26400/28125, Loss: 279.5812\n",
      "Iteration 26500/28125, Loss: 271.5674\n",
      "Iteration 26600/28125, Loss: 280.0924\n",
      "Iteration 26700/28125, Loss: 286.2317\n",
      "Iteration 26800/28125, Loss: 280.6863\n",
      "Iteration 26900/28125, Loss: 278.8620\n",
      "Iteration 27000/28125, Loss: 284.9971\n",
      "Iteration 27100/28125, Loss: 280.3778\n",
      "Iteration 27200/28125, Loss: 270.8862\n",
      "Iteration 27300/28125, Loss: 261.9822\n",
      "Iteration 27400/28125, Loss: 275.5267\n",
      "Iteration 27500/28125, Loss: 275.6508\n",
      "Iteration 27600/28125, Loss: 278.9608\n",
      "Iteration 27700/28125, Loss: 268.2692\n",
      "Iteration 27800/28125, Loss: 253.7456\n",
      "Iteration 27900/28125, Loss: 266.7041\n",
      "Iteration 28000/28125, Loss: 263.8775\n",
      "Iteration 28100/28125, Loss: 267.3660\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 119.6707\n",
      "Iteration 200/14025, Loss: 184.7864\n",
      "Iteration 300/14025, Loss: 224.1671\n",
      "Iteration 400/14025, Loss: 270.4498\n",
      "Iteration 500/14025, Loss: 301.2401\n",
      "Iteration 600/14025, Loss: 330.9036\n",
      "Iteration 700/14025, Loss: 367.3172\n",
      "Iteration 800/14025, Loss: 392.0066\n",
      "Iteration 900/14025, Loss: 396.3415\n",
      "Iteration 1000/14025, Loss: 423.7923\n",
      "Iteration 1100/14025, Loss: 437.9654\n",
      "Iteration 1200/14025, Loss: 455.3682\n",
      "Iteration 1300/14025, Loss: 471.8352\n",
      "Iteration 1400/14025, Loss: 491.0604\n",
      "Iteration 1500/14025, Loss: 498.0043\n",
      "Iteration 1600/14025, Loss: 505.5357\n",
      "Iteration 1700/14025, Loss: 506.7195\n",
      "Iteration 1800/14025, Loss: 518.7056\n",
      "Iteration 1900/14025, Loss: 524.4730\n",
      "Iteration 2000/14025, Loss: 530.2315\n",
      "Iteration 2100/14025, Loss: 524.3090\n",
      "Iteration 2200/14025, Loss: 536.9514\n",
      "Iteration 2300/14025, Loss: 524.3380\n",
      "Iteration 2400/14025, Loss: 526.4803\n",
      "Iteration 2500/14025, Loss: 537.3354\n",
      "Iteration 2600/14025, Loss: 552.9719\n",
      "Iteration 2700/14025, Loss: 543.9621\n",
      "Iteration 2800/14025, Loss: 546.8119\n",
      "Iteration 2900/14025, Loss: 541.9028\n",
      "Iteration 3000/14025, Loss: 535.6639\n",
      "Iteration 3100/14025, Loss: 550.2271\n",
      "Iteration 3200/14025, Loss: 532.6012\n",
      "Iteration 3300/14025, Loss: 526.5133\n",
      "Iteration 3400/14025, Loss: 543.7742\n",
      "Iteration 3500/14025, Loss: 548.1166\n",
      "Iteration 3600/14025, Loss: 535.3699\n",
      "Iteration 3700/14025, Loss: 540.5459\n",
      "Iteration 3800/14025, Loss: 538.7834\n",
      "Iteration 3900/14025, Loss: 531.7741\n",
      "Iteration 4000/14025, Loss: 533.9058\n",
      "Iteration 4100/14025, Loss: 540.7382\n",
      "Iteration 4200/14025, Loss: 542.9508\n",
      "Iteration 4300/14025, Loss: 542.9235\n",
      "Iteration 4400/14025, Loss: 545.6331\n",
      "Iteration 4500/14025, Loss: 547.6017\n",
      "Iteration 4600/14025, Loss: 544.7312\n",
      "Iteration 4700/14025, Loss: 548.4918\n",
      "Iteration 4800/14025, Loss: 545.5244\n",
      "Iteration 4900/14025, Loss: 551.7254\n",
      "Iteration 5000/14025, Loss: 539.6958\n",
      "Iteration 5100/14025, Loss: 534.6938\n",
      "Iteration 5200/14025, Loss: 558.3094\n",
      "Iteration 5300/14025, Loss: 551.4835\n",
      "Iteration 5400/14025, Loss: 546.9880\n",
      "Iteration 5500/14025, Loss: 541.2759\n",
      "Iteration 5600/14025, Loss: 555.8035\n",
      "Iteration 5700/14025, Loss: 546.0208\n",
      "Iteration 5800/14025, Loss: 551.1033\n",
      "Iteration 5900/14025, Loss: 550.4607\n",
      "Iteration 6000/14025, Loss: 546.6177\n",
      "Iteration 6100/14025, Loss: 542.8829\n",
      "Iteration 6200/14025, Loss: 546.1356\n",
      "Iteration 6300/14025, Loss: 538.8669\n",
      "Iteration 6400/14025, Loss: 538.1852\n",
      "Iteration 6500/14025, Loss: 534.1454\n",
      "Iteration 6600/14025, Loss: 537.7661\n",
      "Iteration 6700/14025, Loss: 540.9675\n",
      "Iteration 6800/14025, Loss: 544.8631\n",
      "Iteration 6900/14025, Loss: 536.7485\n",
      "Iteration 7000/14025, Loss: 549.6396\n",
      "Iteration 7100/14025, Loss: 547.4048\n",
      "Iteration 7200/14025, Loss: 554.3042\n",
      "Iteration 7300/14025, Loss: 544.8057\n",
      "Iteration 7400/14025, Loss: 551.7603\n",
      "Iteration 7500/14025, Loss: 553.2191\n",
      "Iteration 7600/14025, Loss: 544.3483\n",
      "Iteration 7700/14025, Loss: 553.4907\n",
      "Iteration 7800/14025, Loss: 550.8663\n",
      "Iteration 7900/14025, Loss: 548.0569\n",
      "Iteration 8000/14025, Loss: 546.3928\n",
      "Iteration 8100/14025, Loss: 535.9900\n",
      "Iteration 8200/14025, Loss: 539.9488\n",
      "Iteration 8300/14025, Loss: 548.0459\n",
      "Iteration 8400/14025, Loss: 546.4892\n",
      "Iteration 8500/14025, Loss: 544.5806\n",
      "Iteration 8600/14025, Loss: 540.4785\n",
      "Iteration 8700/14025, Loss: 530.8675\n",
      "Iteration 8800/14025, Loss: 546.6799\n",
      "Iteration 8900/14025, Loss: 532.3640\n",
      "Iteration 9000/14025, Loss: 541.6757\n",
      "Iteration 9100/14025, Loss: 535.5918\n",
      "Iteration 9200/14025, Loss: 540.1854\n",
      "Iteration 9300/14025, Loss: 544.5110\n",
      "Iteration 9400/14025, Loss: 551.7992\n",
      "Iteration 9500/14025, Loss: 564.2710\n",
      "Iteration 9600/14025, Loss: 551.6462\n",
      "Iteration 9700/14025, Loss: 553.8547\n",
      "Iteration 9800/14025, Loss: 554.5386\n",
      "Iteration 9900/14025, Loss: 571.8741\n",
      "Iteration 10000/14025, Loss: 560.6498\n",
      "Iteration 10100/14025, Loss: 540.8708\n",
      "Iteration 10200/14025, Loss: 557.1842\n",
      "Iteration 10300/14025, Loss: 544.4871\n",
      "Iteration 10400/14025, Loss: 549.5003\n",
      "Iteration 10500/14025, Loss: 544.0021\n",
      "Iteration 10600/14025, Loss: 545.8307\n",
      "Iteration 10700/14025, Loss: 538.1667\n",
      "Iteration 10800/14025, Loss: 536.6034\n",
      "Iteration 10900/14025, Loss: 536.0937\n",
      "Iteration 11000/14025, Loss: 539.7485\n",
      "Iteration 11100/14025, Loss: 537.6731\n",
      "Iteration 11200/14025, Loss: 545.0398\n",
      "Iteration 11300/14025, Loss: 544.7948\n",
      "Iteration 11400/14025, Loss: 546.4988\n",
      "Iteration 11500/14025, Loss: 546.7953\n",
      "Iteration 11600/14025, Loss: 537.6276\n",
      "Iteration 11700/14025, Loss: 542.5239\n",
      "Iteration 11800/14025, Loss: 542.2375\n",
      "Iteration 11900/14025, Loss: 544.2994\n",
      "Iteration 12000/14025, Loss: 548.4353\n",
      "Iteration 12100/14025, Loss: 541.9560\n",
      "Iteration 12200/14025, Loss: 552.4055\n",
      "Iteration 12300/14025, Loss: 549.0542\n",
      "Iteration 12400/14025, Loss: 544.2974\n",
      "Iteration 12500/14025, Loss: 548.9491\n",
      "Iteration 12600/14025, Loss: 547.6405\n",
      "Iteration 12700/14025, Loss: 547.0894\n",
      "Iteration 12800/14025, Loss: 546.1460\n",
      "Iteration 12900/14025, Loss: 547.2246\n",
      "Iteration 13000/14025, Loss: 540.9324\n",
      "Iteration 13100/14025, Loss: 538.3411\n",
      "Iteration 13200/14025, Loss: 549.3736\n",
      "Iteration 13300/14025, Loss: 543.3890\n",
      "Iteration 13400/14025, Loss: 561.4067\n",
      "Iteration 13500/14025, Loss: 553.8899\n",
      "Iteration 13600/14025, Loss: 553.0106\n",
      "Iteration 13700/14025, Loss: 546.9864\n",
      "Iteration 13800/14025, Loss: 547.2880\n",
      "Iteration 13900/14025, Loss: 540.0798\n",
      "Iteration 14000/14025, Loss: 554.1183\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 215.4986\n",
      "Iteration 200/6975, Loss: 348.9244\n",
      "Iteration 300/6975, Loss: 441.2827\n",
      "Iteration 400/6975, Loss: 562.6865\n",
      "Iteration 500/6975, Loss: 643.5154\n",
      "Iteration 600/6975, Loss: 712.9804\n",
      "Iteration 700/6975, Loss: 778.0523\n",
      "Iteration 800/6975, Loss: 852.7996\n",
      "Iteration 900/6975, Loss: 902.5732\n",
      "Iteration 1000/6975, Loss: 965.2275\n",
      "Iteration 1100/6975, Loss: 987.5468\n",
      "Iteration 1200/6975, Loss: 1008.7395\n",
      "Iteration 1300/6975, Loss: 1060.9682\n",
      "Iteration 1400/6975, Loss: 1080.0741\n",
      "Iteration 1500/6975, Loss: 1082.9788\n",
      "Iteration 1600/6975, Loss: 1123.4397\n",
      "Iteration 1700/6975, Loss: 1156.3840\n",
      "Iteration 1800/6975, Loss: 1129.3849\n",
      "Iteration 1900/6975, Loss: 1127.0766\n",
      "Iteration 2000/6975, Loss: 1128.5281\n",
      "Iteration 2100/6975, Loss: 1149.2974\n",
      "Iteration 2200/6975, Loss: 1174.2302\n",
      "Iteration 2300/6975, Loss: 1187.7807\n",
      "Iteration 2400/6975, Loss: 1182.0089\n",
      "Iteration 2500/6975, Loss: 1188.4935\n",
      "Iteration 2600/6975, Loss: 1184.9799\n",
      "Iteration 2700/6975, Loss: 1191.5055\n",
      "Iteration 2800/6975, Loss: 1206.1196\n",
      "Iteration 2900/6975, Loss: 1227.7787\n",
      "Iteration 3000/6975, Loss: 1232.8485\n",
      "Iteration 3100/6975, Loss: 1222.8752\n",
      "Iteration 3200/6975, Loss: 1226.0575\n",
      "Iteration 3300/6975, Loss: 1218.5984\n",
      "Iteration 3400/6975, Loss: 1219.0118\n",
      "Iteration 3500/6975, Loss: 1223.7636\n",
      "Iteration 3600/6975, Loss: 1234.1522\n",
      "Iteration 3700/6975, Loss: 1227.6190\n",
      "Iteration 3800/6975, Loss: 1228.6944\n",
      "Iteration 3900/6975, Loss: 1223.1376\n",
      "Iteration 4000/6975, Loss: 1227.8873\n",
      "Iteration 4100/6975, Loss: 1228.8876\n",
      "Iteration 4200/6975, Loss: 1221.1309\n",
      "Iteration 4300/6975, Loss: 1214.2483\n",
      "Iteration 4400/6975, Loss: 1214.2673\n",
      "Iteration 4500/6975, Loss: 1205.7477\n",
      "Iteration 4600/6975, Loss: 1226.0468\n",
      "Iteration 4700/6975, Loss: 1207.6910\n",
      "Iteration 4800/6975, Loss: 1224.9826\n",
      "Iteration 4900/6975, Loss: 1241.3005\n",
      "Iteration 5000/6975, Loss: 1254.7442\n",
      "Iteration 5100/6975, Loss: 1239.8910\n",
      "Iteration 5200/6975, Loss: 1219.5835\n",
      "Iteration 5300/6975, Loss: 1228.6542\n",
      "Iteration 5400/6975, Loss: 1219.8645\n",
      "Iteration 5500/6975, Loss: 1201.8094\n",
      "Iteration 5600/6975, Loss: 1214.9527\n",
      "Iteration 5700/6975, Loss: 1208.8656\n",
      "Iteration 5800/6975, Loss: 1204.4266\n",
      "Iteration 5900/6975, Loss: 1214.5878\n",
      "Iteration 6000/6975, Loss: 1220.1532\n",
      "Iteration 6100/6975, Loss: 1207.8387\n",
      "Iteration 6200/6975, Loss: 1203.2887\n",
      "Iteration 6300/6975, Loss: 1218.4184\n",
      "Iteration 6400/6975, Loss: 1213.6517\n",
      "Iteration 6500/6975, Loss: 1207.5301\n",
      "Iteration 6600/6975, Loss: 1219.8798\n",
      "Iteration 6700/6975, Loss: 1226.9355\n",
      "Iteration 6800/6975, Loss: 1230.7682\n",
      "Iteration 6900/6975, Loss: 1211.3285\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 463.2255\n",
      "Iteration 200/3450, Loss: 790.0140\n",
      "Iteration 300/3450, Loss: 1105.6745\n",
      "Iteration 400/3450, Loss: 1370.1014\n",
      "Iteration 500/3450, Loss: 1632.5022\n",
      "Iteration 600/3450, Loss: 1817.0944\n",
      "Iteration 700/3450, Loss: 2010.1688\n",
      "Iteration 800/3450, Loss: 2189.8030\n",
      "Iteration 900/3450, Loss: 2338.1241\n",
      "Iteration 1000/3450, Loss: 2443.7168\n",
      "Iteration 1100/3450, Loss: 2537.5350\n",
      "Iteration 1200/3450, Loss: 2635.0916\n",
      "Iteration 1300/3450, Loss: 2740.9389\n",
      "Iteration 1400/3450, Loss: 2786.7967\n",
      "Iteration 1500/3450, Loss: 2821.8521\n",
      "Iteration 1600/3450, Loss: 2848.1652\n",
      "Iteration 1700/3450, Loss: 2947.7752\n",
      "Iteration 1800/3450, Loss: 2970.9042\n",
      "Iteration 1900/3450, Loss: 2981.6767\n",
      "Iteration 2000/3450, Loss: 3018.9466\n",
      "Iteration 2100/3450, Loss: 3033.5751\n",
      "Iteration 2200/3450, Loss: 3054.9121\n",
      "Iteration 2300/3450, Loss: 3047.3356\n",
      "Iteration 2400/3450, Loss: 3046.0237\n",
      "Iteration 2500/3450, Loss: 3071.3540\n",
      "Iteration 2600/3450, Loss: 3049.7261\n",
      "Iteration 2700/3450, Loss: 3047.5166\n",
      "Iteration 2800/3450, Loss: 3068.7243\n",
      "Iteration 2900/3450, Loss: 3015.8596\n",
      "Iteration 3000/3450, Loss: 3080.7826\n",
      "Iteration 3100/3450, Loss: 3094.3356\n",
      "Iteration 3200/3450, Loss: 3084.8479\n",
      "Iteration 3300/3450, Loss: 3100.8930\n",
      "Iteration 3400/3450, Loss: 3150.2356\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 1351.8711\n",
      "Iteration 200/1725, Loss: 2410.7050\n",
      "Iteration 300/1725, Loss: 3402.0871\n",
      "Iteration 400/1725, Loss: 4376.0371\n",
      "Iteration 500/1725, Loss: 5003.5338\n",
      "Iteration 600/1725, Loss: 5662.1574\n",
      "Iteration 700/1725, Loss: 6342.4440\n",
      "Iteration 800/1725, Loss: 6851.6803\n",
      "Iteration 900/1725, Loss: 7267.9814\n",
      "Iteration 1000/1725, Loss: 7513.3939\n",
      "Iteration 1100/1725, Loss: 7884.2798\n",
      "Iteration 1200/1725, Loss: 8138.6328\n",
      "Iteration 1300/1725, Loss: 8357.8145\n",
      "Iteration 1400/1725, Loss: 8254.3335\n",
      "Iteration 1500/1725, Loss: 8639.8695\n",
      "Iteration 1600/1725, Loss: 8725.0939\n",
      "Iteration 1700/1725, Loss: 8880.2829\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 4142.9508\n",
      "Iteration 200/825, Loss: 8118.5772\n",
      "Iteration 300/825, Loss: 11749.7818\n",
      "Iteration 400/825, Loss: 14814.2262\n",
      "Iteration 500/825, Loss: 17809.8266\n",
      "Iteration 600/825, Loss: 20289.0220\n",
      "Iteration 700/825, Loss: 22559.8794\n",
      "Iteration 800/825, Loss: 24579.4564\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 16126.1474\n",
      "Iteration 200/375, Loss: 30976.4087\n",
      "Iteration 300/375, Loss: 44838.3741\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 59299.3129\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "Iteration 100/28125, Loss: 367.6793\n",
      "Iteration 200/28125, Loss: 360.3238\n",
      "Iteration 300/28125, Loss: 381.9064\n",
      "Iteration 400/28125, Loss: 348.0224\n",
      "Iteration 500/28125, Loss: 347.4024\n",
      "Iteration 600/28125, Loss: 357.2430\n",
      "Iteration 700/28125, Loss: 343.0336\n",
      "Iteration 800/28125, Loss: 375.7244\n",
      "Iteration 900/28125, Loss: 391.7042\n",
      "Iteration 1000/28125, Loss: 342.4976\n",
      "Iteration 1100/28125, Loss: 344.2587\n",
      "Iteration 1200/28125, Loss: 343.3807\n",
      "Iteration 1300/28125, Loss: 393.9909\n",
      "Iteration 1400/28125, Loss: 342.6257\n",
      "Iteration 1500/28125, Loss: 342.1546\n",
      "Iteration 1600/28125, Loss: 368.4326\n",
      "Iteration 1700/28125, Loss: 388.6282\n",
      "Iteration 1800/28125, Loss: 382.2046\n",
      "Iteration 1900/28125, Loss: 368.8738\n",
      "Iteration 2000/28125, Loss: 358.0471\n",
      "Iteration 2100/28125, Loss: 396.7877\n",
      "Iteration 2200/28125, Loss: 360.9012\n",
      "Iteration 2300/28125, Loss: 361.2509\n",
      "Iteration 2400/28125, Loss: 346.7573\n",
      "Iteration 2500/28125, Loss: 358.8481\n",
      "Iteration 2600/28125, Loss: 371.4366\n",
      "Iteration 2700/28125, Loss: 377.3924\n",
      "Iteration 2800/28125, Loss: 351.5392\n",
      "Iteration 2900/28125, Loss: 353.1251\n",
      "Iteration 3000/28125, Loss: 364.7791\n",
      "Iteration 3100/28125, Loss: 385.9301\n",
      "Iteration 3200/28125, Loss: 368.6687\n",
      "Iteration 3300/28125, Loss: 363.3329\n",
      "Iteration 3400/28125, Loss: 373.6781\n",
      "Iteration 3500/28125, Loss: 380.6038\n",
      "Iteration 3600/28125, Loss: 350.8802\n",
      "Iteration 3700/28125, Loss: 349.8336\n",
      "Iteration 3800/28125, Loss: 373.0842\n",
      "Iteration 3900/28125, Loss: 355.9073\n",
      "Iteration 4000/28125, Loss: 390.6386\n",
      "Iteration 4100/28125, Loss: 363.2598\n",
      "Iteration 4200/28125, Loss: 378.3000\n",
      "Iteration 4300/28125, Loss: 358.4556\n",
      "Iteration 4400/28125, Loss: 364.2749\n",
      "Iteration 4500/28125, Loss: 369.5395\n",
      "Iteration 4600/28125, Loss: 370.5889\n",
      "Iteration 4700/28125, Loss: 347.4546\n",
      "Iteration 4800/28125, Loss: 363.2213\n",
      "Iteration 4900/28125, Loss: 346.0728\n",
      "Iteration 5000/28125, Loss: 376.3335\n",
      "Iteration 5100/28125, Loss: 363.3423\n",
      "Iteration 5200/28125, Loss: 412.0695\n",
      "Iteration 5300/28125, Loss: 342.4389\n",
      "Iteration 5400/28125, Loss: 398.2128\n",
      "Iteration 5500/28125, Loss: 365.5058\n",
      "Iteration 5600/28125, Loss: 383.7901\n",
      "Iteration 5700/28125, Loss: 368.1256\n",
      "Iteration 5800/28125, Loss: 345.3959\n",
      "Iteration 5900/28125, Loss: 370.2558\n",
      "Iteration 6000/28125, Loss: 387.9218\n",
      "Iteration 6100/28125, Loss: 375.7413\n",
      "Iteration 6200/28125, Loss: 359.9791\n",
      "Iteration 6300/28125, Loss: 392.8645\n",
      "Iteration 6400/28125, Loss: 374.2735\n",
      "Iteration 6500/28125, Loss: 388.0713\n",
      "Iteration 6600/28125, Loss: 391.4925\n",
      "Iteration 6700/28125, Loss: 394.3676\n",
      "Iteration 6800/28125, Loss: 417.5550\n",
      "Iteration 6900/28125, Loss: 351.0921\n",
      "Iteration 7000/28125, Loss: 360.2615\n",
      "Iteration 7100/28125, Loss: 383.2189\n",
      "Iteration 7200/28125, Loss: 366.1303\n",
      "Iteration 7300/28125, Loss: 407.7180\n",
      "Iteration 7400/28125, Loss: 354.8377\n",
      "Iteration 7500/28125, Loss: 343.1709\n",
      "Iteration 7600/28125, Loss: 363.8154\n",
      "Iteration 7700/28125, Loss: 351.2472\n",
      "Iteration 7800/28125, Loss: 352.4301\n",
      "Iteration 7900/28125, Loss: 394.2183\n",
      "Iteration 8000/28125, Loss: 384.0168\n",
      "Iteration 8100/28125, Loss: 345.7952\n",
      "Iteration 8200/28125, Loss: 374.2222\n",
      "Iteration 8300/28125, Loss: 339.4429\n",
      "Iteration 8400/28125, Loss: 370.5265\n",
      "Iteration 8500/28125, Loss: 362.5645\n",
      "Iteration 8600/28125, Loss: 381.8942\n",
      "Iteration 8700/28125, Loss: 386.4273\n",
      "Iteration 8800/28125, Loss: 368.1043\n",
      "Iteration 8900/28125, Loss: 398.2721\n",
      "Iteration 9000/28125, Loss: 387.3932\n",
      "Iteration 9100/28125, Loss: 374.5283\n",
      "Iteration 9200/28125, Loss: 380.4220\n",
      "Iteration 9300/28125, Loss: 389.6253\n",
      "Iteration 9400/28125, Loss: 359.9920\n",
      "Iteration 9500/28125, Loss: 357.1976\n",
      "Iteration 9600/28125, Loss: 361.9458\n",
      "Iteration 9700/28125, Loss: 358.4566\n",
      "Iteration 9800/28125, Loss: 369.5226\n",
      "Iteration 9900/28125, Loss: 354.7897\n",
      "Iteration 10000/28125, Loss: 368.8395\n",
      "Iteration 10100/28125, Loss: 362.1424\n",
      "Iteration 10200/28125, Loss: 376.3515\n",
      "Iteration 10300/28125, Loss: 352.7392\n",
      "Iteration 10400/28125, Loss: 376.8769\n",
      "Iteration 10500/28125, Loss: 371.6215\n",
      "Iteration 10600/28125, Loss: 372.4839\n",
      "Iteration 10700/28125, Loss: 389.6623\n",
      "Iteration 10800/28125, Loss: 360.9421\n",
      "Iteration 10900/28125, Loss: 388.4818\n",
      "Iteration 11000/28125, Loss: 370.8086\n",
      "Iteration 11100/28125, Loss: 373.6742\n",
      "Iteration 11200/28125, Loss: 386.2894\n",
      "Iteration 11300/28125, Loss: 402.8483\n",
      "Iteration 11400/28125, Loss: 378.9045\n",
      "Iteration 11500/28125, Loss: 397.3497\n",
      "Iteration 11600/28125, Loss: 361.5121\n",
      "Iteration 11700/28125, Loss: 397.5983\n",
      "Iteration 11800/28125, Loss: 367.6814\n",
      "Iteration 11900/28125, Loss: 355.0074\n",
      "Iteration 12000/28125, Loss: 366.2894\n",
      "Iteration 12100/28125, Loss: 354.8300\n",
      "Iteration 12200/28125, Loss: 365.5725\n",
      "Iteration 12300/28125, Loss: 360.9543\n",
      "Iteration 12400/28125, Loss: 366.7466\n",
      "Iteration 12500/28125, Loss: 358.3790\n",
      "Iteration 12600/28125, Loss: 382.6938\n",
      "Iteration 12700/28125, Loss: 370.9000\n",
      "Iteration 12800/28125, Loss: 380.0929\n",
      "Iteration 12900/28125, Loss: 378.1596\n",
      "Iteration 13000/28125, Loss: 354.6877\n",
      "Iteration 13100/28125, Loss: 367.8506\n",
      "Iteration 13200/28125, Loss: 356.3374\n",
      "Iteration 13300/28125, Loss: 403.0968\n",
      "Iteration 13400/28125, Loss: 403.8655\n",
      "Iteration 13500/28125, Loss: 400.3257\n",
      "Iteration 13600/28125, Loss: 393.0779\n",
      "Iteration 13700/28125, Loss: 356.7425\n",
      "Iteration 13800/28125, Loss: 341.0555\n",
      "Iteration 13900/28125, Loss: 373.6146\n",
      "Iteration 14000/28125, Loss: 398.2956\n",
      "Iteration 14100/28125, Loss: 383.8698\n",
      "Iteration 14200/28125, Loss: 372.6396\n",
      "Iteration 14300/28125, Loss: 376.8993\n",
      "Iteration 14400/28125, Loss: 375.5710\n",
      "Iteration 14500/28125, Loss: 357.8183\n",
      "Iteration 14600/28125, Loss: 374.7456\n",
      "Iteration 14700/28125, Loss: 381.9723\n",
      "Iteration 14800/28125, Loss: 369.2599\n",
      "Iteration 14900/28125, Loss: 372.7180\n",
      "Iteration 15000/28125, Loss: 348.4632\n",
      "Iteration 15100/28125, Loss: 364.5403\n",
      "Iteration 15200/28125, Loss: 368.1526\n",
      "Iteration 15300/28125, Loss: 367.3544\n",
      "Iteration 15400/28125, Loss: 363.4330\n",
      "Iteration 15500/28125, Loss: 369.6360\n",
      "Iteration 15600/28125, Loss: 411.1620\n",
      "Iteration 15700/28125, Loss: 356.5961\n",
      "Iteration 15800/28125, Loss: 382.4520\n",
      "Iteration 15900/28125, Loss: 348.9770\n",
      "Iteration 16000/28125, Loss: 387.1202\n",
      "Iteration 16100/28125, Loss: 371.4865\n",
      "Iteration 16200/28125, Loss: 368.4251\n",
      "Iteration 16300/28125, Loss: 362.7641\n",
      "Iteration 16400/28125, Loss: 370.6212\n",
      "Iteration 16500/28125, Loss: 401.5047\n",
      "Iteration 16600/28125, Loss: 396.4050\n",
      "Iteration 16700/28125, Loss: 359.7975\n",
      "Iteration 16800/28125, Loss: 390.5841\n",
      "Iteration 16900/28125, Loss: 354.7431\n",
      "Iteration 17000/28125, Loss: 392.8122\n",
      "Iteration 17100/28125, Loss: 366.4187\n",
      "Iteration 17200/28125, Loss: 364.1204\n",
      "Iteration 17300/28125, Loss: 365.1228\n",
      "Iteration 17400/28125, Loss: 371.0476\n",
      "Iteration 17500/28125, Loss: 360.1543\n",
      "Iteration 17600/28125, Loss: 394.1764\n",
      "Iteration 17700/28125, Loss: 372.9761\n",
      "Iteration 17800/28125, Loss: 357.6381\n",
      "Iteration 17900/28125, Loss: 380.8505\n",
      "Iteration 18000/28125, Loss: 364.4142\n",
      "Iteration 18100/28125, Loss: 368.8616\n",
      "Iteration 18200/28125, Loss: 389.9883\n",
      "Iteration 18300/28125, Loss: 373.6747\n",
      "Iteration 18400/28125, Loss: 373.1843\n",
      "Iteration 18500/28125, Loss: 350.7602\n",
      "Iteration 18600/28125, Loss: 368.1332\n",
      "Iteration 18700/28125, Loss: 385.3342\n",
      "Iteration 18800/28125, Loss: 359.4848\n",
      "Iteration 18900/28125, Loss: 379.5542\n",
      "Iteration 19000/28125, Loss: 378.3512\n",
      "Iteration 19100/28125, Loss: 367.6298\n",
      "Iteration 19200/28125, Loss: 389.6643\n",
      "Iteration 19300/28125, Loss: 366.1705\n",
      "Iteration 19400/28125, Loss: 400.4457\n",
      "Iteration 19500/28125, Loss: 395.3786\n",
      "Iteration 19600/28125, Loss: 368.6206\n",
      "Iteration 19700/28125, Loss: 379.7862\n",
      "Iteration 19800/28125, Loss: 385.3000\n",
      "Iteration 19900/28125, Loss: 363.9931\n",
      "Iteration 20000/28125, Loss: 368.5470\n",
      "Iteration 20100/28125, Loss: 337.6329\n",
      "Iteration 20200/28125, Loss: 398.9050\n",
      "Iteration 20300/28125, Loss: 357.2535\n",
      "Iteration 20400/28125, Loss: 374.4518\n",
      "Iteration 20500/28125, Loss: 391.4761\n",
      "Iteration 20600/28125, Loss: 374.3960\n",
      "Iteration 20700/28125, Loss: 369.5663\n",
      "Iteration 20800/28125, Loss: 365.6371\n",
      "Iteration 20900/28125, Loss: 412.2964\n",
      "Iteration 21000/28125, Loss: 380.7263\n",
      "Iteration 21100/28125, Loss: 388.7905\n",
      "Iteration 21200/28125, Loss: 383.1410\n",
      "Iteration 21300/28125, Loss: 400.5295\n",
      "Iteration 21400/28125, Loss: 360.3555\n",
      "Iteration 21500/28125, Loss: 380.7001\n",
      "Iteration 21600/28125, Loss: 376.7485\n",
      "Iteration 21700/28125, Loss: 375.1460\n",
      "Iteration 21800/28125, Loss: 389.0702\n",
      "Iteration 21900/28125, Loss: 367.3925\n",
      "Iteration 22000/28125, Loss: 359.2129\n",
      "Iteration 22100/28125, Loss: 386.9346\n",
      "Iteration 22200/28125, Loss: 365.2898\n",
      "Iteration 22300/28125, Loss: 361.5713\n",
      "Iteration 22400/28125, Loss: 362.2551\n",
      "Iteration 22500/28125, Loss: 392.3873\n",
      "Iteration 22600/28125, Loss: 349.3662\n",
      "Iteration 22700/28125, Loss: 356.4303\n",
      "Iteration 22800/28125, Loss: 391.7979\n",
      "Iteration 22900/28125, Loss: 376.2220\n",
      "Iteration 23000/28125, Loss: 364.8989\n",
      "Iteration 23100/28125, Loss: 357.6030\n",
      "Iteration 23200/28125, Loss: 371.2663\n",
      "Iteration 23300/28125, Loss: 364.6379\n",
      "Iteration 23400/28125, Loss: 368.5745\n",
      "Iteration 23500/28125, Loss: 379.3967\n",
      "Iteration 23600/28125, Loss: 376.2755\n",
      "Iteration 23700/28125, Loss: 392.1365\n",
      "Iteration 23800/28125, Loss: 363.7649\n",
      "Iteration 23900/28125, Loss: 367.2380\n",
      "Iteration 24000/28125, Loss: 370.3200\n",
      "Iteration 24100/28125, Loss: 378.8507\n",
      "Iteration 24200/28125, Loss: 394.7694\n",
      "Iteration 24300/28125, Loss: 421.5443\n",
      "Iteration 24400/28125, Loss: 359.1564\n",
      "Iteration 24500/28125, Loss: 364.7721\n",
      "Iteration 24600/28125, Loss: 391.3917\n",
      "Iteration 24700/28125, Loss: 359.3479\n",
      "Iteration 24800/28125, Loss: 371.6731\n",
      "Iteration 24900/28125, Loss: 371.8287\n",
      "Iteration 25000/28125, Loss: 381.4092\n",
      "Iteration 25100/28125, Loss: 397.5944\n",
      "Iteration 25200/28125, Loss: 378.2854\n",
      "Iteration 25300/28125, Loss: 391.4069\n",
      "Iteration 25400/28125, Loss: 362.5146\n",
      "Iteration 25500/28125, Loss: 380.0601\n",
      "Iteration 25600/28125, Loss: 355.4373\n",
      "Iteration 25700/28125, Loss: 371.4949\n",
      "Iteration 25800/28125, Loss: 378.9588\n",
      "Iteration 25900/28125, Loss: 355.8275\n",
      "Iteration 26000/28125, Loss: 368.3476\n",
      "Iteration 26100/28125, Loss: 364.5557\n",
      "Iteration 26200/28125, Loss: 368.1100\n",
      "Iteration 26300/28125, Loss: 427.8071\n",
      "Iteration 26400/28125, Loss: 369.0758\n",
      "Iteration 26500/28125, Loss: 386.3366\n",
      "Iteration 26600/28125, Loss: 395.6574\n",
      "Iteration 26700/28125, Loss: 373.6252\n",
      "Iteration 26800/28125, Loss: 386.0346\n",
      "Iteration 26900/28125, Loss: 374.2273\n",
      "Iteration 27000/28125, Loss: 357.2681\n",
      "Iteration 27100/28125, Loss: 352.4947\n",
      "Iteration 27200/28125, Loss: 346.3693\n",
      "Iteration 27300/28125, Loss: 368.9870\n",
      "Iteration 27400/28125, Loss: 411.8262\n",
      "Iteration 27500/28125, Loss: 386.0249\n",
      "Iteration 27600/28125, Loss: 383.9895\n",
      "Iteration 27700/28125, Loss: 353.1546\n",
      "Iteration 27800/28125, Loss: 357.2482\n",
      "Iteration 27900/28125, Loss: 378.6626\n",
      "Iteration 28000/28125, Loss: 346.9621\n",
      "Iteration 28100/28125, Loss: 383.0499\n",
      "Training completed. 75 epochs, 375 iterations per epoch.\n",
      "Iteration 100/14025, Loss: 600.9059\n",
      "Iteration 200/14025, Loss: 639.9553\n",
      "Iteration 300/14025, Loss: 673.6982\n",
      "Iteration 400/14025, Loss: 664.5944\n",
      "Iteration 500/14025, Loss: 659.9958\n",
      "Iteration 600/14025, Loss: 661.6869\n",
      "Iteration 700/14025, Loss: 663.2364\n",
      "Iteration 800/14025, Loss: 677.6602\n",
      "Iteration 900/14025, Loss: 632.8431\n",
      "Iteration 1000/14025, Loss: 649.1383\n",
      "Iteration 1100/14025, Loss: 677.6146\n",
      "Iteration 1200/14025, Loss: 654.2707\n",
      "Iteration 1300/14025, Loss: 633.1156\n",
      "Iteration 1400/14025, Loss: 696.3963\n",
      "Iteration 1500/14025, Loss: 652.2802\n",
      "Iteration 1600/14025, Loss: 678.3913\n",
      "Iteration 1700/14025, Loss: 622.1391\n",
      "Iteration 1800/14025, Loss: 661.8632\n",
      "Iteration 1900/14025, Loss: 659.5039\n",
      "Iteration 2000/14025, Loss: 667.2152\n",
      "Iteration 2100/14025, Loss: 652.5613\n",
      "Iteration 2200/14025, Loss: 677.7371\n",
      "Iteration 2300/14025, Loss: 671.6793\n",
      "Iteration 2400/14025, Loss: 641.5830\n",
      "Iteration 2500/14025, Loss: 654.5919\n",
      "Iteration 2600/14025, Loss: 709.2401\n",
      "Iteration 2700/14025, Loss: 715.0518\n",
      "Iteration 2800/14025, Loss: 719.2922\n",
      "Iteration 2900/14025, Loss: 641.4719\n",
      "Iteration 3000/14025, Loss: 668.7051\n",
      "Iteration 3100/14025, Loss: 665.7880\n",
      "Iteration 3200/14025, Loss: 660.7514\n",
      "Iteration 3300/14025, Loss: 686.4120\n",
      "Iteration 3400/14025, Loss: 724.2581\n",
      "Iteration 3500/14025, Loss: 671.0663\n",
      "Iteration 3600/14025, Loss: 653.8568\n",
      "Iteration 3700/14025, Loss: 677.0712\n",
      "Iteration 3800/14025, Loss: 687.3619\n",
      "Iteration 3900/14025, Loss: 696.4070\n",
      "Iteration 4000/14025, Loss: 681.8547\n",
      "Iteration 4100/14025, Loss: 676.6965\n",
      "Iteration 4200/14025, Loss: 653.4262\n",
      "Iteration 4300/14025, Loss: 696.0855\n",
      "Iteration 4400/14025, Loss: 665.8467\n",
      "Iteration 4500/14025, Loss: 631.9658\n",
      "Iteration 4600/14025, Loss: 693.1220\n",
      "Iteration 4700/14025, Loss: 686.8976\n",
      "Iteration 4800/14025, Loss: 687.1660\n",
      "Iteration 4900/14025, Loss: 673.1011\n",
      "Iteration 5000/14025, Loss: 647.9312\n",
      "Iteration 5100/14025, Loss: 661.1693\n",
      "Iteration 5200/14025, Loss: 727.1195\n",
      "Iteration 5300/14025, Loss: 666.3893\n",
      "Iteration 5400/14025, Loss: 636.6282\n",
      "Iteration 5500/14025, Loss: 656.3240\n",
      "Iteration 5600/14025, Loss: 684.8009\n",
      "Iteration 5700/14025, Loss: 643.6666\n",
      "Iteration 5800/14025, Loss: 644.5474\n",
      "Iteration 5900/14025, Loss: 692.6652\n",
      "Iteration 6000/14025, Loss: 674.1515\n",
      "Iteration 6100/14025, Loss: 684.8905\n",
      "Iteration 6200/14025, Loss: 651.7709\n",
      "Iteration 6300/14025, Loss: 664.7174\n",
      "Iteration 6400/14025, Loss: 660.3002\n",
      "Iteration 6500/14025, Loss: 684.8728\n",
      "Iteration 6600/14025, Loss: 686.7724\n",
      "Iteration 6700/14025, Loss: 662.1234\n",
      "Iteration 6800/14025, Loss: 670.2859\n",
      "Iteration 6900/14025, Loss: 628.8880\n",
      "Iteration 7000/14025, Loss: 697.8483\n",
      "Iteration 7100/14025, Loss: 680.9601\n",
      "Iteration 7200/14025, Loss: 686.4430\n",
      "Iteration 7300/14025, Loss: 657.3672\n",
      "Iteration 7400/14025, Loss: 678.4222\n",
      "Iteration 7500/14025, Loss: 633.5139\n",
      "Iteration 7600/14025, Loss: 639.9609\n",
      "Iteration 7700/14025, Loss: 696.5740\n",
      "Iteration 7800/14025, Loss: 649.0294\n",
      "Iteration 7900/14025, Loss: 711.0107\n",
      "Iteration 8000/14025, Loss: 712.3026\n",
      "Iteration 8100/14025, Loss: 657.7472\n",
      "Iteration 8200/14025, Loss: 679.1052\n",
      "Iteration 8300/14025, Loss: 692.2552\n",
      "Iteration 8400/14025, Loss: 688.4472\n",
      "Iteration 8500/14025, Loss: 622.8830\n",
      "Iteration 8600/14025, Loss: 661.4447\n",
      "Iteration 8700/14025, Loss: 637.8514\n",
      "Iteration 8800/14025, Loss: 666.8980\n",
      "Iteration 8900/14025, Loss: 678.3998\n",
      "Iteration 9000/14025, Loss: 689.5409\n",
      "Iteration 9100/14025, Loss: 673.6953\n",
      "Iteration 9200/14025, Loss: 645.2854\n",
      "Iteration 9300/14025, Loss: 627.2537\n",
      "Iteration 9400/14025, Loss: 683.5509\n",
      "Iteration 9500/14025, Loss: 688.1235\n",
      "Iteration 9600/14025, Loss: 684.1083\n",
      "Iteration 9700/14025, Loss: 681.4013\n",
      "Iteration 9800/14025, Loss: 684.8213\n",
      "Iteration 9900/14025, Loss: 672.9527\n",
      "Iteration 10000/14025, Loss: 680.8268\n",
      "Iteration 10100/14025, Loss: 673.4590\n",
      "Iteration 10200/14025, Loss: 704.5024\n",
      "Iteration 10300/14025, Loss: 652.6345\n",
      "Iteration 10400/14025, Loss: 656.2913\n",
      "Iteration 10500/14025, Loss: 676.9710\n",
      "Iteration 10600/14025, Loss: 686.2595\n",
      "Iteration 10700/14025, Loss: 619.8584\n",
      "Iteration 10800/14025, Loss: 685.2067\n",
      "Iteration 10900/14025, Loss: 685.8169\n",
      "Iteration 11000/14025, Loss: 646.0179\n",
      "Iteration 11100/14025, Loss: 675.2880\n",
      "Iteration 11200/14025, Loss: 643.6437\n",
      "Iteration 11300/14025, Loss: 698.4004\n",
      "Iteration 11400/14025, Loss: 683.0067\n",
      "Iteration 11500/14025, Loss: 697.8648\n",
      "Iteration 11600/14025, Loss: 647.8775\n",
      "Iteration 11700/14025, Loss: 647.4776\n",
      "Iteration 11800/14025, Loss: 654.5521\n",
      "Iteration 11900/14025, Loss: 662.0919\n",
      "Iteration 12000/14025, Loss: 696.8605\n",
      "Iteration 12100/14025, Loss: 687.2148\n",
      "Iteration 12200/14025, Loss: 709.9299\n",
      "Iteration 12300/14025, Loss: 673.0278\n",
      "Iteration 12400/14025, Loss: 635.9757\n",
      "Iteration 12500/14025, Loss: 664.4109\n",
      "Iteration 12600/14025, Loss: 686.5425\n",
      "Iteration 12700/14025, Loss: 708.3637\n",
      "Iteration 12800/14025, Loss: 696.3170\n",
      "Iteration 12900/14025, Loss: 651.6409\n",
      "Iteration 13000/14025, Loss: 662.6676\n",
      "Iteration 13100/14025, Loss: 665.9820\n",
      "Iteration 13200/14025, Loss: 665.8139\n",
      "Iteration 13300/14025, Loss: 667.2993\n",
      "Iteration 13400/14025, Loss: 711.6664\n",
      "Iteration 13500/14025, Loss: 654.0815\n",
      "Iteration 13600/14025, Loss: 679.8933\n",
      "Iteration 13700/14025, Loss: 654.8439\n",
      "Iteration 13800/14025, Loss: 704.9190\n",
      "Iteration 13900/14025, Loss: 662.1627\n",
      "Iteration 14000/14025, Loss: 680.7003\n",
      "Training completed. 75 epochs, 187 iterations per epoch.\n",
      "Iteration 100/6975, Loss: 1054.2262\n",
      "Iteration 200/6975, Loss: 1272.7159\n",
      "Iteration 300/6975, Loss: 1276.6455\n",
      "Iteration 400/6975, Loss: 1330.4765\n",
      "Iteration 500/6975, Loss: 1351.0210\n",
      "Iteration 600/6975, Loss: 1300.9014\n",
      "Iteration 700/6975, Loss: 1274.7793\n",
      "Iteration 800/6975, Loss: 1380.7120\n",
      "Iteration 900/6975, Loss: 1400.0355\n",
      "Iteration 1000/6975, Loss: 1384.7065\n",
      "Iteration 1100/6975, Loss: 1377.8534\n",
      "Iteration 1200/6975, Loss: 1298.9909\n",
      "Iteration 1300/6975, Loss: 1424.3654\n",
      "Iteration 1400/6975, Loss: 1377.8727\n",
      "Iteration 1500/6975, Loss: 1318.7186\n",
      "Iteration 1600/6975, Loss: 1334.0234\n",
      "Iteration 1700/6975, Loss: 1492.2866\n",
      "Iteration 1800/6975, Loss: 1342.4221\n",
      "Iteration 1900/6975, Loss: 1316.0655\n",
      "Iteration 2000/6975, Loss: 1287.0214\n",
      "Iteration 2100/6975, Loss: 1307.3363\n",
      "Iteration 2200/6975, Loss: 1413.5337\n",
      "Iteration 2300/6975, Loss: 1417.0921\n",
      "Iteration 2400/6975, Loss: 1387.4154\n",
      "Iteration 2500/6975, Loss: 1379.3606\n",
      "Iteration 2600/6975, Loss: 1465.6646\n",
      "Iteration 2700/6975, Loss: 1265.2284\n",
      "Iteration 2800/6975, Loss: 1340.2291\n",
      "Iteration 2900/6975, Loss: 1382.5881\n",
      "Iteration 3000/6975, Loss: 1465.6513\n",
      "Iteration 3100/6975, Loss: 1348.0933\n",
      "Iteration 3200/6975, Loss: 1365.9455\n",
      "Iteration 3300/6975, Loss: 1269.0871\n",
      "Iteration 3400/6975, Loss: 1385.6761\n",
      "Iteration 3500/6975, Loss: 1398.0972\n",
      "Iteration 3600/6975, Loss: 1400.9645\n",
      "Iteration 3700/6975, Loss: 1352.9858\n",
      "Iteration 3800/6975, Loss: 1298.4707\n",
      "Iteration 3900/6975, Loss: 1399.9050\n",
      "Iteration 4000/6975, Loss: 1363.4853\n",
      "Iteration 4100/6975, Loss: 1330.2233\n",
      "Iteration 4200/6975, Loss: 1330.6770\n",
      "Iteration 4300/6975, Loss: 1366.8689\n",
      "Iteration 4400/6975, Loss: 1456.7504\n",
      "Iteration 4500/6975, Loss: 1356.7560\n",
      "Iteration 4600/6975, Loss: 1362.6455\n",
      "Iteration 4700/6975, Loss: 1342.6329\n",
      "Iteration 4800/6975, Loss: 1265.8367\n",
      "Iteration 4900/6975, Loss: 1359.3406\n",
      "Iteration 5000/6975, Loss: 1399.8020\n",
      "Iteration 5100/6975, Loss: 1407.2394\n",
      "Iteration 5200/6975, Loss: 1312.2620\n",
      "Iteration 5300/6975, Loss: 1493.5192\n",
      "Iteration 5400/6975, Loss: 1296.8435\n",
      "Iteration 5500/6975, Loss: 1300.5020\n",
      "Iteration 5600/6975, Loss: 1307.4037\n",
      "Iteration 5700/6975, Loss: 1349.4408\n",
      "Iteration 5800/6975, Loss: 1330.9227\n",
      "Iteration 5900/6975, Loss: 1322.4117\n",
      "Iteration 6000/6975, Loss: 1376.5748\n",
      "Iteration 6100/6975, Loss: 1312.3476\n",
      "Iteration 6200/6975, Loss: 1306.0944\n",
      "Iteration 6300/6975, Loss: 1395.7279\n",
      "Iteration 6400/6975, Loss: 1437.2506\n",
      "Iteration 6500/6975, Loss: 1327.6880\n",
      "Iteration 6600/6975, Loss: 1367.5110\n",
      "Iteration 6700/6975, Loss: 1386.6673\n",
      "Iteration 6800/6975, Loss: 1310.3014\n",
      "Iteration 6900/6975, Loss: 1392.0133\n",
      "Training completed. 75 epochs, 93 iterations per epoch.\n",
      "Iteration 100/3450, Loss: 2552.9650\n",
      "Iteration 200/3450, Loss: 2968.5604\n",
      "Iteration 300/3450, Loss: 3150.0220\n",
      "Iteration 400/3450, Loss: 3337.9574\n",
      "Iteration 500/3450, Loss: 3261.4847\n",
      "Iteration 600/3450, Loss: 3196.2618\n",
      "Iteration 700/3450, Loss: 3443.2589\n",
      "Iteration 800/3450, Loss: 3221.9445\n",
      "Iteration 900/3450, Loss: 3479.7635\n",
      "Iteration 1000/3450, Loss: 3220.4742\n",
      "Iteration 1100/3450, Loss: 3233.0852\n",
      "Iteration 1200/3450, Loss: 3372.8592\n",
      "Iteration 1300/3450, Loss: 3265.5570\n",
      "Iteration 1400/3450, Loss: 3290.4694\n",
      "Iteration 1500/3450, Loss: 3355.1336\n",
      "Iteration 1600/3450, Loss: 3224.1874\n",
      "Iteration 1700/3450, Loss: 3192.7609\n",
      "Iteration 1800/3450, Loss: 3205.2496\n",
      "Iteration 1900/3450, Loss: 3274.7423\n",
      "Iteration 2000/3450, Loss: 3176.1562\n",
      "Iteration 2100/3450, Loss: 3241.4309\n",
      "Iteration 2200/3450, Loss: 3347.2150\n",
      "Iteration 2300/3450, Loss: 3200.0504\n",
      "Iteration 2400/3450, Loss: 3014.9981\n",
      "Iteration 2500/3450, Loss: 3323.3448\n",
      "Iteration 2600/3450, Loss: 3234.9897\n",
      "Iteration 2700/3450, Loss: 3311.9994\n",
      "Iteration 2800/3450, Loss: 3308.5829\n",
      "Iteration 2900/3450, Loss: 3054.4479\n",
      "Iteration 3000/3450, Loss: 3240.3176\n",
      "Iteration 3100/3450, Loss: 3120.1149\n",
      "Iteration 3200/3450, Loss: 3188.2955\n",
      "Iteration 3300/3450, Loss: 3236.4574\n",
      "Iteration 3400/3450, Loss: 3296.2409\n",
      "Training completed. 75 epochs, 46 iterations per epoch.\n",
      "Iteration 100/1725, Loss: 7357.2232\n",
      "Iteration 200/1725, Loss: 8744.0943\n",
      "Iteration 300/1725, Loss: 9604.9982\n",
      "Iteration 400/1725, Loss: 9637.7794\n",
      "Iteration 500/1725, Loss: 9320.6832\n",
      "Iteration 600/1725, Loss: 9166.6081\n",
      "Iteration 700/1725, Loss: 9588.3692\n",
      "Iteration 800/1725, Loss: 9388.5067\n",
      "Iteration 900/1725, Loss: 10110.0685\n",
      "Iteration 1000/1725, Loss: 9373.9574\n",
      "Iteration 1100/1725, Loss: 9881.3370\n",
      "Iteration 1200/1725, Loss: 9471.0239\n",
      "Iteration 1300/1725, Loss: 9397.9497\n",
      "Iteration 1400/1725, Loss: 9356.7241\n",
      "Iteration 1500/1725, Loss: 10201.3333\n",
      "Iteration 1600/1725, Loss: 9883.9982\n",
      "Iteration 1700/1725, Loss: 9035.7207\n",
      "Training completed. 75 epochs, 23 iterations per epoch.\n",
      "Iteration 100/825, Loss: 23949.6812\n",
      "Iteration 200/825, Loss: 31012.2324\n",
      "Iteration 300/825, Loss: 33737.0218\n",
      "Iteration 400/825, Loss: 33589.0624\n",
      "Iteration 500/825, Loss: 32365.0154\n",
      "Iteration 600/825, Loss: 31819.6340\n",
      "Iteration 700/825, Loss: 32054.7116\n",
      "Iteration 800/825, Loss: 32428.3718\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Iteration 100/375, Loss: 86228.9641\n",
      "Iteration 200/375, Loss: 108831.1973\n",
      "Iteration 300/375, Loss: 115182.2202\n",
      "Training completed. 75 epochs, 5 iterations per epoch.\n",
      "Iteration 100/150, Loss: 342813.1884\n",
      "Training completed. 75 epochs, 2 iterations per epoch.\n",
      "\n",
      "Grid search complete!\n",
      "Best training accuracy: 0.7517\n",
      "Best validation accuracy: 0.3330\n",
      "Best hyperparameters: {'learning_rate': 0.0001, 'regularization': 0.1, 'batch_size': 256}\n"
     ]
    }
   ],
   "source": [
    "list_learing_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "list_reg = [0.0001, 0.001, 0.01, 0.1]\n",
    "list_batch = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "best_accuracy_train = 0\n",
    "best_accuracy_val = 0 \n",
    "best_params = {}\n",
    "num_epochs = 75\n",
    "\n",
    "for rate in list_learing_rate:\n",
    "    for reg in list_reg:\n",
    "        for batch in list_batch:\n",
    "            soft_mx = SoftmaxLayer(10)\n",
    "            soft_mx.fit(x_train, y_train, num_epochs, rate, batch, reg, 0)\n",
    "            y_pred_val = soft_mx.predict(x_val)\n",
    "            y_pred_train = soft_mx.predict(x_train) \n",
    "            acc_val = soft_mx.accuracy(y_val, y_pred_val)\n",
    "            acc_train = soft_mx.accuracy(y_train, y_pred_train)\n",
    "            if acc_val >= best_accuracy_val:\n",
    "                best_accuracy_val = acc_val\n",
    "                best_accuracy_train = acc_train\n",
    "                best_params = {\"learning_rate\": rate, \"regularization\": reg, \"batch_size\": batch}\n",
    "                print(\"train accuracy:\", acc_train)\n",
    "                print(\"val accuracy:\", best_accuracy_val)\n",
    "                print(\"batch_size:\", batch, \"reg:\", reg, \"rate:\", rate)\n",
    "\n",
    "# Print the final best hyperparameters and accuracies\n",
    "print(\"\\nGrid search complete!\")\n",
    "print(f\"Best training accuracy: {best_accuracy_train:.4f}\")\n",
    "print(f\"Best validation accuracy: {best_accuracy_val:.4f}\")\n",
    "print(f\"Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2h. Verify reproducibility of results  \n",
    "\n",
    "Train a fresh net with the best hyperparameter values that you found in your search give you the best validation accuracy above.\n",
    "\n",
    "Print out the accuracy on the validation set. It should match exactly your printout above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d80c7d4c55573d2062249025f9c669da",
     "grade": false,
     "grade_id": "cell-6c9140bcd6974b1a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy with best hyperparameters: 0.3330\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters from the grid search\n",
    "best_learning_rate = 0.0001\n",
    "best_regularization = 0.1\n",
    "best_batch_size = 256\n",
    "num_epochs = 75  # Training duration\n",
    "\n",
    "# Initialize a new softmax model\n",
    "softmax_model = SoftmaxLayer(10)\n",
    "\n",
    "# Train the model using the best hyperparameters\n",
    "softmax_model.fit(x_train, y_train, num_epochs, best_learning_rate, best_batch_size, best_regularization, verbose=0, r_seed = 0)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = softmax_model.predict(x_val)\n",
    "\n",
    "# Compute accuracy on the validation set\n",
    "val_accuracy = softmax_model.accuracy(y_val, y_val_pred)\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(f\"Validation accuracy with best hyperparameters: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2i. Evaluate best model on test set\n",
    "\n",
    "**Question 5:** Now that you have \"good\" parameter values recorded, train a new model with the best learning rate, regularization strength, and batch size values in the cell below. What accuracy do you get on the **test set**? How does this compare to chance performance?\n",
    "\n",
    "*Recall: The test set should NOT be used in your grid search. It should only be processed once AFTER you conclude your grid search.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e26be878580b5029686c19c29b277ae",
     "grade": true,
     "grade_id": "cell-86e0a0c4c82d2789",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "33.60%, which is slightly higher than the 33.30% from the validation set. In this case, since there are 10 classes, there is a 10% chance if randomly guessing by chance, so model accuracy is significantly higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99e41f8211e11b63003efcdb326f9715",
     "grade": false,
     "grade_id": "cell-85e56e6b49ea491b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/825, Loss: 1.5763\n",
      "Iteration 200/825, Loss: 1.5532\n",
      "Iteration 300/825, Loss: 1.4926\n",
      "Iteration 400/825, Loss: 1.5757\n",
      "Iteration 500/825, Loss: 1.5435\n",
      "Iteration 600/825, Loss: 1.5947\n",
      "Iteration 700/825, Loss: 1.6503\n",
      "Iteration 800/825, Loss: 1.7705\n",
      "Training completed. 75 epochs, 11 iterations per epoch.\n",
      "Test set accuracy: 0.3360\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "regularization = 0.1\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 75\n",
    "\n",
    "bestNet = SoftmaxLayer(10)\n",
    "\n",
    "bestNet.fit(x_train, y_train, num_epochs, learning_rate, batch_size, regularization, verbose=1, r_seed = 0)\n",
    "\n",
    "y_val_pred = bestNet.predict(x_val)\n",
    "y_train_pred = bestNet.predict(x_train)\n",
    "y_test_pred = bestNet.predict(x_test)\n",
    "\n",
    "test_accuracy = bestNet.accuracy(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test set accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2j. Visualize learned weights\n",
    "\n",
    "Run the following code that plots the network weights going to each output neuron. If all goes well, you should see something really cool! Include the plot in your submitted project to show me what you got!\n",
    "\n",
    "**Note:** the quality of your visualizations will depend on:\n",
    "- The quality of the hyperparameters that you got via grid search.\n",
    "- How many epochs that you trained the network before plotting the weights\n",
    "\n",
    "One extension idea: is to find the combination of the above that result in the best visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the class names\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)\n",
    "\n",
    "# We don't care about the bias wt\n",
    "wts = bestNet.wts\n",
    "# Reshape the wt vectors into spatial 'image' configurations to visualization\n",
    "wts = wts.reshape(32, 32, 3, 10)\n",
    "\n",
    "# Make a large new empty figure/plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Loop through each output neuron\n",
    "for i in range(10):\n",
    "  # Make a 2x5 grid of images\n",
    "  plt.subplot(2, 5, i+1)\n",
    "\n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  currImg = 255.0 * (wts[:, :, :, i].squeeze() - np.min(wts)) / (np.max(wts) - np.min(wts))\n",
    "\n",
    "  plt.imshow(currImg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])\n",
    "plt.subplots_adjust(wspace=0.35, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
