{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veer Khosla\n",
    "\n",
    "CS 251: Data Analysis and Visualization\n",
    "\n",
    "Fall 2023\n",
    "\n",
    "Project 6: Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Preprocess full spam email dataset \n",
    "\n",
    "Before you build a Naive Bayes spam email classifier, run the full spam email dataset through your preprocessing code.\n",
    "\n",
    "Download and extract the full **Enron** emails (*zip file should be ~29MB large*). You should see a base `enron` folder, with `spam` and `ham` subfolders when you extract the zip file (these are the 2 classes).\n",
    "\n",
    "Run the test code below to check everything over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email_preprocessor as epp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `count_words` and `find_top_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq, num_emails = epp.count_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You found 32625 emails in the datset. You should have found 32625.\n"
     ]
    }
   ],
   "source": [
    "print(f'You found {num_emails} emails in the datset. You should have found 32625.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your top 10 words are\n",
      "['the', 'to', 'and', 'of', 'a', 'in', 'for', 'you', 'is', 'enron']\n",
      "and they should be\n",
      "['the', 'to', 'and', 'of', 'a', 'in', 'for', 'you', 'is', 'enron']\n",
      "The associated counts are\n",
      "[277459, 203659, 148873, 139578, 111796, 100961, 80765, 77592, 68097, 60852]\n",
      "and they should be\n",
      "[277459, 203659, 148873, 139578, 111796, 100961, 80765, 77592, 68097, 60852]\n"
     ]
    }
   ],
   "source": [
    "top_words, top_counts = epp.find_top_words(word_freq)\n",
    "print(f\"Your top 10 words are\\n{top_words[:10]}\\nand they should be\\n['the', 'to', 'and', 'of', 'a', 'in', 'for', 'you', 'is', 'enron']\")\n",
    "print(f\"The associated counts are\\n{top_counts[:10]}\\nand they should be\\n[277459, 203659, 148873, 139578, 111796, 100961, 80765, 77592, 68097, 60852]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Make feature and class vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, y = epp.make_feature_vectors(top_words, num_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify class label coding\n",
    "\n",
    "There are\n",
    "- 16544 `ham` emails\n",
    "- 16081 `spam` emails.\n",
    "\n",
    "In the cell below, print out the number of emails that have class label `0` and the number that have class label `1`.\n",
    "- The count for class label `0` should be 16544\n",
    "- The count for class label `1` should be 16081.\n",
    "\n",
    "If the counts across the labels are reversed, recode class label `0` as `1` and class label `1` as `0` in the label vector `y`. If you do this, print out the counts for each label again and verify you get the above counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count for class label 0: 16544\n",
      "Count for class label 1: 16081\n",
      "\n",
      "After recoding:\n",
      "Count for class label 0: 16081\n",
      "Count for class label 1: 16544\n"
     ]
    }
   ],
   "source": [
    "count_class_0 = np.sum(y == 0)\n",
    "count_class_1 = np.sum(y == 1)\n",
    "\n",
    "print(\"Count for class label 0:\", count_class_0)\n",
    "print(\"Count for class label 1:\", count_class_1)\n",
    "\n",
    "if count_class_0 > count_class_1:\n",
    "    y = 1 - y\n",
    "\n",
    "    count_class_0 = np.sum(y == 0)\n",
    "    count_class_1 = np.sum(y == 1)\n",
    "\n",
    "    print(\"\\nAfter recoding:\")\n",
    "    print(\"Count for class label 0:\", count_class_0)\n",
    "    print(\"Count for class label 1:\", count_class_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Make train and test splits of the dataset\n",
    "\n",
    "Here we divide the email features into a 80/20 train/test split (80% of data used to train the supervised learning model, 20% we withhold and use for testing / prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes for train/test splits:\n",
      "Train (26100, 200), classes (26100,)\n",
      "Test (6525, 200), classes (6525,)\n",
      "\n",
      "They should be:\n",
      "Train (26100, 200), classes (26100,)\n",
      "Test (6525, 200), classes (6525,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x_train, y_train, inds_train, x_test, y_test, inds_test = epp.make_train_test_sets(features, y)\n",
    "\n",
    "print('Shapes for train/test splits:')\n",
    "print(f'Train {x_train.shape}, classes {y_train.shape}')\n",
    "print(f'Test {x_test.shape}, classes {y_test.shape}')\n",
    "print('\\nThey should be:\\nTrain (26100, 200), classes (26100,)\\nTest (6525, 200), classes (6525,)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Save data in binary format\n",
    "\n",
    "It adds a lot of overhead to have to run through your raw email -> train/test feature split every time you wanted to work on your project! In this step, you will export the data in memory to disk in a binary format. That way, you can quickly load all the data back into memory (directly in ndarray format) whenever you want to work with it again. No need to parse from text files!\n",
    "\n",
    "Running the following cell uses numpy's `save` function to make six files in `.npy` format (e.g. `email_train_x.npy`, `email_train_y.npy`, `email_train_inds.npy`, `email_test_x.npy`, `email_test_y.npy`, `email_test_inds.npy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/email_train_x.npy', x_train)\n",
    "np.save('data/email_train_y.npy', y_train)\n",
    "np.save('data/email_train_inds.npy', inds_train)\n",
    "np.save('data/email_test_x.npy', x_test)\n",
    "np.save('data/email_test_y.npy', y_test)\n",
    "np.save('data/email_test_inds.npy', inds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Naive Bayes Classifier\n",
    "\n",
    "After finishing your email preprocessing pipeline, implement the one other supervised learning algorithm we we will use to classify email, **Naive Bayes**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Implement Naive Bayes\n",
    "\n",
    "In `naive_bayes.py`, implement the following methods:\n",
    "- Constructor\n",
    "- get methods\n",
    "- `train(data, y)`: Train the Naive Bayes classifier so that it records the \"statistics\" of the training set: class priors (i.e. how likely an email is in the training set to be spam or ham?) and the class likelihoods (the probability of a word appearing in each class â€” spam or ham).\n",
    "- `predict(data)`: Combine the class likelihoods and priors to compute the posterior distribution. The predicted class for a test sample is the class that yields the highest posterior probability.\n",
    "- `accuracy(y, y_pred)`: The usual definition :)\n",
    "\n",
    "\n",
    "#### Bayes rule ingredients: Priors and likelihood (`train`)\n",
    "\n",
    "To compute class predictions (probability that a test example belong to either spam or ham classes), we need to evaluate **Bayes Rule**. This means computing the priors and likelihoods based on the training data.\n",
    "\n",
    "**Prior:** $$P_c = \\frac{N_c}{N}$$ where $P_c$ is the prior for class $c$ (spam or ham), $N_c$ is the number of training samples that belong to class $c$ and $N$ is the total number of training samples.\n",
    "\n",
    "**Likelihood:** $$L_{c,w} = \\frac{T_{c,w} + 1}{T_{c} + M}$$ where\n",
    "- $L_{c,w}$ is the likelihood that word $w$ belongs to class $c$ (*i.e. what we are solving for*)\n",
    "- $T_{c,w}$ is the total count of **word $w$** in emails that are only in class $c$ (*either spam or ham*)\n",
    "- $T_{c}$ is the total count of **all words** that appear in emails of the class $c$ (*total number of words in all spam emails or total number of words in all ham emails*)\n",
    "- $M$ is the number of features (*number of top words*).\n",
    "\n",
    "#### Bayes rule ingredients: Posterior (`predict`)\n",
    "\n",
    "To make predictions, we now combine the prior and likelihood to get the posterior:\n",
    "\n",
    "**Log Posterior:** $$Log(\\text{Post}_{i, c}) = Log(P_c) + \\sum_{j \\in J_i}x_{i,j}Log(L_{c,j})$$\n",
    "\n",
    " where\n",
    "- $\\text{Post}_{i,c}$ is the posterior for class $c$ for test sample $i$(*i.e. evidence that email $i$ is spam or ham*). We solve for its logarithm.\n",
    "- $Log(P_c)$ is the logarithm of the prior for class $c$.\n",
    "- $x_{i,j}$ is the number of times the jth word appears in the ith email.\n",
    "- $Log(L_{c,j})$: is the log-likelihood of the jth word in class $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naive_bayes import NaiveBayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `train`\n",
    "\n",
    "###### Class priors and likelihoods\n",
    "\n",
    "The following test should be used only if storing the class priors and likelihoods directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your class priors are: [0.28 0.22 0.32 0.18]\n",
      "and should be          [0.28 0.22 0.32 0.18].\n",
      "Your class likelihoods shape is (4, 6) and should be (4, 6).\n",
      "Your likelihoods are:\n",
      "[[0.15997 0.15091 0.2079  0.19106 0.14184 0.14832]\n",
      " [0.11859 0.16821 0.17914 0.16905 0.18082 0.18419]\n",
      " [0.16884 0.17318 0.14495 0.14332 0.18784 0.18187]\n",
      " [0.16126 0.17011 0.15831 0.13963 0.18977 0.18092]]\n",
      "and should be\n",
      "[[0.15997 0.15091 0.2079  0.19106 0.14184 0.14832]\n",
      " [0.11859 0.16821 0.17914 0.16905 0.18082 0.18419]\n",
      " [0.16884 0.17318 0.14495 0.14332 0.18784 0.18187]\n",
      " [0.16126 0.17011 0.15831 0.13963 0.18977 0.18092]]\n"
     ]
    }
   ],
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_test = np.random.randint(low=0, high=20, size=(100, 6))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_test, y_test)\n",
    "\n",
    "print(f'Your class priors are: {nbc.get_priors()}\\nand should be          [0.28 0.22 0.32 0.18].')\n",
    "print(f'Your class likelihoods shape is {nbc.get_likelihoods().shape} and should be (4, 6).')\n",
    "print(f'Your likelihoods are:\\n{nbc.get_likelihoods()}')\n",
    "\n",
    "print(f'and should be')\n",
    "print('''[[0.15997 0.15091 0.2079  0.19106 0.14184 0.14832]\n",
    " [0.11859 0.16821 0.17914 0.16905 0.18082 0.18419]\n",
    " [0.16884 0.17318 0.14495 0.14332 0.18784 0.18187]\n",
    " [0.16126 0.17011 0.15831 0.13963 0.18977 0.18092]]''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Log of class priors and likelihoods\n",
    "\n",
    "This test should be used only if storing the log of the class priors and likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your log class priors are: [0.28 0.22 0.32 0.18]\n",
      "and should be              [-1.27297 -1.51413 -1.13943 -1.7148 ].\n",
      "Your log class likelihoods shape is (4, 6) and should be (4, 6).\n",
      "Your log likelihoods are:\n",
      "[[0.15997 0.15091 0.2079  0.19106 0.14184 0.14832]\n",
      " [0.11859 0.16821 0.17914 0.16905 0.18082 0.18419]\n",
      " [0.16884 0.17318 0.14495 0.14332 0.18784 0.18187]\n",
      " [0.16126 0.17011 0.15831 0.13963 0.18977 0.18092]]\n",
      "and should be\n",
      "[[-1.83274 -1.89109 -1.57069 -1.65516 -1.95306 -1.90841]\n",
      " [-2.13211 -1.78255 -1.71958 -1.77756 -1.71023 -1.6918 ]\n",
      " [-1.77881 -1.75342 -1.93136 -1.94266 -1.67217 -1.70448]\n",
      " [-1.82475 -1.77132 -1.84321 -1.96879 -1.66192 -1.70968]]\n"
     ]
    }
   ],
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_test = np.random.randint(low=0, high=20, size=(100, 6))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_test, y_test)\n",
    "\n",
    "print(f'Your log class priors are: {nbc.get_priors()}\\nand should be              [-1.27297 -1.51413 -1.13943 -1.7148 ].')\n",
    "print(f'Your log class likelihoods shape is {nbc.get_likelihoods().shape} and should be (4, 6).')\n",
    "print(f'Your log likelihoods are:\\n{nbc.get_likelihoods()}')\n",
    "\n",
    "\n",
    "print(f'and should be')\n",
    "print('''[[-1.83274 -1.89109 -1.57069 -1.65516 -1.95306 -1.90841]\n",
    " [-2.13211 -1.78255 -1.71958 -1.77756 -1.71023 -1.6918 ]\n",
    " [-1.77881 -1.75342 -1.93136 -1.94266 -1.67217 -1.70448]\n",
    " [-1.82475 -1.77132 -1.84321 -1.96879 -1.66192 -1.70968]]''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your predicted classes are\n",
      "[2 0 0 3 0 3 2 1 2 3 1 0 0 1 0]\n",
      "and should be\n",
      "[2 0 0 3 0 3 2 1 2 3 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "num_test_classes = 4\n",
    "np.random.seed(0)\n",
    "data_train = np.random.randint(low=0, high=15, size=(100, 10))\n",
    "data_test = np.random.randint(low=0, high=15, size=(15, 10))\n",
    "y_test = np.random.randint(low=0, high=num_test_classes, size=(100,))\n",
    "\n",
    "nbc = NaiveBayes(num_classes=num_test_classes)\n",
    "nbc.train(data_train, y_test)\n",
    "test_y_pred = nbc.predict(data_test)\n",
    "\n",
    "print(f'Your predicted classes are\\n{test_y_pred}\\nand should be\\n[2 0 0 3 0 3 2 1 2 3 1 0 0 1 0]]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Spam filtering\n",
    "\n",
    "Use your Naive Bayes classifier to predict whether emails in the Enron email dataset are spam! Start by running the following code that uses `np.load` to load in the train/test split that you created last week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email_preprocessor as ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('data/email_train_x.npy')\n",
    "y_train = np.load('data/email_train_y.npy')\n",
    "inds_train = np.load('data/email_train_inds.npy')\n",
    "x_test = np.load('data/email_test_x.npy')\n",
    "y_test = np.load('data/email_test_y.npy')\n",
    "inds_test = np.load('data/email_test_inds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.18%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "enron_nbc = NaiveBayes(num_classes=num_classes)\n",
    "\n",
    "enron_nbc.train(x_train, y_train)\n",
    "\n",
    "test_y_pred = enron_nbc.predict(x_test)\n",
    "\n",
    "accuracy = enron_nbc.accuracy(y_test, test_y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Questions\n",
    "\n",
    "**Question 7:** What accuracy do you get on the test set with Naive Bayes. It should be roughly 89%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 7:** 96.18 ?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Confusion matrix\n",
    "\n",
    "To get a better sense of the errors that the Naive Bayes classifier makes, create a confusion matrix. \n",
    "\n",
    "- Implement `confusion_matrix` in `naive_bayes.py`.\n",
    "- Print out a confusion matrix of the spam classification results. Assign the confusion matrix below to the variable `conf_matrix_nb`.\n",
    "- Run below to help test your confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2951.  242.]\n",
      " [   7. 3325.]]\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(num_classes=2)\n",
    "\n",
    "nb.train(data=x_train, y=y_train)\n",
    "\n",
    "y_test_pred = nb.predict(data=x_test)\n",
    "\n",
    "conf_matrix_nb = nb.confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(conf_matrix_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of entries in your confusion matrix is 6525 and should be 6525.\n",
      "The total number of ham entries in your confusion matrix is 3193 and should be 3193.\n",
      "The total number of spam entries in your confusion matrix is 3332 and should be 3332.\n"
     ]
    }
   ],
   "source": [
    "print(f'The total number of entries in your confusion matrix is {int(conf_matrix_nb.sum())} and should be {len(y_test)}.')\n",
    "print(f'The total number of ham entries in your confusion matrix is {int(conf_matrix_nb[0].sum())} and should be {int(np.sum(y_test == 0))}.')\n",
    "print(f'The total number of spam entries in your confusion matrix is {int(conf_matrix_nb[1].sum())} and should be {int(np.sum(y_test == 1))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4e. Questions\n",
    "\n",
    "**Question 8:** Interpret the confusion matrix, using the convention that positive detection means spam (*e.g. a false positive means classifying a ham email as spam*). What types of errors are made more frequently by the classifier? What does this mean (*i.e. X (spam/ham) is more likely to be classified than Y (spam/ham) than the other way around*)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 8:** The classifier more often makes errors of false positives, mistaking ham for spam, than false negatives, mistaking spam for ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Comparison with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knn import KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. KNN spam email classification accuracy\n",
    "Run a similar analysis to what you did with Naive Bayes above. When computing accuracy on the test set, you may want to reduce the size of the test set (e.g. to the first 500 emails in the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.40%\n",
      "[[267.   0.]\n",
      " [  3. 230.]]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load('data/email_train_x.npy')\n",
    "y_train = np.load('data/email_train_y.npy')\n",
    "inds_train = np.load('data/email_train_inds.npy')\n",
    "x_test = np.load('data/email_test_x.npy')[:500]\n",
    "y_test = np.load('data/email_test_y.npy')[:500]\n",
    "inds_test = np.load('data/email_test_inds.npy')\n",
    "num_classes = 2\n",
    "\n",
    "enron_knn = KNN(num_classes)\n",
    "enron_knn.train(x_train, y_train)\n",
    "\n",
    "test_y_pred = enron_knn.predict(x_test, k=3)\n",
    "\n",
    "accuracy = enron_knn.accuracy(y_test, test_y_pred)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "confusion_matrix = enron_knn.confusion_matrix(y_test, test_y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. KNN spam email confusion matrix\n",
    "Copy-paste your `confusion_matrix` method into `knn.py` so that you can run the same analysis on a KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9:** What accuracy did you get on the test set (potentially reduced in size)?\n",
    "\n",
    "**Question 10:** How does the confusion matrix compare to that obtained by Naive Bayes (*If you reduced the test set size, keep that in mind*)?\n",
    "\n",
    "**Question 11:** Briefly describe at least one pro/con of KNN compared to Naive Bayes on this dataset.\n",
    "\n",
    "**Question 12:** When potentially reducing the size of the test set here, why is it important that we shuffled our train and test set?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9:** I got 99% accuracy\n",
    "\n",
    "**Answer 10:** the confusion matrix shows 3 false positives and 0 false negatives, which is very low, showing that the accuracy is correct in predicting \n",
    "\n",
    "**Answer 11:** KNN works well with datasets that have decision boundaries like this one. However, it can also take long computing time on larger data sets.\n",
    "\n",
    "**Answer 12:** It is important to shuffle the train and test sets because it ensures representativeness and reduces biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Sources and AI\n",
    "\n",
    "Sources:\n",
    "1. Ruby Nunez (Friend). Helped me on some KNN problems I had and some naive bayes tests.\n",
    "2. Also: https://www.geeksforgeeks.org/ml-naive-bayes-scratch-implementation-using-python/ - link to naive bayes\n",
    "3. class notes and pictures from website\n",
    "\n",
    "AI Used: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Classify your own datasets\n",
    "\n",
    "- Find datasets that you find interesting and run classification on them using your KNN algorithm (and if applicable, Naive Bayes). Analysis the performance of your classifer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Better text preprocessing\n",
    "\n",
    "- If you look at the top words extracted from the email dataset, many of them are common \"stop words\" (e.g. a, the, to, etc.) that do not carry much meaning when it comes to differentiating between spam vs. non-spam email. Improve your preprocessing pipeline by building your top words without stop words. Analyze performance differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature size\n",
    "\n",
    "- Explore how the number of selected features for the email dataset influences accuracy and runtime performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Distance metrics\n",
    "- Compare KNN performance with the $L^2$ and $L^1$ distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. K-Fold Cross-Validation\n",
    "\n",
    "- Research this technique and apply it to data and your KNN and/or Naive Bayes classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Email error analysis\n",
    "\n",
    "- Dive deeper into the properties of the emails that were misclassified (FP and/or FN) by Naive Bayes or KNN. What is their word composition? How many words were skipped because they were not in the training set? What could plausibly account for the misclassifications?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Investigate the misclassification errors\n",
    "\n",
    "Numbers are nice, but they may not the best for developing your intuition. Sometimes, you want to see what an misclassification *actually looks like* to help you improve your algorithm. Retrieve the actual text of some example emails of false positive and false negative misclassifications to see if helps you understand why the misclassification occurred. Here is an example workflow:\n",
    "\n",
    "- Decide on how many FP and FN emails you would like to retrieve. Find the indices of this many false positive and false negative misclassification. Remember to use your `test_inds` array to look up the index of the emails BEFORE shuffling happened.\n",
    "- Implement the function `retrieve_emails` in `email_preprocessor.py` to return the string of the raw email at the error indices.\n",
    "- Call your function to print out the emails that produced misclassifications.\n",
    "\n",
    "Do the FP and FN emails make sense? Why? Do the emails have properties in common? Can you quantify and interpret them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. KNN for regression\n",
    "\n",
    "KNN can also be used to perform regression between one or more independent variables and a dependent variable. The potential advantage of this approach is that the regression performed by KNN does not assume any specific form of the regression curve (e.g. line, polynomial, etc.) â€” the regression is entirely training data-dependent.\n",
    "\n",
    "KNN for regression is largely the same as for classification except for the following change during prediction:\n",
    "- For each test sample (validation data), the predicted \"y value\" is the average \"y value\" of the K nearest training samples.\n",
    "- You can use MSE to evaluate how well the regression fits the test samples that you plug in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
