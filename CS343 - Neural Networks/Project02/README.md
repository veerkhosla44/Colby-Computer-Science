# Project 02: Multi-Layer Perceptron (MLP)

Built and trained a feedforward neural network with one or more hidden layers. Implemented forward and backward propagation using NumPy.

### Concepts
- Neural network architecture (input, hidden, output)
- Activation functions: sigmoid, tanh, ReLU
- Backpropagation and chain rule
- Softmax output and cross-entropy loss

### Tasks
- Created a configurable MLP class from scratch
- Implemented layer-wise gradient calculations
- Trained using stochastic gradient descent
- Visualized classification results and learning curves
